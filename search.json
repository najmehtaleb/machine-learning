[
  {
    "objectID": "06-imbalanced-classification/foundation.html",
    "href": "06-imbalanced-classification/foundation.html",
    "title": "Intuition for Imbalanced Classification",
    "section": "",
    "text": "Foundation\nCreate and Plot a Binary Classification Problem\n\nfrom numpy import where\nfrom matplotlib import pyplot\nfrom sklearn.datasets import make_blobs\n\n\nX, y = make_blobs(n_samples=1000, centers=2 ,n_features=2, cluster_std=3, random_state=1)\n\n\nfor class_value in range(2):\n    row_ix = where(y == class_value)\n    pyplot.scatter(X[row_ix, 0], X[row_ix, 1])\n\npyplot.show()\n\n\n\n\nCreate Synthetic Dataset with a Class Distribution\n\nfrom numpy import unique, hstack, vstack, where\nfrom matplotlib import pyplot\nfrom sklearn.datasets import make_blobs\n\n\ndef get_dataset(proportions):\n    n_classes = len(proportions)\n    largest = max([v for k,v in proportions.items()])\n    n_samples = largest * n_classes\n    X, y = make_blobs(n_samples=n_samples, centers=n_classes, n_features=2, \n                      cluster_std=3, random_state=1)\n    \n    X_list, y_list = [], []\n    for k, v in proportions.items():\n        row_ix = where(y == k)[0]\n        selected = row_ix[:v]\n        X_list.append(X[selected, :])\n        y_list.append(y[selected])\n        \n    return vstack(X_list), hstack(y_list)\n\n\ndef plot_dataset(X, y):\n    n_classes = len(unique(y))\n    for class_value in range(n_classes):\n        row_ix = where(y == class_value)[0]\n        pyplot.scatter(X[row_ix, 0], X[row_ix, 1], label=str(class_value))\n        \n    pyplot.legend()\n    pyplot.show()\n\n\nproportions = {0:5000, 1:5000}\n\n\nX, y = get_dataset(proportions)\n\n\nplot_dataset(X, y)\n\n\n\n\nEffect of Skewed Class Distributions\n1:10 Imbalanced Class Distribution\n\nproportions = {0:10000, 1:1000}\nX, y = get_dataset(proportions)\nplot_dataset(X, y)\n\n\n\n\n1:100 Imbalanced Class Distribution\n\nproportions = {0:10000, 1:100}\nX, y = get_dataset(proportions)\nplot_dataset(X, y)\n\n\n\n\n1:1000 Imbalanced Class Distribution\n\nproportions = {0:10000, 1:10}\nX, y = get_dataset(proportions)\nplot_dataset(X, y)\n\n\n\n\n\nChallenge of Imbalanced Classification\n\nfrom matplotlib import pyplot\nfrom numpy import where \nfrom collections import Counter\nfrom sklearn.datasets import make_classification\n\nCompounding Effect of Dataset Size\n\nsizes = [100, 1000, 10000, 100000]\n\n\nfor i in range(len(sizes)):\n    n = sizes[i]\n    X, y = make_classification(n_samples=n, n_features=2, n_redundant=0, n_clusters_per_class=1, \n                               weights=[0.99], flip_y=0, random_state=1)\n    counter = Counter(y)\n    print(f'Size={n}, Ratio={counter}')\n    \n    pyplot.subplot(2, 2, 1+i)\n    pyplot.title('n=%d' % n)\n    pyplot.xticks([])\n    pyplot.yticks([])\n    \n    for label, _ in counter.items():\n        row_ix = where(y == label)[0]\n        pyplot.scatter(X[row_ix, 0], X[row_ix, 1], label=str(label))\n    \n    pyplot.legend()\n\npyplot.show()\n\nSize=100, Ratio=Counter({0: 99, 1: 1})\nSize=1000, Ratio=Counter({0: 990, 1: 10})\nSize=10000, Ratio=Counter({0: 9900, 1: 100})\nSize=100000, Ratio=Counter({0: 99000, 1: 1000})\n\n\n\n\n\nCompounding Effect of Label Noise\n\nnoise = [0, 0.01, 0.05, 0.07]\n\n\nfor i in range(len(noise)):\n    n = noise[i]\n    X, y = make_classification(n_samples=1000, n_features=2, n_redundant=0, \n                              n_clusters_per_class=1, weights=[0.99], flip_y=n, random_state=1)\n    counter = Counter(y)\n    print(f'Noise= {int(n*100)}, Ratio= {counter}')\n    \n    pyplot.subplot(2, 2, 1+i)\n    pyplot.title(f'noise= {int(n*100)}')\n    pyplot.xticks([])\n    pyplot.yticks([])\n    \n    for label, _ in counter.items():\n        row_ix = where(y == label)[0]\n        pyplot.scatter(X[row_ix, 0], X[row_ix, 1], label=str(label))\n    pyplot.legend()\n    \npyplot.show()\n\nNoise= 0, Ratio= Counter({0: 990, 1: 10})\nNoise= 1, Ratio= Counter({0: 983, 1: 17})\nNoise= 5, Ratio= Counter({0: 963, 1: 37})\nNoise= 7, Ratio= Counter({0: 959, 1: 41})\n\n\n\n\n\nCompounding Effect of Data Distribution\n\nclusters = [1, 2]\n\n\nfor i in range(len(clusters)):\n    c = clusters[i]\n    X, y = make_classification(n_samples=10000, n_features=2, n_redundant=0,\n                              n_clusters_per_class=c, weights=[0.99], flip_y=0, random_state=1)\n    counter = Counter(y)\n    \n    pyplot.subplot(1, 2, 1+i)\n    pyplot.title(f'Clusters= {c}')\n    pyplot.xticks([])\n    pyplot.yticks([])\n    \n    for label, _ in counter.items():\n        row_ix = where(y == label)[0]\n        pyplot.scatter(X[row_ix, 0], X[row_ix, 1], label=str(label))\n    \n    pyplot.legend()\n    \npyplot.show()"
  },
  {
    "objectID": "06-imbalanced-classification/evaluation.html",
    "href": "06-imbalanced-classification/evaluation.html",
    "title": "Failure of Accuracy",
    "section": "",
    "text": "Model Evaluation\n\nfrom sklearn.datasets import make_classification\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.model_selection import cross_val_score, RepeatedStratifiedKFold\n\n\ndef evaluate_model(X, y, model):\n    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n    scores = cross_val_score(model, X, y, scoring='accuracy', cv = cv)\n    return scores\n\n\nX, y = make_classification(n_samples=10000, n_features=2, n_redundant=0,\n                          n_clusters_per_class=1, weights=[0.99], flip_y=0, random_state=4)\n\n\nmodel = DummyClassifier(strategy='most_frequent')\n\n\nscores = evaluate_model(X, y, model)\n\n\nprint(f'Mean Accuracy: {scores.mean()*100 : 0.2f}%')\n\n\nPrecision, Recall, and F-measure\n\nCalculate Precision\ncalculates precision for 1:100 dataset with 90 tp and 30 fp\n\nfrom sklearn.metrics import precision_score,recall_score\n\n\n# define actual\nact_pos = [1 for _ in range(100)]\nact_neg = [0 for _ in range(10000)]\ny_true = act_pos + act_neg\n\n# define predictions\npred_pos = [0 for _ in range(10)] + [1 for _ in range(90)]\npred_neg = [1 for _ in range(30)] + [0 for _ in range(9970)]\ny_pred = pred_pos + pred_neg\n\n\n# calculate prediction\nprecision = precision_score(y_true, y_pred, average='binary')\nprint(f'Precision: {precision: 0.3f}')\n\nPrecision:  0.750\n\n\nCalculates precision for 1:1:100 dataset with 50tp, 20fp, 99tp, 51fp\n\n# define actual\nact_pos1 = [1 for _ in range(100)]\nact_pos2 = [2 for _ in range(100)]\nact_neg = [0 for _ in range(10000)]\ny_true = act_pos1 + act_pos2 + act_neg\n\n# define predictions\npred_pos1 = [0 for _ in range(50)] + [1 for _ in range(50)]\npred_pos2 = [0 for _ in range(1)] + [2 for _ in range(99)]\npred_neg = [1 for _ in range(20)] + [2 for _ in range(51)] + [0 for _ in range(9929)]\ny_pred = pred_pos1 + pred_pos2 + pred_neg\n\n\n# calculate prediction\nprecision = precision_score(y_true, y_pred, labels=[1,2], average= 'micro')\nprint( ' Precision: %.3f ' % precision)\n\n Precision: 0.677 \n\n\n\n\nCalculate Recall\nCalculates recall for 1:100 dataset with 90 tp and 10 fn\n\nfrom sklearn.metrics import recall_score\n\n\n# define actual\nact_pos = [1 for _ in range(100)]\nact_neg = [0 for _ in range(10000)]\ny_true = act_pos + act_neg\n\n# define predictions\npred_pos = [0 for _ in range(10)] + [1 for _ in range(90)]\npred_neg = [0 for _ in range(10000)]\ny_pred = pred_pos + pred_neg\n\n\n# calculate recall\nrecall = recall_score(y_true, y_pred, average='binary')\nprint(f'Recall: {recall: .3f}')\n\nRecall:  0.900\n\n\nCalculates recall for 1:1:100 dataset with 77tp, 23fn and 95tp, 5fn\n\n# define actual\nact_pos1 = [1 for _ in range(100)]\nact_pos2 = [2 for _ in range(100)]\nact_neg = [0 for _ in range(10000)]\ny_true = act_pos1 + act_pos2 + act_neg\n\n\n# define predictions\npred_pos1 = [0 for _ in range(23)] + [1 for _ in range(77)]\npred_pos2 = [0 for _ in range(5)] + [2 for _ in range(95)]\npred_neg = [0 for _ in range(10000)]\ny_pred = pred_pos1 + pred_pos2 + pred_neg\n\n\n# calculate recall\nrecall = recall_score(y_true, y_pred, labels=[1,2], average='micro')\nprint(f'Recall: {recall:.3f}')\n\nRecall: 0.860\n\n\n\n\nF-measure\nCalculates f1 for 1:100 dataset with 95tp, 5fn, 55fp\n\nfrom sklearn.metrics import f1_score\n\n\n# define actual\nact_pos = [1 for _ in range(100)]\nact_neg = [0 for _ in range(10000)]\ny_true = act_pos + act_neg\n\n\n# define predictions\npred_pos = [0 for _ in range(5)] + [1 for _ in range(95)]\npred_neg = [1 for _ in range(55)] + [0 for _ in range(9945)]\ny_pred = pred_pos + pred_neg\n\n\n# calculate score\nscore = f1_score(y_true, y_pred, average= 'binary')\nprint(f'F-measure: {score: .3f} ')\n\nF-measure:  0.760 \n\n\n\n\n\nROC Curves and Precision-Recall Curves\n\nfrom sklearn.datasets import make_classification\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_curve, roc_auc_score, precision_recall_curve\nfrom matplotlib import pyplot\n\n\nX, y = make_classification(n_samples=1000, n_classes=2, random_state=1)\n\n\ntrainX, testX, trainy, testy = train_test_split(X, y, test_size=0.5, random_state=2)\n\nROC Curves and ROC AUC\n\nmodel = LogisticRegression(solver='lbfgs')\n\n\nmodel.fit(trainX, traiy)\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression()\n\n\n\nyhat = model.predict_proba(testX)\n\n\npos_probs = yhat[:, 1]\n\n\nfpr, tpr, _ = roc_curve(testy, pos_probs)\n\n\npyplot.plot([0,1], [0,1], linestyle='--', label='No Skill')\npyplot.plot(fpr, tpr, marker= '.', label='Logistic')\npyplot.xlabel('False Positive Rate')\npyplot.ylabel('True Positive Rate')\npyplot.legend()\npyplot.show()\n\n\n\n\nROC Area Under Curve (AUC) Score\n\nmodel = DummyClassifier(strategy='stratified')\n\n\nmodel.fit(trainX, traiy)\n\nDummyClassifier(strategy='stratified')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DummyClassifierDummyClassifier(strategy='stratified')\n\n\n\nyhat = model.predict_proba(testX)\n\n\npos_probs = yhat[:, 1]\n\n\nroc_auc = roc_auc_score(testy, pos_probs)\nprint(f'No skill ROC AUC {roc_auc: .3f}')\n\nNo skill ROC AUC  0.472\n\n\n\nmodel = LogisticRegression(solver='lbfgs')\nmodel.fit(trainX, traiy)\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression()\n\n\n\nyhat = model.predict_proba(testX)\npos_probs = yhat[:, 1]\n\n\nroc_auc = roc_auc_score(testy, pos_probs)\nprint(f'Logistic ROC AUC {roc_auc: 0.3f}')\n\nLogistic ROC AUC  0.903\n\n\nPrecision-Recall Curves and AUC\n\nno_skill = len(y[y==1]) / len(y)\n\n\nprecision, recall, _ = precision_recall_curve(testy, pos_probs)\n\n\npyplot.plot([0,1], [no_skill, no_skill], linestyle='--', label='No Skill')\npyplot.plot(recall, precision, marker='.', label='logistic')\npyplot.xlabel('Recall')\npyplot.ylabel('Precision')\npyplot.legend()\npyplot.show()\n\n\n\n\nPrecision-Recall Area Under Curve (AUC) Score\n\nfrom sklearn.metrics import auc\n\n\nmodel = DummyClassifier(strategy='stratified')\nmodel.fit(trainX, traiy)\n\nDummyClassifier(strategy='stratified')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DummyClassifierDummyClassifier(strategy='stratified')\n\n\n\nyhat = model.predict_proba(testX)\n\n\npos_probs = yhat[:,1]\n\n\nprecision, recall, _ = precision_recall_curve(testy, pos_probs)\nauc_score = auc(recall, precision)\nprint(f'No Skill PR AUC: {auc_score: 0.3f}')\n\nNo Skill PR AUC:  0.607\n\n\n\nmodel = LogisticRegression(solver='lbfgs')\nmodel.fit(trainX, trainy)\nyhat = model.predict_proba(testX)\npos_probs = yhat[:, 1]\n\n\nprecision, recall, _ = precision_recall_curve(testy, pos_probs)\nauc_score = auc(recall, precision)\nprint(f'Logistic PR AUC: {auc_score: 0.3f}')\n\nLogistic PR AUC:  0.898\n\n\n\n\nProbability Scoring Methods\n\n\nCross-Validation for Imbalanced Datasets"
  },
  {
    "objectID": "05-xgboost/xgboost.html",
    "href": "05-xgboost/xgboost.html",
    "title": "First XGBoost Model",
    "section": "",
    "text": "import pickle\nfrom numpy import loadtxt\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\n\ndata_path = '/home/naji/Desktop/github-repos/machine-learning/nbs/0-datasets/'\npima_file = 'pima-indians-diabetes.csv'\n\n\nseed = 7\n\n\ndataset = loadtxt(data_path+pima_file, delimiter=',')\ndataset\n\narray([[  6.   , 148.   ,  72.   , ...,   0.627,  50.   ,   1.   ],\n       [  1.   ,  85.   ,  66.   , ...,   0.351,  31.   ,   0.   ],\n       [  8.   , 183.   ,  64.   , ...,   0.672,  32.   ,   1.   ],\n       ...,\n       [  5.   , 121.   ,  72.   , ...,   0.245,  30.   ,   0.   ],\n       [  1.   , 126.   ,  60.   , ...,   0.349,  47.   ,   1.   ],\n       [  1.   ,  93.   ,  70.   , ...,   0.315,  23.   ,   0.   ]])\n\n\n\nX = dataset[:, 0:8]\ny = dataset[:, 8]\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=seed)\n\n\nmodel = XGBClassifier()\n\n\nmodel.fit(X_train, y_train)\n\nXGBClassifier(base_score=0.5, booster='gbtree', callbacks=None,\n              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n              early_stopping_rounds=None, enable_categorical=False,\n              eval_metric=None, feature_types=None, gamma=0, gpu_id=-1,\n              grow_policy='depthwise', importance_type=None,\n              interaction_constraints='', learning_rate=0.300000012,\n              max_bin=256, max_cat_threshold=64, max_cat_to_onehot=4,\n              max_delta_step=0, max_depth=6, max_leaves=0, min_child_weight=1,\n              missing=nan, monotone_constraints='()', n_estimators=100,\n              n_jobs=0, num_parallel_tree=1, predictor='auto', random_state=0, ...)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.XGBClassifierXGBClassifier(base_score=0.5, booster='gbtree', callbacks=None,\n              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n              early_stopping_rounds=None, enable_categorical=False,\n              eval_metric=None, feature_types=None, gamma=0, gpu_id=-1,\n              grow_policy='depthwise', importance_type=None,\n              interaction_constraints='', learning_rate=0.300000012,\n              max_bin=256, max_cat_threshold=64, max_cat_to_onehot=4,\n              max_delta_step=0, max_depth=6, max_leaves=0, min_child_weight=1,\n              missing=nan, monotone_constraints='()', n_estimators=100,\n              n_jobs=0, num_parallel_tree=1, predictor='auto', random_state=0, ...)\n\n\n\npredictions = model.predict(X_test)\n\n\naccuracy = accuracy_score(y_test, predictions)\n\n\nprint(f'Accuracy is: {accuracy*100: .2f}')\n\nAccuracy is:  74.02\n\n\nVisualize Individual Trees Within A Model\n\nfrom xgboost import plot_tree\nfrom matplotlib import pyplot\n\n\nplot_tree(model)\npyplot.show()\n\n\n\n\n\nplot_tree(model, num_trees=0, rankdir='LR')\npyplot.show()\n\n\n\n\n\npickle.dump(model, open('models/pima.pickle.dat', 'wb'))\nprint('Saved model to: pima.pickle.dat')\n\n\nloadet_model = pickle.load(open('models/pima.pickle.dat', 'rb'))\nprint('Loaded model from: pima.pickle.dat')\n\nLoaded model from: pima.pickle.dat\n\n\n\npredictions = loadet_model.predict(X_test)\n\n\naccuracy = accuracy_score(y_test, predictions)\nprint(f'Accuracy is: {accuracy*100 : 0.2f}%')\n\nAccuracy is:  74.02%\n\n\n\nFeature Importance With XGBoost and Feature Selection\nManually Plot Feature Importance\n\nprint(model.feature_importances_)\n\n[0.08907107 0.23959665 0.08799458 0.09824964 0.09801763 0.15170808\n 0.09959181 0.13577053]\n\n\n\npyplot.bar(range(len(model.feature_importances_)), model.feature_importances_)\npyplot.show()\n\n\n\n\nUsing the Built-in XGBoost Feature Importance Plot\n\nfrom xgboost import plot_importance\n\n\nplot_importance(model)\npyplot.show()\n\n\n\n\nFeature Selection with XGBoost Feature Importance Scores\n\n\nMonitor Training Performance and Early Stopping\nMonitoring Training Performance With XGBoost\n\neval_set = [(X_test, y_test)]\n\n\nmodel = XGBClassifier(eval_metric='error')\n\n\nmodel.fit(X_train, y_train, eval_set=eval_set, verbose=True)\n\n[0] validation_0-error:0.28346\n[1] validation_0-error:0.25984\n[2] validation_0-error:0.25591\n[3] validation_0-error:0.24803\n[4] validation_0-error:0.24409\n[5] validation_0-error:0.24803\n[6] validation_0-error:0.25591\n[7] validation_0-error:0.24803\n[8] validation_0-error:0.25591\n[9] validation_0-error:0.24409\n[10]    validation_0-error:0.24803\n[11]    validation_0-error:0.24409\n[12]    validation_0-error:0.23228\n[13]    validation_0-error:0.24016\n[14]    validation_0-error:0.23622\n[15]    validation_0-error:0.24409\n[16]    validation_0-error:0.25591\n[17]    validation_0-error:0.23622\n[18]    validation_0-error:0.24016\n[19]    validation_0-error:0.23622\n[20]    validation_0-error:0.23622\n[21]    validation_0-error:0.23622\n[22]    validation_0-error:0.23622\n[23]    validation_0-error:0.24409\n[24]    validation_0-error:0.24409\n[25]    validation_0-error:0.24016\n[26]    validation_0-error:0.24409\n[27]    validation_0-error:0.24409\n[28]    validation_0-error:0.25591\n[29]    validation_0-error:0.25197\n[30]    validation_0-error:0.24803\n[31]    validation_0-error:0.25591\n[32]    validation_0-error:0.25591\n[33]    validation_0-error:0.25984\n[34]    validation_0-error:0.26378\n[35]    validation_0-error:0.26378\n[36]    validation_0-error:0.26378\n[37]    validation_0-error:0.26772\n[38]    validation_0-error:0.26378\n[39]    validation_0-error:0.25984\n[40]    validation_0-error:0.25591\n[41]    validation_0-error:0.24409\n[42]    validation_0-error:0.24803\n[43]    validation_0-error:0.24803\n[44]    validation_0-error:0.25591\n[45]    validation_0-error:0.25197\n[46]    validation_0-error:0.26378\n[47]    validation_0-error:0.26378\n[48]    validation_0-error:0.26378\n[49]    validation_0-error:0.25984\n[50]    validation_0-error:0.27165\n[51]    validation_0-error:0.26772\n[52]    validation_0-error:0.27165\n[53]    validation_0-error:0.26772\n[54]    validation_0-error:0.26378\n[55]    validation_0-error:0.26378\n[56]    validation_0-error:0.26378\n[57]    validation_0-error:0.26772\n[58]    validation_0-error:0.27165\n[59]    validation_0-error:0.26772\n[60]    validation_0-error:0.27165\n[61]    validation_0-error:0.27165\n[62]    validation_0-error:0.26772\n[63]    validation_0-error:0.26772\n[64]    validation_0-error:0.26378\n[65]    validation_0-error:0.25984\n[66]    validation_0-error:0.27165\n[67]    validation_0-error:0.27559\n[68]    validation_0-error:0.26772\n[69]    validation_0-error:0.26378\n[70]    validation_0-error:0.26378\n[71]    validation_0-error:0.26772\n[72]    validation_0-error:0.26772\n[73]    validation_0-error:0.26772\n[74]    validation_0-error:0.26772\n[75]    validation_0-error:0.26772\n[76]    validation_0-error:0.26772\n[77]    validation_0-error:0.27165\n[78]    validation_0-error:0.26772\n[79]    validation_0-error:0.27165\n[80]    validation_0-error:0.27165\n[81]    validation_0-error:0.28346\n[82]    validation_0-error:0.27559\n[83]    validation_0-error:0.27165\n[84]    validation_0-error:0.27559\n[85]    validation_0-error:0.26772\n[86]    validation_0-error:0.26772\n[87]    validation_0-error:0.26378\n[88]    validation_0-error:0.26772\n[89]    validation_0-error:0.26378\n[90]    validation_0-error:0.27165\n[91]    validation_0-error:0.26772\n[92]    validation_0-error:0.27165\n[93]    validation_0-error:0.26378\n[94]    validation_0-error:0.27165\n[95]    validation_0-error:0.26378\n[96]    validation_0-error:0.25984\n[97]    validation_0-error:0.26378\n[98]    validation_0-error:0.25984\n[99]    validation_0-error:0.25984\n\n\nXGBClassifier(base_score=0.5, booster='gbtree', callbacks=None,\n              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n              early_stopping_rounds=None, enable_categorical=False,\n              eval_metric='error', feature_types=None, gamma=0, gpu_id=-1,\n              grow_policy='depthwise', importance_type=None,\n              interaction_constraints='', learning_rate=0.300000012,\n              max_bin=256, max_cat_threshold=64, max_cat_to_onehot=4,\n              max_delta_step=0, max_depth=6, max_leaves=0, min_child_weight=1,\n              missing=nan, monotone_constraints='()', n_estimators=100,\n              n_jobs=0, num_parallel_tree=1, predictor='auto', random_state=0, ...)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.XGBClassifierXGBClassifier(base_score=0.5, booster='gbtree', callbacks=None,\n              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n              early_stopping_rounds=None, enable_categorical=False,\n              eval_metric='error', feature_types=None, gamma=0, gpu_id=-1,\n              grow_policy='depthwise', importance_type=None,\n              interaction_constraints='', learning_rate=0.300000012,\n              max_bin=256, max_cat_threshold=64, max_cat_to_onehot=4,\n              max_delta_step=0, max_depth=6, max_leaves=0, min_child_weight=1,\n              missing=nan, monotone_constraints='()', n_estimators=100,\n              n_jobs=0, num_parallel_tree=1, predictor='auto', random_state=0, ...)\n\n\n\npredictions = model.predict(X_test)\n\n\naccuracy = accuracy_score(y_test, predictions)\nprint(f'Accuracy: {accuracy*100: .2f}')\n\nAccuracy:  74.02\n\n\nEvaluate XGBoost Models With Learning Curves\n\nmodel = XGBClassifier(eval_metric=['error', 'logloss'])\n\n\neval_set = [(X_train, y_train), (X_test, y_test)]\n\n\nmodel.fit(X_train, y_train, eval_set=eval_set, verbose=True)\n\n[0] validation_0-error:0.13619  validation_0-logloss:0.55257    validation_1-error:0.28346  validation_1-logloss:0.60491\n[1] validation_0-error:0.10895  validation_0-logloss:0.46754    validation_1-error:0.25984  validation_1-logloss:0.55934\n[2] validation_0-error:0.10506  validation_0-logloss:0.40734    validation_1-error:0.25591  validation_1-logloss:0.53068\n[3] validation_0-error:0.09144  validation_0-logloss:0.36480    validation_1-error:0.24803  validation_1-logloss:0.51795\n[4] validation_0-error:0.08560  validation_0-logloss:0.33012    validation_1-error:0.24409  validation_1-logloss:0.51153\n[5] validation_0-error:0.07782  validation_0-logloss:0.29868    validation_1-error:0.24803  validation_1-logloss:0.50934\n[6] validation_0-error:0.06809  validation_0-logloss:0.27852    validation_1-error:0.25591  validation_1-logloss:0.50818\n[7] validation_0-error:0.06615  validation_0-logloss:0.26182    validation_1-error:0.24803  validation_1-logloss:0.51097\n[8] validation_0-error:0.06226  validation_0-logloss:0.24578    validation_1-error:0.25591  validation_1-logloss:0.51760\n[9] validation_0-error:0.05642  validation_0-logloss:0.23298    validation_1-error:0.24409  validation_1-logloss:0.51912\n[10]    validation_0-error:0.04669  validation_0-logloss:0.21955    validation_1-error:0.24803  validation_1-logloss:0.52503\n[11]    validation_0-error:0.04280  validation_0-logloss:0.21051    validation_1-error:0.24409  validation_1-logloss:0.52697\n[12]    validation_0-error:0.03502  validation_0-logloss:0.20083    validation_1-error:0.23228  validation_1-logloss:0.53335\n[13]    validation_0-error:0.03696  validation_0-logloss:0.19466    validation_1-error:0.24016  validation_1-logloss:0.53905\n[14]    validation_0-error:0.03502  validation_0-logloss:0.18725    validation_1-error:0.23622  validation_1-logloss:0.54545\n[15]    validation_0-error:0.02918  validation_0-logloss:0.17765    validation_1-error:0.24409  validation_1-logloss:0.54613\n[16]    validation_0-error:0.02724  validation_0-logloss:0.16747    validation_1-error:0.25591  validation_1-logloss:0.54982\n[17]    validation_0-error:0.02140  validation_0-logloss:0.15879    validation_1-error:0.23622  validation_1-logloss:0.55226\n[18]    validation_0-error:0.01946  validation_0-logloss:0.15115    validation_1-error:0.24016  validation_1-logloss:0.55355\n[19]    validation_0-error:0.00973  validation_0-logloss:0.14529    validation_1-error:0.23622  validation_1-logloss:0.55847\n[20]    validation_0-error:0.00973  validation_0-logloss:0.14282    validation_1-error:0.23622  validation_1-logloss:0.56063\n[21]    validation_0-error:0.00778  validation_0-logloss:0.13959    validation_1-error:0.23622  validation_1-logloss:0.56665\n[22]    validation_0-error:0.00778  validation_0-logloss:0.13253    validation_1-error:0.23622  validation_1-logloss:0.57418\n[23]    validation_0-error:0.00778  validation_0-logloss:0.12705    validation_1-error:0.24409  validation_1-logloss:0.57448\n[24]    validation_0-error:0.00778  validation_0-logloss:0.12430    validation_1-error:0.24409  validation_1-logloss:0.57511\n[25]    validation_0-error:0.00584  validation_0-logloss:0.12175    validation_1-error:0.24016  validation_1-logloss:0.58052\n[26]    validation_0-error:0.00584  validation_0-logloss:0.11715    validation_1-error:0.24409  validation_1-logloss:0.58830\n[27]    validation_0-error:0.00584  validation_0-logloss:0.11200    validation_1-error:0.24409  validation_1-logloss:0.59717\n[28]    validation_0-error:0.00195  validation_0-logloss:0.10682    validation_1-error:0.25591  validation_1-logloss:0.60530\n[29]    validation_0-error:0.00389  validation_0-logloss:0.10413    validation_1-error:0.25197  validation_1-logloss:0.60871\n[30]    validation_0-error:0.00195  validation_0-logloss:0.09942    validation_1-error:0.24803  validation_1-logloss:0.61161\n[31]    validation_0-error:0.00195  validation_0-logloss:0.09640    validation_1-error:0.25591  validation_1-logloss:0.61695\n[32]    validation_0-error:0.00195  validation_0-logloss:0.09168    validation_1-error:0.25591  validation_1-logloss:0.61717\n[33]    validation_0-error:0.00000  validation_0-logloss:0.08941    validation_1-error:0.25984  validation_1-logloss:0.62061\n[34]    validation_0-error:0.00195  validation_0-logloss:0.08648    validation_1-error:0.26378  validation_1-logloss:0.61886\n[35]    validation_0-error:0.00000  validation_0-logloss:0.08371    validation_1-error:0.26378  validation_1-logloss:0.61903\n[36]    validation_0-error:0.00000  validation_0-logloss:0.08277    validation_1-error:0.26378  validation_1-logloss:0.62187\n[37]    validation_0-error:0.00000  validation_0-logloss:0.08041    validation_1-error:0.26772  validation_1-logloss:0.62557\n[38]    validation_0-error:0.00000  validation_0-logloss:0.07842    validation_1-error:0.26378  validation_1-logloss:0.62663\n[39]    validation_0-error:0.00000  validation_0-logloss:0.07651    validation_1-error:0.25984  validation_1-logloss:0.62743\n[40]    validation_0-error:0.00000  validation_0-logloss:0.07424    validation_1-error:0.25591  validation_1-logloss:0.62667\n[41]    validation_0-error:0.00000  validation_0-logloss:0.07202    validation_1-error:0.24409  validation_1-logloss:0.63148\n[42]    validation_0-error:0.00000  validation_0-logloss:0.07012    validation_1-error:0.24803  validation_1-logloss:0.63695\n[43]    validation_0-error:0.00000  validation_0-logloss:0.06862    validation_1-error:0.24803  validation_1-logloss:0.64021\n[44]    validation_0-error:0.00000  validation_0-logloss:0.06629    validation_1-error:0.25591  validation_1-logloss:0.64323\n[45]    validation_0-error:0.00000  validation_0-logloss:0.06394    validation_1-error:0.25197  validation_1-logloss:0.64747\n[46]    validation_0-error:0.00000  validation_0-logloss:0.06231    validation_1-error:0.26378  validation_1-logloss:0.64921\n[47]    validation_0-error:0.00000  validation_0-logloss:0.06090    validation_1-error:0.26378  validation_1-logloss:0.65250\n[48]    validation_0-error:0.00000  validation_0-logloss:0.05953    validation_1-error:0.26378  validation_1-logloss:0.65838\n[49]    validation_0-error:0.00000  validation_0-logloss:0.05801    validation_1-error:0.25984  validation_1-logloss:0.66152\n[50]    validation_0-error:0.00000  validation_0-logloss:0.05643    validation_1-error:0.27165  validation_1-logloss:0.66584\n[51]    validation_0-error:0.00000  validation_0-logloss:0.05549    validation_1-error:0.26772  validation_1-logloss:0.66783\n[52]    validation_0-error:0.00000  validation_0-logloss:0.05462    validation_1-error:0.27165  validation_1-logloss:0.67103\n[53]    validation_0-error:0.00000  validation_0-logloss:0.05347    validation_1-error:0.26772  validation_1-logloss:0.67425\n[54]    validation_0-error:0.00000  validation_0-logloss:0.05253    validation_1-error:0.26378  validation_1-logloss:0.67873\n[55]    validation_0-error:0.00000  validation_0-logloss:0.05153    validation_1-error:0.26378  validation_1-logloss:0.67768\n[56]    validation_0-error:0.00000  validation_0-logloss:0.05051    validation_1-error:0.26378  validation_1-logloss:0.68269\n[57]    validation_0-error:0.00000  validation_0-logloss:0.04942    validation_1-error:0.26772  validation_1-logloss:0.68738\n[58]    validation_0-error:0.00000  validation_0-logloss:0.04892    validation_1-error:0.27165  validation_1-logloss:0.69011\n[59]    validation_0-error:0.00000  validation_0-logloss:0.04799    validation_1-error:0.26772  validation_1-logloss:0.69266\n[60]    validation_0-error:0.00000  validation_0-logloss:0.04720    validation_1-error:0.27165  validation_1-logloss:0.69469\n[61]    validation_0-error:0.00000  validation_0-logloss:0.04643    validation_1-error:0.27165  validation_1-logloss:0.70239\n[62]    validation_0-error:0.00000  validation_0-logloss:0.04535    validation_1-error:0.26772  validation_1-logloss:0.70504\n[63]    validation_0-error:0.00000  validation_0-logloss:0.04454    validation_1-error:0.26772  validation_1-logloss:0.70622\n[64]    validation_0-error:0.00000  validation_0-logloss:0.04379    validation_1-error:0.26378  validation_1-logloss:0.70810\n[65]    validation_0-error:0.00000  validation_0-logloss:0.04315    validation_1-error:0.25984  validation_1-logloss:0.71247\n[66]    validation_0-error:0.00000  validation_0-logloss:0.04241    validation_1-error:0.27165  validation_1-logloss:0.71706\n[67]    validation_0-error:0.00000  validation_0-logloss:0.04163    validation_1-error:0.27559  validation_1-logloss:0.71636\n[68]    validation_0-error:0.00000  validation_0-logloss:0.04085    validation_1-error:0.26772  validation_1-logloss:0.71625\n[69]    validation_0-error:0.00000  validation_0-logloss:0.04036    validation_1-error:0.26378  validation_1-logloss:0.71904\n[70]    validation_0-error:0.00000  validation_0-logloss:0.03993    validation_1-error:0.26378  validation_1-logloss:0.72348\n\n\n[71]    validation_0-error:0.00000  validation_0-logloss:0.03907    validation_1-error:0.26772  validation_1-logloss:0.72573\n[72]    validation_0-error:0.00000  validation_0-logloss:0.03835    validation_1-error:0.26772  validation_1-logloss:0.72761\n[73]    validation_0-error:0.00000  validation_0-logloss:0.03762    validation_1-error:0.26772  validation_1-logloss:0.72992\n[74]    validation_0-error:0.00000  validation_0-logloss:0.03719    validation_1-error:0.26772  validation_1-logloss:0.73336\n[75]    validation_0-error:0.00000  validation_0-logloss:0.03669    validation_1-error:0.26772  validation_1-logloss:0.73444\n[76]    validation_0-error:0.00000  validation_0-logloss:0.03632    validation_1-error:0.26772  validation_1-logloss:0.73795\n[77]    validation_0-error:0.00000  validation_0-logloss:0.03588    validation_1-error:0.27165  validation_1-logloss:0.74054\n[78]    validation_0-error:0.00000  validation_0-logloss:0.03521    validation_1-error:0.26772  validation_1-logloss:0.74512\n[79]    validation_0-error:0.00000  validation_0-logloss:0.03464    validation_1-error:0.27165  validation_1-logloss:0.74767\n[80]    validation_0-error:0.00000  validation_0-logloss:0.03432    validation_1-error:0.27165  validation_1-logloss:0.74878\n[81]    validation_0-error:0.00000  validation_0-logloss:0.03380    validation_1-error:0.28346  validation_1-logloss:0.75047\n[82]    validation_0-error:0.00000  validation_0-logloss:0.03343    validation_1-error:0.27559  validation_1-logloss:0.75475\n[83]    validation_0-error:0.00000  validation_0-logloss:0.03297    validation_1-error:0.27165  validation_1-logloss:0.75587\n[84]    validation_0-error:0.00000  validation_0-logloss:0.03245    validation_1-error:0.27559  validation_1-logloss:0.75861\n[85]    validation_0-error:0.00000  validation_0-logloss:0.03208    validation_1-error:0.26772  validation_1-logloss:0.75890\n[86]    validation_0-error:0.00000  validation_0-logloss:0.03169    validation_1-error:0.26772  validation_1-logloss:0.76230\n[87]    validation_0-error:0.00000  validation_0-logloss:0.03139    validation_1-error:0.26378  validation_1-logloss:0.76483\n[88]    validation_0-error:0.00000  validation_0-logloss:0.03111    validation_1-error:0.26772  validation_1-logloss:0.76738\n[89]    validation_0-error:0.00000  validation_0-logloss:0.03077    validation_1-error:0.26378  validation_1-logloss:0.77021\n[90]    validation_0-error:0.00000  validation_0-logloss:0.03041    validation_1-error:0.27165  validation_1-logloss:0.77393\n[91]    validation_0-error:0.00000  validation_0-logloss:0.03003    validation_1-error:0.26772  validation_1-logloss:0.77259\n[92]    validation_0-error:0.00000  validation_0-logloss:0.02976    validation_1-error:0.27165  validation_1-logloss:0.77214\n[93]    validation_0-error:0.00000  validation_0-logloss:0.02947    validation_1-error:0.26378  validation_1-logloss:0.77362\n[94]    validation_0-error:0.00000  validation_0-logloss:0.02912    validation_1-error:0.27165  validation_1-logloss:0.77521\n[95]    validation_0-error:0.00000  validation_0-logloss:0.02877    validation_1-error:0.26378  validation_1-logloss:0.77405\n[96]    validation_0-error:0.00000  validation_0-logloss:0.02853    validation_1-error:0.25984  validation_1-logloss:0.77413\n[97]    validation_0-error:0.00000  validation_0-logloss:0.02833    validation_1-error:0.26378  validation_1-logloss:0.77805\n[98]    validation_0-error:0.00000  validation_0-logloss:0.02809    validation_1-error:0.25984  validation_1-logloss:0.77660\n[99]    validation_0-error:0.00000  validation_0-logloss:0.02787    validation_1-error:0.25984  validation_1-logloss:0.77681\n\n\nXGBClassifier(base_score=0.5, booster='gbtree', callbacks=None,\n              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n              early_stopping_rounds=None, enable_categorical=False,\n              eval_metric=['error', 'logloss'], feature_types=None, gamma=0,\n              gpu_id=-1, grow_policy='depthwise', importance_type=None,\n              interaction_constraints='', learning_rate=0.300000012,\n              max_bin=256, max_cat_threshold=64, max_cat_to_onehot=4,\n              max_delta_step=0, max_depth=6, max_leaves=0, min_child_weight=1,\n              missing=nan, monotone_constraints='()', n_estimators=100,\n              n_jobs=0, num_parallel_tree=1, predictor='auto', random_state=0, ...)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.XGBClassifierXGBClassifier(base_score=0.5, booster='gbtree', callbacks=None,\n              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n              early_stopping_rounds=None, enable_categorical=False,\n              eval_metric=['error', 'logloss'], feature_types=None, gamma=0,\n              gpu_id=-1, grow_policy='depthwise', importance_type=None,\n              interaction_constraints='', learning_rate=0.300000012,\n              max_bin=256, max_cat_threshold=64, max_cat_to_onehot=4,\n              max_delta_step=0, max_depth=6, max_leaves=0, min_child_weight=1,\n              missing=nan, monotone_constraints='()', n_estimators=100,\n              n_jobs=0, num_parallel_tree=1, predictor='auto', random_state=0, ...)\n\n\n\npredictions = model.predict(X_test)\n\n\naccuracy = accuracy_score(y_test, predictions)\nprint(f'Accuracy: {accuracy*100 : 0.2f}%')\n\nAccuracy:  74.02%\n\n\n\nresults = model.evals_result()\n\n\nepochs = len(results['validation_0']['error'])\n\n\nx_axis = range(0, epochs)\n\n\n# plot log loss\nfig, ax = pyplot.subplots()\nax.plot(x_axis, results['validation_0']['logloss'], label='Train')\nax.plot(x_axis, results['validation_1']['logloss'], label='Test')\nax.legend()\npyplot.ylabel('Log Loss')\npyplot.title('XGBoost Log Loss')\npyplot.show()\n\n\n\n\n\n# plot classification error\nfig, ax = pyplot.subplots()\nax.plot(x_axis, results['validation_0']['error'], label= 'Train')\nax.plot(x_axis, results['validation_1']['error'], label= 'Test')\nax.legend()\npyplot.ylabel('Classification Error')\npyplot.title('XGBoost Classification Error')\npyplot.show()\n\n\n\n\nEarly Stopping With XGBoost\n\nmodel = XGBClassifier(eval_metric='logloss')\n\n\neval_set = [(X_test, y_test)]\n\n\nmodel.fit(X_train, y_train, early_stopping_rounds=10, eval_set=eval_set, verbose=True)\n\n[0] validation_0-logloss:0.60491\n[1] validation_0-logloss:0.55934\n[2] validation_0-logloss:0.53068\n[3] validation_0-logloss:0.51795\n[4] validation_0-logloss:0.51153\n[5] validation_0-logloss:0.50934\n[6] validation_0-logloss:0.50818\n[7] validation_0-logloss:0.51097\n[8] validation_0-logloss:0.51760\n[9] validation_0-logloss:0.51912\n[10]    validation_0-logloss:0.52503\n[11]    validation_0-logloss:0.52697\n[12]    validation_0-logloss:0.53335\n[13]    validation_0-logloss:0.53905\n[14]    validation_0-logloss:0.54545\n[15]    validation_0-logloss:0.54613\n\n\nXGBClassifier(base_score=0.5, booster='gbtree', callbacks=None,\n              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n              early_stopping_rounds=None, enable_categorical=False,\n              eval_metric='logloss', feature_types=None, gamma=0, gpu_id=-1,\n              grow_policy='depthwise', importance_type=None,\n              interaction_constraints='', learning_rate=0.300000012,\n              max_bin=256, max_cat_threshold=64, max_cat_to_onehot=4,\n              max_delta_step=0, max_depth=6, max_leaves=0, min_child_weight=1,\n              missing=nan, monotone_constraints='()', n_estimators=100,\n              n_jobs=0, num_parallel_tree=1, predictor='auto', random_state=0, ...)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.XGBClassifierXGBClassifier(base_score=0.5, booster='gbtree', callbacks=None,\n              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n              early_stopping_rounds=None, enable_categorical=False,\n              eval_metric='logloss', feature_types=None, gamma=0, gpu_id=-1,\n              grow_policy='depthwise', importance_type=None,\n              interaction_constraints='', learning_rate=0.300000012,\n              max_bin=256, max_cat_threshold=64, max_cat_to_onehot=4,\n              max_delta_step=0, max_depth=6, max_leaves=0, min_child_weight=1,\n              missing=nan, monotone_constraints='()', n_estimators=100,\n              n_jobs=0, num_parallel_tree=1, predictor='auto', random_state=0, ...)\n\n\n\n\nTune Multithreading Support for XGBoost\n\n\nXGBoost Tuning\n\nTune the Number and Size of Decision Trees with XGBoost\n\nimport matplotlib\nfrom matplotlib import pyplot\nfrom pandas import read_csv\nfrom sklearn.model_selection import GridSearchCV, StratifiedKFold\nfrom sklearn.preprocessing import LabelEncoder\n\n\nmatplotlib.use('Agg')\n\nTune the Number of Decision Trees\n\nfile_path = '/home/naji/Desktop/github-repos/machine-learning/nbs/0-datasets/otto/'\n\n\ndata = read_csv(file_path + 'train.csv')\ndata\n\n\n\n\n\n  \n    \n      \n      id\n      feat_1\n      feat_2\n      feat_3\n      feat_4\n      feat_5\n      feat_6\n      feat_7\n      feat_8\n      feat_9\n      feat_10\n      feat_11\n      feat_12\n      feat_13\n      feat_14\n      feat_15\n      feat_16\n      feat_17\n      feat_18\n      feat_19\n      feat_20\n      feat_21\n      feat_22\n      feat_23\n      feat_24\n      feat_25\n      feat_26\n      feat_27\n      feat_28\n      feat_29\n      feat_30\n      feat_31\n      feat_32\n      feat_33\n      feat_34\n      feat_35\n      feat_36\n      feat_37\n      feat_38\n      feat_39\n      ...\n      feat_55\n      feat_56\n      feat_57\n      feat_58\n      feat_59\n      feat_60\n      feat_61\n      feat_62\n      feat_63\n      feat_64\n      feat_65\n      feat_66\n      feat_67\n      feat_68\n      feat_69\n      feat_70\n      feat_71\n      feat_72\n      feat_73\n      feat_74\n      feat_75\n      feat_76\n      feat_77\n      feat_78\n      feat_79\n      feat_80\n      feat_81\n      feat_82\n      feat_83\n      feat_84\n      feat_85\n      feat_86\n      feat_87\n      feat_88\n      feat_89\n      feat_90\n      feat_91\n      feat_92\n      feat_93\n      target\n    \n  \n  \n    \n      0\n      1\n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n      0\n      2\n      0\n      0\n      0\n      0\n      1\n      0\n      4\n      1\n      1\n      0\n      0\n      2\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      2\n      0\n      0\n      11\n      0\n      1\n      1\n      0\n      1\n      0\n      7\n      0\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      2\n      1\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      Class_1\n    \n    \n      1\n      2\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      2\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n      ...\n      0\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n      0\n      2\n      1\n      0\n      1\n      0\n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      Class_1\n    \n    \n      2\n      3\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      6\n      0\n      0\n      2\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      Class_1\n    \n    \n      3\n      4\n      1\n      0\n      0\n      1\n      6\n      1\n      5\n      0\n      0\n      1\n      1\n      0\n      1\n      0\n      0\n      1\n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      7\n      2\n      2\n      0\n      0\n      0\n      58\n      0\n      10\n      0\n      0\n      0\n      0\n      0\n      3\n      0\n      ...\n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      2\n      1\n      5\n      0\n      0\n      4\n      0\n      0\n      2\n      1\n      0\n      1\n      0\n      0\n      1\n      1\n      2\n      2\n      0\n      22\n      0\n      1\n      2\n      0\n      0\n      0\n      0\n      0\n      0\n      Class_1\n    \n    \n      4\n      5\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      4\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n      3\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      4\n      0\n      1\n      0\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n      0\n      Class_1\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      61873\n      61874\n      1\n      0\n      0\n      1\n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      3\n      0\n      0\n      9\n      0\n      2\n      0\n      0\n      0\n      7\n      0\n      3\n      6\n      1\n      0\n      0\n      65\n      1\n      0\n      4\n      3\n      1\n      1\n      1\n      2\n      1\n      0\n      ...\n      3\n      1\n      0\n      0\n      0\n      1\n      0\n      22\n      0\n      1\n      4\n      11\n      3\n      0\n      0\n      3\n      0\n      1\n      1\n      2\n      0\n      0\n      29\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      2\n      0\n      Class_9\n    \n    \n      61874\n      61875\n      4\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      4\n      2\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      2\n      0\n      0\n      0\n      1\n      0\n      0\n      0\n      2\n      1\n      0\n      2\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      2\n      0\n      0\n      1\n      5\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      11\n      0\n      0\n      0\n      0\n      0\n      0\n      2\n      0\n      0\n      2\n      0\n      0\n      1\n      0\n      Class_9\n    \n    \n      61875\n      61876\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      3\n      1\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      3\n      0\n      0\n      2\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      19\n      0\n      0\n      4\n      0\n      0\n      0\n      0\n      18\n      0\n      0\n      0\n      0\n      2\n      0\n      0\n      0\n      2\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      3\n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      Class_9\n    \n    \n      61876\n      61877\n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      3\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      2\n      1\n      0\n      0\n      1\n      2\n      0\n      0\n      2\n      1\n      0\n      0\n      5\n      0\n      0\n      0\n      ...\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n      2\n      0\n      0\n      0\n      0\n      6\n      0\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      3\n      10\n      0\n      Class_9\n    \n    \n      61877\n      61878\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      3\n      0\n      1\n      1\n      1\n      1\n      0\n      0\n      0\n      3\n      0\n      2\n      1\n      0\n      0\n      0\n      9\n      0\n      0\n      0\n      0\n      0\n      0\n      2\n      0\n      0\n      0\n      ...\n      1\n      0\n      1\n      0\n      0\n      3\n      0\n      4\n      0\n      0\n      0\n      0\n      10\n      2\n      0\n      0\n      0\n      0\n      0\n      3\n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      2\n      0\n      Class_9\n    \n  \n\n61878 rows  95 columns\n\n\n\n\ndataset = data.values\n\n\nX = dataset[:, 0:94]\ny = dataset[:, 94]\n\n\nlabel_encoded_y = LabelEncoder().fit_transform(y)\n\n\nmodel = XGBClassifier()\n\n\nn_estimators = range(50, 150, 50)\n\n\nparam_grid = dict(n_estimators=n_estimators)\n\n\nkfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=7)\n\n\ngrid_search = GridSearchCV(model, param_grid, scoring='neg_log_loss', cv=kfold)\n\n\ngrid_result = grid_search.fit(X, label_encoded_y)\n\nKeyboardInterrupt: \n\n\n\n# summarize results\nprint(f'Best: {}')\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\n\n\nfor mean, stdev, param in zip(means, stds, params):\n    print(f'{mean} ({stdev}) with {param}')\n\n\npyplott.errorbar(n_estimators, means, yerr=stds)\npyplot.title(\"XGBoost n_estimators vs Log Loss\")\npyplot.xlabel('n_estimators')\npyplot.ylabel('Log Loss')\npyplot.savefig('n_estimators.png')\n\nTune the Size of Decision Trees\n\nmodel = XGBClassifier()\n\n\nmax_depth = range(1, 5, 2)\n\n\nparam_grid = dict(max_depth=max_depth)\n\n\nkfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=7)\n\n\ngrid_search = GridSearchCV(model, param_grid, scoring='neg_log_loss', cv=kfold, verbose=1)\n\n\ngrid_result = grid_search.fit(X, label_encoded_y)\n\nFitting 10 folds for each of 2 candidates, totalling 20 fits\n\n\nKeyboardInterrupt: \n\n\n\nprint(f'Best: {grid_result.best_score_} using {grid_result.best_params_}')\n\n\n# summarize results\nprint(f'Best: {}')\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\n\nTune The Number and Size of Trees\n\n\nTune Learning Rate and Number of Trees with XGBoost\n\n\nTuning Stochastic Gradient Boosting with XGBoost"
  },
  {
    "objectID": "01-ml-ecosystem/numpy.html",
    "href": "01-ml-ecosystem/numpy.html",
    "title": "NumPy",
    "section": "",
    "text": "import numpy\n\nCreate Array\n\nmylist = [1,2,3]\n\n\nmyarray = numpy.array(mylist)\n\n\nprint(myarray)\n\n[1 2 3]\n\n\n\nprint(myarray.shape)\n\n(3,)\n\n\nAccess Data\n\nmylist = [[1,2,3], [4,5,6]]\n\n\nmyarray = numpy.array(mylist)\n\n\nprint(myarray)\n\n[[1 2 3]\n [4 5 6]]\n\n\n\nprint(myarray.shape)\n\n(2, 3)\n\n\n\nprint(f'First row is: {myarray[0]}')\n\nFirst row is: [1 2 3]\n\n\n\nprint(f'Last row is: {myarray[-1]}')\n\nLast row is: [4 5 6]\n\n\n\nprint(f'An element at specific row {0} and col {2} is: {myarray[0,2]}')\n\nAn element at specific row 0 and col 2 is: 3\n\n\n\nprint(f'Whole col {2} is: {myarray[:,2]}')\n\nWhole col 2 is: [3 6]\n\n\nArithmetic\n\narray1 = numpy.array([2,3,4])\narray2 = numpy.array([5,6,7])\n\n\nprint(f'Adding two arrays: {array1 + array2}')\n\nAdding two arrays: [ 7  9 11]\n\n\n\nprint(f'Multiplying two arrays: {array1*array2}')\n\nMultiplying two arrays: [10 18 28]"
  },
  {
    "objectID": "01-ml-ecosystem/matplotlib.html",
    "href": "01-ml-ecosystem/matplotlib.html",
    "title": "Matplotlib",
    "section": "",
    "text": "Matplotlib can be used for creating plots and charts. The library is generally used as follows:  Call a plotting function with some data (e.g..plot()).  Call many functions to setup the properties of the plot (e.g.labels and colors).  Make the plot visible (e.g..show()).\n\nimport numpy\nimport matplotlib.pyplot as plt\n\nLine Plot\n\nmyarray = numpy.array([1,2,3])\n\n\nplt.plot(myarray)\nplt.xlabel('X axis')\nplt.ylabel('Y axis')\nplt.show()\n\n\n\n\nScatter Plot\n\nx = numpy.array([1,2,3])\ny = numpy.array([2,4,6])\n\n\nplt.scatter(x,y)\nplt.xlabel('X axis')\nplt.ylabel('Y axis')\nplt.show()"
  },
  {
    "objectID": "01-ml-ecosystem/intro.html",
    "href": "01-ml-ecosystem/intro.html",
    "title": "Introduction",
    "section": "",
    "text": "Python ecosystem for machine learning\n\nPython and its rising use for machine learning.\nSciPy and the functionality it provides with NumPy, Matplotlib and Pandas.\nscikit-learn that provides all of the machine learning algorithms.\n\nPython\nPython is a general purpose interpreted programming language. It is easy to learn and use primarily because the language focuses on readability.\nIt is a popular language in general, consistently appearing in the top 10 programming languages in surveys on StackOverflow 1 . Its a dynamic language and very suited to interactive development and quick prototyping with the power to support the development of large applications. It is also widely used for machine learning and data science because of the excellent library support and because it is a general purpose programming language (unlike R or Matlab).\nSciPy\nSciPy is an ecosystem of Python libraries for mathematics, science and engineering. It is an add-on to Python that you will need for machine learning. The SciPy ecosystem is comprised of the following core modules relevant to machine learning:  NumPy: A foundation for SciPy that allows you to efficiently work with data in arrays.  Matplotlib: Allows you to create 2D charts and plots from data.  Pandas: Tools and data structures to organize and analyze your data.\nTo be effective at machine learning in Python you must install and become familiar with SciPy. Specifically:  You will prepare your data as NumPy arrays for modeling in machine learning algorithms.  You will use Matplotlib (and wrappers of Matplotlib in other frameworks) to create plots and charts of your data.  You will use Pandas to load, explore, and better understand your data.\nscikit-learn\nThe scikit-learn library is how you can develop and practice machine learning in Python. It is built upon and requires the SciPy ecosystem. The name scikit suggests that it is a SciPy plug-in or toolkit. The focus of the library is machine learning algorithms for classification, regression, clustering and more. It also provides tools for related tasks such as evaluating models, tuning parameters and pre-processing data.\n\nAnalyze Data\nch 3, 4, 5\n\n\nPrepare Data\nch 6, 7\n\n\nEvaluate Algorithms\nch 8, 9, 10, 11, 12, 13\n\n\nImprove Results\nch 14, 15\n\n\nPresent Results\nch 16, 17"
  },
  {
    "objectID": "01-ml-ecosystem/panda.html",
    "href": "01-ml-ecosystem/panda.html",
    "title": "Pandas",
    "section": "",
    "text": "Pandas provides data structures and functionality to quickly manipulate and analyze data. The key to understanding Pandas for machine learning is understanding the Series and DataFrame data structures.\n\nimport numpy\nimport pandas\n\nSeries\nA series is a one dimensional array of data where the rows are labeled using a time axis.\n\nmyarray = numpy.array([1,2,3])\nrowname = ['a', 'b', 'c']\n\n\nmyseries = pandas.Series(myarray, index=rowname)\n\n\nprint(myseries)\n\na    1\nb    2\nc    3\ndtype: int64\n\n\n\nprint(myseries[0])\n\n1\n\n\n\nprint(myseries['a'])\n\n1\n\n\nDataFrame\n\nmyarray = numpy.array([[1,2,3], [4,5,6]])\n\n\nrows = ['a', 'b']\n\n\ncolumns = ['one', 'two', 'three']\n\n\nmydataframe = pandas.DataFrame(myarray, index=rows, columns=columns)\n\n\nprint(mydataframe)\n\n   one  two  three\na    1    2      3\nb    4    5      6\n\n\n\nprint(f'Method 1:\\nColumn one:\\n{mydataframe.one}')\n\nMethod 1:\nColumn one:\na    1\nb    4\nName: one, dtype: int64\n\n\n\nprint('Method 2:\\nColumn one:\\n%s' % mydataframe['one'])\n\nMethod 2:\nColumn one:\na    1\nb    4\nName: one, dtype: int64"
  },
  {
    "objectID": "01-ml-ecosystem/datasets.html",
    "href": "01-ml-ecosystem/datasets.html",
    "title": "machine-learning",
    "section": "",
    "text": "Datasets\n\nIris Flower Dataset\nhttps://www.kaggle.com/datasets/arshid/iris-flower-dataset\nBoston Housing Dataset\nhttps://www.kaggle.com/code/prasadperera/the-boston-housing-dataset/data\nOil Spill Dataset\nhttps://www.kaggle.com/datasets/ashrafkhan94/oil-spill?select=oil-spill.csv\nHorse Colic Dataset\nhttps://raw.githubusercontent.com/jbrownlee/Datasets/master/horse-colic.csv\nhttps://www.kaggle.com/datasets/uciml/horse-colic\nBreast Cancer Categorical Dataset\nhttps://raw.githubusercontent.com/jbrownlee/Datasets/master/breast-cancer.csv\nPima Indians Dataset\nThe Pima Indians dataset is used to demonstrate data loading in this lesson. It will also be used in many of the lessons to come. This dataset describes the medical records for Pima Indians and whether or not each patient will have an onset of diabetes within five years. As such it is a classification problem. It is a good dataset for demonstration because all of the input attributes are numeric and the output variable to be predicted is binary (0 or 1)."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine Learning",
    "section": "",
    "text": "This file will become your README and also the index of your documentation."
  },
  {
    "objectID": "04-ensemble-learning/boosting.html",
    "href": "04-ensemble-learning/boosting.html",
    "title": "Boosting",
    "section": "",
    "text": "from matplotlib import pyplot\nfrom numpy import arange, asarray\nfrom sklearn.datasets import make_classification, make_regression\nfrom sklearn.model_selection import RepeatedStratifiedKFold, RepeatedKFold, cross_val_score"
  },
  {
    "objectID": "04-ensemble-learning/boosting.html#evaluate-adaboost-ensembles",
    "href": "04-ensemble-learning/boosting.html#evaluate-adaboost-ensembles",
    "title": "Boosting",
    "section": "Evaluate AdaBoost Ensembles",
    "text": "Evaluate AdaBoost Ensembles\nAdaBoost for Classification\n\nfrom sklearn.ensemble import AdaBoostClassifier\n\n\nX , y = make_clas_dataset()\n\n\nmodel = AdaBoostClassifier()\n\n\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n\n\nresults = cross_val_score(model, X, y, scoring='accuracy', cv=cv)\n\n\nprint(f'{results.mean(): .3f} ({results.std():.3f})')\n\n 0.806 (0.041)\n\n\n\nmodel.fit(X, y)\n\nAdaBoostClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.AdaBoostClassifierAdaBoostClassifier()\n\n\n\nrow = [-3.47224758, 1.95378146, 0.04875169, -0.91592588, -3.54022468, 1.96405547,\n-7.72564954, -2.64787168, -1.81726906, -1.67104974, 2.33762043, -4.30273117, 0.4839841,\n-1.28253034, -10.6704077, -0.7641103, -3.58493721, 2.07283886, 0.08385173, 0.91461126]\n\n\nyhat = model.predict([row])\n\n\nprint(f'{yhat[0]}')\n\n0\n\n\nAdaBoost for Regression\n\nfrom sklearn.ensemble import AdaBoostRegressor\n\n\nX,y = make_reg_dataset()\n\n\nmodel = AdaBoostRegressor()\n\n\ncv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n\n\nresults = cross_val_score(model, X, y, scoring='neg_mean_absolute_error' ,cv=cv)\nprint(f'{results.mean():.3f} ({results.std():0.3f})')\n\n-73.060 (4.739)\n\n\n\nmodel.fit(X,y)\n\nAdaBoostRegressor()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.AdaBoostRegressorAdaBoostRegressor()\n\n\n\nrow = [1.20871625, 0.88440466, -0.9030013, -0.22687731, -0.82940077, -1.14410988,\n1.26554256, -0.2842871, 1.43929072, 0.74250241, 0.34035501, 0.45363034, 0.1778756,\n-1.75252881, -1.33337384, -1.50337215, -0.45099008, 0.46160133, 0.58385557, -1.79936198]\n\n\nyhat = model.predict([row])\nprint(f'{yhat[0]}')\n\n-20.370316817882475"
  },
  {
    "objectID": "04-ensemble-learning/boosting.html#adaboost-hyperparameters",
    "href": "04-ensemble-learning/boosting.html#adaboost-hyperparameters",
    "title": "Boosting",
    "section": "AdaBoost Hyperparameters",
    "text": "AdaBoost Hyperparameters\n\nfrom sklearn.ensemble import AdaBoostClassifier\n\nExplore Number of Trees\n\ndef get_models():\n    models = dict()\n    n_trees = [10, 50, 100]\n    for n in n_trees:\n        models[str(n)] = AdaBoostClassifier(n_estimators=n)\n    return models\n\n\nX,y = make_clas_dataset()\n\n\nmodels = get_models()\nmodels\n\n{'10': AdaBoostClassifier(n_estimators=10),\n '50': AdaBoostClassifier(),\n '100': AdaBoostClassifier(n_estimators=100)}\n\n\n\nshow_results(X, y, models)\n\n10: 0.773 (0.039)\n50: 0.806 (0.041)\n100: 0.801 (0.032)\n\n\n\n\n\nExplore Weak Learner\n\nfrom sklearn.tree import DecisionTreeClassifier\n\n\ndef get_models():\n    models = dict()\n    for i in range(1,5):\n        base = DecisionTreeClassifier(max_depth=i)\n        models[str(i)] = AdaBoostClassifier(base_estimator=base)\n    return models\n\n\nX, y = make_clas_dataset()\n\n\nmodels = get_models()\nmodels\n\n{'1': AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1)),\n '2': AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=2)),\n '3': AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=3)),\n '4': AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=4))}\n\n\n\nshow_results(X, y, models)\n\n1: 0.806 (0.041)\n2: 0.864 (0.028)\n3: 0.870 (0.028)\n4: 0.897 (0.034)\n\n\n\n\n\nExplore Learning Rate\n\ndef get_models():\n    models = dict()\n    for i in arange(0.1, 0.5, 0.1):\n        key = '%0.1f' % i\n        models[key] = AdaBoostClassifier(learning_rate=i)\n    return models\n\n\nX, y = make_clas_dataset()\n\n\nmodels = get_models()\nmodels\n\n{'0.1': AdaBoostClassifier(learning_rate=0.1),\n '0.2': AdaBoostClassifier(learning_rate=0.2),\n '0.3': AdaBoostClassifier(learning_rate=0.30000000000000004),\n '0.4': AdaBoostClassifier(learning_rate=0.4)}\n\n\n\nshow_results(X, y, models)\n\n0.1: 0.767 (0.049)\n0.2: 0.786 (0.042)\n0.3: 0.802 (0.040)\n0.4: 0.798 (0.037)\n\n\n\n\n\nExplore Alternate Algorithm\n\nfrom sklearn.linear_model import LogisticRegression\n\n\nX, y = make_clas_dataset()\n\n\nmodel = AdaBoostClassifier(base_estimator=LogisticRegression())\n\n\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n\n\nresults = cross_val_score(model, X, y)\nprint(f'{results.mean(): 0.3f} ({results.std():.3f})')\n\n 0.793 (0.034)\n\n\nGrid Search Hyperparameters\n\nfrom sklearn.model_selection import GridSearchCV\n\n\nX, y = make_clas_dataset()\n\n\nmodel = AdaBoostClassifier()\n\n\ngrid = dict()\ngrid['n_estimators'] = [10, 50, 100]\ngrid['learning_rate'] = [0.0001, 0.001, 0.01]\n\n\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n\n\ngrid_search = GridSearchCV(estimator=model, param_grid=grid, scoring='accuracy',cv=cv)\n\n\ngrid_result = grid_search.fit(X, y)\n\n\nprint(f'Best {grid_result.best_score_} using ({grid_result.best_params_})')\n\nBest 0.715 using ({'learning_rate': 0.01, 'n_estimators': 100})\n\n\n\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\n\n\nfor mean, stdev, param in zip(means, stds, params):\n    print(f'{mean:0.3f} ({stdev:0.3f}) with {param}')\n\n0.646 (0.036) with {'learning_rate': 0.0001, 'n_estimators': 10}\n0.647 (0.037) with {'learning_rate': 0.0001, 'n_estimators': 50}\n0.647 (0.037) with {'learning_rate': 0.0001, 'n_estimators': 100}\n0.647 (0.037) with {'learning_rate': 0.001, 'n_estimators': 10}\n0.647 (0.038) with {'learning_rate': 0.001, 'n_estimators': 50}\n0.654 (0.046) with {'learning_rate': 0.001, 'n_estimators': 100}\n0.648 (0.042) with {'learning_rate': 0.01, 'n_estimators': 10}\n0.672 (0.046) with {'learning_rate': 0.01, 'n_estimators': 50}\n0.715 (0.053) with {'learning_rate': 0.01, 'n_estimators': 100}"
  },
  {
    "objectID": "04-ensemble-learning/boosting.html#evaluate-gradient-boosting-ensembles",
    "href": "04-ensemble-learning/boosting.html#evaluate-gradient-boosting-ensembles",
    "title": "Boosting",
    "section": "Evaluate Gradient Boosting Ensembles",
    "text": "Evaluate Gradient Boosting Ensembles\nGradient Boosting for Classification\n\nfrom sklearn.ensemble import GradientBoostingClassifier\n\n\nX, y = make_clas_dataset()\n\n\nmodel = GradientBoostingClassifier()\n\n\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n\n\nresults = cross_val_score(model, X, y, scoring='accuracy', cv=cv)\nprint(f'{results.mean(): 0.3f} ({results.std(): 0.3f})')\n\n 0.888 ( 0.032)\n\n\n\nmodel.fit(X, y)\n\nGradientBoostingClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GradientBoostingClassifierGradientBoostingClassifier()\n\n\n\nrow = [0.2929949, -4.21223056, -1.288332, -2.17849815, -0.64527665, 2.58097719, 0.28422388,\n-7.1827928, -1.91211104, 2.73729512, 0.81395695, 3.96973717, -2.66939799, 3.34692332,\n4.19791821, 0.99990998, -0.30201875, -4.43170633, -2.82646737, 0.44916808]\n\n\nyhat = model.predict([row])\nprint(yhat[0])\n\n0\n\n\nGradient Boosting for Regression\n\nfrom sklearn.ensemble import GradientBoostingRegressor\n\n\nX, y = make_reg_dataset(random_state=7)\n\n\nmodel = GradientBoostingRegressor()\n\n\ncv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n\n\nresults = cross_val_score(model, X, y, scoring='neg_mean_absolute_error', cv=cv)\nprint(f'{results.mean(): 0.3f} ({results.std(): .3f})')\n\n\nmodel.fit(X, y)\n\n\nrow = [0.20543991, -0.97049844, -0.81403429, -0.23842689, -0.60704084, -0.48541492,\n0.53113006, 2.01834338, -0.90745243, -1.85859731, -1.02334791, -0.6877744, 0.60984819,\n-0.70630121, -1.29161497, 1.32385441, 1.42150747, 1.26567231, 2.56569098, -0.11154792]\n\n\nyhat = model.predict([row])\nprint(f'{yhat[0]: 0.3f}')"
  },
  {
    "objectID": "04-ensemble-learning/boosting.html#gradient-boosting-hyperparameters",
    "href": "04-ensemble-learning/boosting.html#gradient-boosting-hyperparameters",
    "title": "Boosting",
    "section": "Gradient Boosting Hyperparameters",
    "text": "Gradient Boosting Hyperparameters\n\nfrom sklearn.ensemble import GradientBoostingClassifier\n\nExplore Number of Trees\n\ndef get_models():\n    models = dict()\n    n_trees = [10, 50, 100]\n    for n in n_trees:\n        models[str(n)] = GradientBoostingClassifier(n_estimators=n)\n    return models\n\n\nX, y = make_clas_dataset()\n\n\nmodels = get_models()\nmodels\n\n\nshow_results(X, y, models)\n\nExplore Number of Samples\n\ndef get_models():\n    models = dict()\n    for i in arange(0.1, 0.5, 0.1):\n        key = '%0.1f' % i\n        models[key] = GradientBoostingClassifier(subsample=i)\n    return models\n\n\nX, y = make_clas_dataset()\n\n\nmodels = get_models()\nmodels\n\n\nshow_results(X, y, models)\n\nExplore Number of Features\n\ndef get_models():\n    models = dict()\n    for i in range(1, 5):\n        models[str(i)] = GradientBoostingClassifier(max_features=i)\n    return models\n\n\nX, y = make_clas_dataset()\n\n\nmodels = get_models()\n\n\nshow_results(X, y, models)\n\nExplore Learning Rate\n\ndef get_models():\n    models = dict()\n    for i in [0.0001, 0.001, 0.01]:\n        key = '%.4f' % i\n        models[key] = GradientBoostingClassifier(learning_rate=i)\n    return models\n\n\nX, y = make_clas_dataset()\n\n\nmodels = get_models()\nmodels\n\n\nshow_results(X, y, models)\n\nExplore Tree Depth\n\ndef get_models():\n    models = dict()\n    for i in range(1,5):\n        models[str(i)] = GradientBoostingClassifier(max_depth=i)\n    return models\n\n\nX, y = make_clas_dataset()\n\n\nmodels = get_models()\nmodels\n\n\nshow_results(X, y, models)\n\nGrid Search Hyperparameters\n\nfrom sklearn.model_selection import GridSearchCV\n\n\nX, y = make_clas_dataset()\n\n\nmodel = GradientBoostingClassifier()\n\n\ngrid = dict()\ngrid['n_estimators'] = [10, 50]\ngrid['learning_rate'] = [0.0001, 0.001]\ngrid['subsample'] = [0.5, 0.7]\ngrid['max_depth'] = [3, 7]\ngrid\n\n\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n\n\ngrid_search = GridSearchCV(estimator=model, param_grid=grid, cv=cv, scoring='accuracy')\n\n\ngrid_result = grid_search.fit(X, y )\n\n\nprint(f'Best Result: {grid_result.best_score_: .3f} using {grid_result.best_params_}')\n\n\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\n\n\nfor mean, stdev, param in zip(means, stds, params):\n    print(f'{mean: .3f} ({stdev: .3f}) with {param}')"
  },
  {
    "objectID": "04-ensemble-learning/boosting.html#evaluate-xgboost-ensembles",
    "href": "04-ensemble-learning/boosting.html#evaluate-xgboost-ensembles",
    "title": "Boosting",
    "section": "Evaluate XGBoost Ensembles",
    "text": "Evaluate XGBoost Ensembles\n\nfrom xgboost import XGBClassifier\n\nXGBoost Ensemble for Classification\n\nX, y = make_clas_dataset()\n\n\nmodel = XGBClassifier()\n\n\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n\n\nresults = cross_val_score(model, X, y, scoring='accuracy', cv=cv)\n\n\nprint(f'{results.mean(): .3f} ({results.std(): .3f})')\n\n\nmodel.fit(X, y)\n\n\nrow = [0.2929949, -4.21223056, -1.288332, -2.17849815, -0.64527665, 2.58097719, 0.28422388,\n-7.1827928, -1.91211104, 2.73729512, 0.81395695, 3.96973717, -2.66939799, 3.34692332,\n4.19791821, 0.99990998, -0.30201875, -4.43170633, -2.82646737, 0.44916808]\n\n\nyhat = model.predict(asarray([row]))\nprint(f'{yhat[0]}')\n\nXGBoost Ensemble for Regression\n\nfrom xgboost import XGBRegressor\n\n\nX, y = make_reg_dataset()\n\n\nmodel = XGBRegressor()\n\n\nmodel.fit(X, y)\n\n\ncv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n\n\nresults = cross_val_score(model, X, y, scoring='neg_mean_absolute_error', cv=cv)\nprint(f'{results.mean():.3f} ({results.std(): .3f})')\n\n\nmodel.fit(X, y)\n\n\nrow = [0.20543991, -0.97049844, -0.81403429, -0.23842689, -0.60704084, -0.48541492,\n0.53113006, 2.01834338, -0.90745243, -1.85859731, -1.02334791, -0.6877744, 0.60984819,\n-0.70630121, -1.29161497, 1.32385441, 1.42150747, 1.26567231, 2.56569098, -0.11154792]\n\n\nyhat = model.predict(asarray([row]))\nprint(f'{yhat[0]}')"
  },
  {
    "objectID": "04-ensemble-learning/boosting.html#xgboost-hyperparameters",
    "href": "04-ensemble-learning/boosting.html#xgboost-hyperparameters",
    "title": "Boosting",
    "section": "XGBoost Hyperparameters",
    "text": "XGBoost Hyperparameters\n\nfrom xgboost import XGBClassifier\n\nExplore Number of Trees\n\ndef get_models():\n    models = dict()\n    n_trees = [10, 50, 100]\n    for n in n_trees:\n        models[str(n)] = XGBClassifier(n_estimators=n)\n    return models\n\n\nX, y = make_clas_dataset()\n\n\nmodels = get_models()\nmodels\n\n{'10': XGBClassifier(base_score=None, booster=None, callbacks=None,\n               colsample_bylevel=None, colsample_bynode=None,\n               colsample_bytree=None, early_stopping_rounds=None,\n               enable_categorical=False, eval_metric=None, feature_types=None,\n               gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n               interaction_constraints=None, learning_rate=None, max_bin=None,\n               max_cat_threshold=None, max_cat_to_onehot=None,\n               max_delta_step=None, max_depth=None, max_leaves=None,\n               min_child_weight=None, missing=nan, monotone_constraints=None,\n               n_estimators=10, n_jobs=None, num_parallel_tree=None,\n               predictor=None, random_state=None, ...),\n '50': XGBClassifier(base_score=None, booster=None, callbacks=None,\n               colsample_bylevel=None, colsample_bynode=None,\n               colsample_bytree=None, early_stopping_rounds=None,\n               enable_categorical=False, eval_metric=None, feature_types=None,\n               gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n               interaction_constraints=None, learning_rate=None, max_bin=None,\n               max_cat_threshold=None, max_cat_to_onehot=None,\n               max_delta_step=None, max_depth=None, max_leaves=None,\n               min_child_weight=None, missing=nan, monotone_constraints=None,\n               n_estimators=50, n_jobs=None, num_parallel_tree=None,\n               predictor=None, random_state=None, ...),\n '100': XGBClassifier(base_score=None, booster=None, callbacks=None,\n               colsample_bylevel=None, colsample_bynode=None,\n               colsample_bytree=None, early_stopping_rounds=None,\n               enable_categorical=False, eval_metric=None, feature_types=None,\n               gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n               interaction_constraints=None, learning_rate=None, max_bin=None,\n               max_cat_threshold=None, max_cat_to_onehot=None,\n               max_delta_step=None, max_depth=None, max_leaves=None,\n               min_child_weight=None, missing=nan, monotone_constraints=None,\n               n_estimators=100, n_jobs=None, num_parallel_tree=None,\n               predictor=None, random_state=None, ...)}\n\n\n\nshow_results(X, y, models)\n\n10: 0.863 (0.039)\n50: 0.909 (0.033)\n100: 0.909 (0.034)\n\n\n\n\n\nExplore Tree Depth\n\ndef get_models():\n    models = dict()\n    for i in range(1,5):\n        models[str(i)] = XGBClassifier(max_depth=i)\n    return models\n\n\nX, y = make_clas_dataset()\n\n\nmodels = get_models()\nmodels\n\n{'1': XGBClassifier(base_score=None, booster=None, callbacks=None,\n               colsample_bylevel=None, colsample_bynode=None,\n               colsample_bytree=None, early_stopping_rounds=None,\n               enable_categorical=False, eval_metric=None, feature_types=None,\n               gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n               interaction_constraints=None, learning_rate=None, max_bin=None,\n               max_cat_threshold=None, max_cat_to_onehot=None,\n               max_delta_step=None, max_depth=1, max_leaves=None,\n               min_child_weight=None, missing=nan, monotone_constraints=None,\n               n_estimators=100, n_jobs=None, num_parallel_tree=None,\n               predictor=None, random_state=None, ...),\n '2': XGBClassifier(base_score=None, booster=None, callbacks=None,\n               colsample_bylevel=None, colsample_bynode=None,\n               colsample_bytree=None, early_stopping_rounds=None,\n               enable_categorical=False, eval_metric=None, feature_types=None,\n               gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n               interaction_constraints=None, learning_rate=None, max_bin=None,\n               max_cat_threshold=None, max_cat_to_onehot=None,\n               max_delta_step=None, max_depth=2, max_leaves=None,\n               min_child_weight=None, missing=nan, monotone_constraints=None,\n               n_estimators=100, n_jobs=None, num_parallel_tree=None,\n               predictor=None, random_state=None, ...),\n '3': XGBClassifier(base_score=None, booster=None, callbacks=None,\n               colsample_bylevel=None, colsample_bynode=None,\n               colsample_bytree=None, early_stopping_rounds=None,\n               enable_categorical=False, eval_metric=None, feature_types=None,\n               gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n               interaction_constraints=None, learning_rate=None, max_bin=None,\n               max_cat_threshold=None, max_cat_to_onehot=None,\n               max_delta_step=None, max_depth=3, max_leaves=None,\n               min_child_weight=None, missing=nan, monotone_constraints=None,\n               n_estimators=100, n_jobs=None, num_parallel_tree=None,\n               predictor=None, random_state=None, ...),\n '4': XGBClassifier(base_score=None, booster=None, callbacks=None,\n               colsample_bylevel=None, colsample_bynode=None,\n               colsample_bytree=None, early_stopping_rounds=None,\n               enable_categorical=False, eval_metric=None, feature_types=None,\n               gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n               interaction_constraints=None, learning_rate=None, max_bin=None,\n               max_cat_threshold=None, max_cat_to_onehot=None,\n               max_delta_step=None, max_depth=4, max_leaves=None,\n               min_child_weight=None, missing=nan, monotone_constraints=None,\n               n_estimators=100, n_jobs=None, num_parallel_tree=None,\n               predictor=None, random_state=None, ...)}\n\n\n\nshow_results(X, y, models)\n\n1: 0.804 (0.037)\n2: 0.879 (0.031)\n3: 0.910 (0.029)\n4: 0.919 (0.027)\n\n\n\n\n\nExplore Learning Rate\n\ndef get_models():\n    models = dict()\n    rates = [0.0001, 0.001, 0.01]\n    for r in rates:\n        key = '%.4f' % r\n        models[key] = XGBClassifier(eta=r)\n    return models\n\n\nX, y = make_clas_dataset()\n\n\nmodels = get_models()\nmodels\n\n{'0.0001': XGBClassifier(base_score=None, booster=None, callbacks=None,\n               colsample_bylevel=None, colsample_bynode=None,\n               colsample_bytree=None, early_stopping_rounds=None,\n               enable_categorical=False, eta=0.0001, eval_metric=None,\n               feature_types=None, gamma=None, gpu_id=None, grow_policy=None,\n               importance_type=None, interaction_constraints=None,\n               learning_rate=None, max_bin=None, max_cat_threshold=None,\n               max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n               max_leaves=None, min_child_weight=None, missing=nan,\n               monotone_constraints=None, n_estimators=100, n_jobs=None,\n               num_parallel_tree=None, predictor=None, ...),\n '0.0010': XGBClassifier(base_score=None, booster=None, callbacks=None,\n               colsample_bylevel=None, colsample_bynode=None,\n               colsample_bytree=None, early_stopping_rounds=None,\n               enable_categorical=False, eta=0.001, eval_metric=None,\n               feature_types=None, gamma=None, gpu_id=None, grow_policy=None,\n               importance_type=None, interaction_constraints=None,\n               learning_rate=None, max_bin=None, max_cat_threshold=None,\n               max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n               max_leaves=None, min_child_weight=None, missing=nan,\n               monotone_constraints=None, n_estimators=100, n_jobs=None,\n               num_parallel_tree=None, predictor=None, ...),\n '0.0100': XGBClassifier(base_score=None, booster=None, callbacks=None,\n               colsample_bylevel=None, colsample_bynode=None,\n               colsample_bytree=None, early_stopping_rounds=None,\n               enable_categorical=False, eta=0.01, eval_metric=None,\n               feature_types=None, gamma=None, gpu_id=None, grow_policy=None,\n               importance_type=None, interaction_constraints=None,\n               learning_rate=None, max_bin=None, max_cat_threshold=None,\n               max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n               max_leaves=None, min_child_weight=None, missing=nan,\n               monotone_constraints=None, n_estimators=100, n_jobs=None,\n               num_parallel_tree=None, predictor=None, ...)}\n\n\n\nshow_results(X, y, models)\n\n0.0001: 0.796 (0.035)\n0.0010: 0.802 (0.034)\n0.0100: 0.837 (0.034)\n\n\n\n\n\nExplore Sample Size\n\ndef get_models():\n    models = dict()\n    for i in arange(0.1, 0.4, 0.1):\n        key = '%.1f' % i\n        models[key] = XGBClassifier(subsample=i)\n    return models\n\n\nX, y = make_clas_dataset()\n\n\nmodels = get_models()\nmodels\n\n{'0.1': XGBClassifier(base_score=None, booster=None, callbacks=None,\n               colsample_bylevel=None, colsample_bynode=None,\n               colsample_bytree=None, early_stopping_rounds=None,\n               enable_categorical=False, eval_metric=None, feature_types=None,\n               gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n               interaction_constraints=None, learning_rate=None, max_bin=None,\n               max_cat_threshold=None, max_cat_to_onehot=None,\n               max_delta_step=None, max_depth=None, max_leaves=None,\n               min_child_weight=None, missing=nan, monotone_constraints=None,\n               n_estimators=100, n_jobs=None, num_parallel_tree=None,\n               predictor=None, random_state=None, ...),\n '0.2': XGBClassifier(base_score=None, booster=None, callbacks=None,\n               colsample_bylevel=None, colsample_bynode=None,\n               colsample_bytree=None, early_stopping_rounds=None,\n               enable_categorical=False, eval_metric=None, feature_types=None,\n               gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n               interaction_constraints=None, learning_rate=None, max_bin=None,\n               max_cat_threshold=None, max_cat_to_onehot=None,\n               max_delta_step=None, max_depth=None, max_leaves=None,\n               min_child_weight=None, missing=nan, monotone_constraints=None,\n               n_estimators=100, n_jobs=None, num_parallel_tree=None,\n               predictor=None, random_state=None, ...),\n '0.3': XGBClassifier(base_score=None, booster=None, callbacks=None,\n               colsample_bylevel=None, colsample_bynode=None,\n               colsample_bytree=None, early_stopping_rounds=None,\n               enable_categorical=False, eval_metric=None, feature_types=None,\n               gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n               interaction_constraints=None, learning_rate=None, max_bin=None,\n               max_cat_threshold=None, max_cat_to_onehot=None,\n               max_delta_step=None, max_depth=None, max_leaves=None,\n               min_child_weight=None, missing=nan, monotone_constraints=None,\n               n_estimators=100, n_jobs=None, num_parallel_tree=None,\n               predictor=None, random_state=None, ...),\n '0.4': XGBClassifier(base_score=None, booster=None, callbacks=None,\n               colsample_bylevel=None, colsample_bynode=None,\n               colsample_bytree=None, early_stopping_rounds=None,\n               enable_categorical=False, eval_metric=None, feature_types=None,\n               gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n               interaction_constraints=None, learning_rate=None, max_bin=None,\n               max_cat_threshold=None, max_cat_to_onehot=None,\n               max_delta_step=None, max_depth=None, max_leaves=None,\n               min_child_weight=None, missing=nan, monotone_constraints=None,\n               n_estimators=100, n_jobs=None, num_parallel_tree=None,\n               predictor=None, random_state=None, ...)}\n\n\n\nshow_results(X, y, models)\n\n0.1: 0.849 (0.030)\n0.2: 0.885 (0.027)\n0.3: 0.909 (0.031)\n0.4: 0.910 (0.025)\n\n\n\n\n\nExplore Number of Features\n\ndef get_models():\n    models = dict()\n    for i in arange(0.1, 0.3, 0.1):\n        key = '%.1f' % i\n        models[key] = XGBClassifier(colsample_bytree=i)\n    return models\n\n\nX, y = make_clas_dataset()\n\n\nmodels = get_models()\nmodels\n\n{'0.1': XGBClassifier(base_score=None, booster=None, callbacks=None,\n               colsample_bylevel=None, colsample_bynode=None,\n               colsample_bytree=0.1, early_stopping_rounds=None,\n               enable_categorical=False, eval_metric=None, feature_types=None,\n               gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n               interaction_constraints=None, learning_rate=None, max_bin=None,\n               max_cat_threshold=None, max_cat_to_onehot=None,\n               max_delta_step=None, max_depth=None, max_leaves=None,\n               min_child_weight=None, missing=nan, monotone_constraints=None,\n               n_estimators=100, n_jobs=None, num_parallel_tree=None,\n               predictor=None, random_state=None, ...),\n '0.2': XGBClassifier(base_score=None, booster=None, callbacks=None,\n               colsample_bylevel=None, colsample_bynode=None,\n               colsample_bytree=0.2, early_stopping_rounds=None,\n               enable_categorical=False, eval_metric=None, feature_types=None,\n               gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n               interaction_constraints=None, learning_rate=None, max_bin=None,\n               max_cat_threshold=None, max_cat_to_onehot=None,\n               max_delta_step=None, max_depth=None, max_leaves=None,\n               min_child_weight=None, missing=nan, monotone_constraints=None,\n               n_estimators=100, n_jobs=None, num_parallel_tree=None,\n               predictor=None, random_state=None, ...)}\n\n\n\nshow_results(X, y, models)\n\n0.1: 0.830 (0.035)\n0.2: 0.896 (0.026)"
  },
  {
    "objectID": "04-ensemble-learning/boosting.html#evaluate-lightgbm-ensembles",
    "href": "04-ensemble-learning/boosting.html#evaluate-lightgbm-ensembles",
    "title": "Boosting",
    "section": "Evaluate LightGBM Ensembles",
    "text": "Evaluate LightGBM Ensembles\nLightGBM Ensemble for Classification\n\nfrom lightgbm import LGBMClassifier\n\n\nX, y = make_clas_dataset()\n\n\nmodel = LGBMClassifier()\n\n\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n\n\nresults = cross_val_score(model, X, y, scoring='accuracy', cv=cv)\nprint(f'{results.mean(): .3f} ({results.std(): .3f})')\n\n 0.918 ( 0.032)\n\n\n\nmodel.fit(X, y)\n\nLGBMClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LGBMClassifierLGBMClassifier()\n\n\n\nrow = [0.2929949, -4.21223056, -1.288332, -2.17849815, -0.64527665, 2.58097719, 0.28422388,\n-7.1827928, -1.91211104, 2.73729512, 0.81395695, 3.96973717, -2.66939799, 3.34692332,\n4.19791821, 0.99990998, -0.30201875, -4.43170633, -2.82646737, 0.44916808]\n\n\nyhat = model.predict([row])\nprint(f'{yhat[0]}')\n\n0\n\n\nLightGBM Ensemble for Regression\n\nfrom lightgbm import LGBMRegressor\n\n\nX, y = make_reg_dataset()\n\n\nmodel = LGBMRegressor()\n\n\ncv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n\n\nresults = cross_val_score(model, X, y, scoring='neg_mean_absolute_error', cv=cv)\n\n\nprint(f'{results.mean(): .3f} ({results.std(): .3f})')\n\n-42.727 ( 3.651)\n\n\n\nmodel.fit(X, y)\n\nLGBMRegressor()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LGBMRegressorLGBMRegressor()\n\n\n\nrow = [0.20543991, -0.97049844, -0.81403429, -0.23842689, -0.60704084, -0.48541492,\n0.53113006, 2.01834338, -0.90745243, -1.85859731, -1.02334791, -0.6877744, 0.60984819,\n-0.70630121, -1.29161497, 1.32385441, 1.42150747, 1.26567231, 2.56569098, -0.11154792]\n\n\nyhat = model.predict([row])\nprint(f'{yhat[0]}')\n\n-141.25948750929035"
  },
  {
    "objectID": "04-ensemble-learning/boosting.html#lightgbm-hyperparameters",
    "href": "04-ensemble-learning/boosting.html#lightgbm-hyperparameters",
    "title": "Boosting",
    "section": "LightGBM Hyperparameters",
    "text": "LightGBM Hyperparameters\n\nfrom lightgbm import LGBMClassifier\n\nExplore Number of Trees\n\ndef get_models():\n    models = dict()\n    n_trees = [10, 50]\n    for n in n_trees:\n        models[str(n)] = LGBMClassifier(n_estimators = n)\n    return models\n\n\nX, y = make_clas_dataset()\n\n\nmodels = get_models()\nmodels\n\n{'10': LGBMClassifier(n_estimators=10), '50': LGBMClassifier(n_estimators=50)}\n\n\n\nshow_results(X, y, models)\n\n10: 0.829 (0.033)\n50: 0.900 (0.038)\n\n\n\n\n\nExplore Tree Depth\n\ndef get_models():\n    models = dict()\n    for i in range(1,5):\n        models[str(i)] = LGBMClassifier(max_depth=i, num_leaves=2**i)\n    return models\n\n\nX, y = make_clas_dataset()\n\n\nmodels = get_models()\nmodels\n\n{'1': LGBMClassifier(max_depth=1, num_leaves=2),\n '2': LGBMClassifier(max_depth=2, num_leaves=4),\n '3': LGBMClassifier(max_depth=3, num_leaves=8),\n '4': LGBMClassifier(max_depth=4, num_leaves=16)}\n\n\n\nshow_results(X, y, models)\n\n1: 0.788 (0.040)\n2: 0.852 (0.035)\n3: 0.886 (0.032)\n4: 0.901 (0.029)\n\n\n\n\n\nExplore Learning Rate\n\ndef get_models():\n    models = dict()\n    rates = [0.0001, 0.001, 0.01]\n    for r in rates:\n        key = '%.4f' % r\n        models[key] = LGBMClassifier(learning_rate=r)\n    return models\n\n\nX, y = make_clas_dataset()\n\n\nmodels = get_models()\nmodels\n\n{'0.0001': LGBMClassifier(learning_rate=0.0001),\n '0.0010': LGBMClassifier(learning_rate=0.001),\n '0.0100': LGBMClassifier(learning_rate=0.01)}\n\n\n\nshow_results(X, y, models)\n\n0.0001: 0.779 (0.036)\n0.0010: 0.788 (0.039)\n0.0100: 0.826 (0.035)\n\n\n\n\n\nExplore Boosting Type\n\ndef get_models():\n    models = dict()\n    types = ['gbdt', 'dart', 'goss']\n    for t in types:\n        models[t] = LGBMClassifier(boosting_type=t)\n    return models\n\n\nX, y = make_clas_dataset()\n\n\nmodels = get_models()\n\n\nshow_results(X, y, models)\n\ngbdt: 0.918 (0.032)\ndart: 0.893 (0.029)\ngoss: 0.898 (0.032)"
  },
  {
    "objectID": "04-ensemble-learning/intro.html",
    "href": "04-ensemble-learning/intro.html",
    "title": "Introduction",
    "section": "",
    "text": "Ensemble learning algorithms are those techniques that combine the predictions of two or more machine learning algorithms with the goal of improving predictive skill. Ensemble learning algorithms are a more advanced subfield of machine learning, often turned to on machine learning projects when predictive performance is the most important objective. As such, ensembles are widely used by top participants and winners of competitive machine learning competitions."
  },
  {
    "objectID": "04-ensemble-learning/intro.html#what-is-ensemble-learning",
    "href": "04-ensemble-learning/intro.html#what-is-ensemble-learning",
    "title": "Introduction",
    "section": "What is Ensemble Learning",
    "text": "What is Ensemble Learning\nMany decisions we make in life are based on the opinions of multiple other people. This includes choosing a book to read based on reviews, choosing a course of action based on the advice of multiple medical doctors, and determining guilt. Often, decision making by a group of individuals results in a better outcome than a decision made by any one member of the group. This is generally referred to as the wisdom of the crowd. We can achieve a similar result by combining the predictions of multiple machine learning models for regression and classification predictive modeling problems.\nApplied machine learning often involves fitting and evaluating models on a dataset. Given that we cannot know which model will perform best on the dataset beforehand, this may involve a lot of trial and error until we find a model that performs well or best for our project. This is akin to making a decision using a single expert. Perhaps the best expert we can find. A complementary approach is to prepare multiple different models, then combine their predictions. This is called an ensemble machine learning model, or simply an ensemble, and the process of finding a well-performing ensemble model is referred to as ensemble learning."
  },
  {
    "objectID": "04-ensemble-learning/bagging.html",
    "href": "04-ensemble-learning/bagging.html",
    "title": "Bagging",
    "section": "",
    "text": "from matplotlib import pyplot\nfrom numpy import arange\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import cross_val_score, RepeatedStratifiedKFold, RepeatedKFold\nfrom sklearn.ensemble import BaggingClassifier, BaggingRegressor\nfrom sklearn.datasets import make_classification, make_regression"
  },
  {
    "objectID": "04-ensemble-learning/bagging.html#evaluate-bagging-ensembles",
    "href": "04-ensemble-learning/bagging.html#evaluate-bagging-ensembles",
    "title": "Bagging",
    "section": "Evaluate Bagging Ensembles",
    "text": "Evaluate Bagging Ensembles\nBagging for Classification\n\nrow = [-3.47224758, 1.95378146, 0.04875169, -0.91592588, -3.54022468, 1.96405547,\n-7.72564954, -2.64787168, -1.81726906, -1.67104974, 2.33762043, -4.30273117, 0.4839841,\n-1.28253034, -10.6704077, -0.7641103, -3.58493721, 2.07283886, 0.08385173, 0.91461126]\n\n\nfrom sklearn.ensemble import BaggingClassifier\n\n\nX, Y = make_clas_dataset()\nX.shape, Y.shape\n\n((1000, 20), (1000,))\n\n\n\nmodel = BaggingClassifier()\n\n\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n\n\nresults = cross_val_score(model, X, Y, scoring='accuracy', cv=cv)\nprint(f'Accuracy: {results.mean(): .3f} ({results.std(): .3f})')\n\nAccuracy:  0.865 ( 0.037)\n\n\nWe can also use the Bagging model as a final model and make predictions for classification.\n\nmodel.fit(X, Y)\n\nBaggingClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.BaggingClassifierBaggingClassifier()\n\n\n\nyhat = model.predict([row])\nyhat\n\narray([1])\n\n\n\nprint(f'Predicted Class: {yhat[0]}')\n\nPredicted Class: 1\n\n\nBagging for Regression\n\nfrom sklearn.ensemble import BaggingRegressor\n\n\nX, Y = make_reg_dataset()\n\n\nmodel = BaggingRegressor()\n\n\ncv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n\n\nresults = cross_val_score(model, X, Y, \n                          scoring='neg_mean_absolute_error', cv=cv)\n\n\nprint(f'MAE: {results.mean(): .3f} ({results.std(): .3f})')\n\nMAE: -99.772 ( 10.417)\n\n\n\nmodel.fit(X, Y)\n\nBaggingRegressor()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.BaggingRegressorBaggingRegressor()\n\n\n\nyhat = model.predict([row])\n\n\nprint(f'Prediction: {yhat[0]}')\n\nPrediction: -88.26220906961095"
  },
  {
    "objectID": "04-ensemble-learning/bagging.html#bagging-hyperparameters",
    "href": "04-ensemble-learning/bagging.html#bagging-hyperparameters",
    "title": "Bagging",
    "section": "Bagging Hyperparameters",
    "text": "Bagging Hyperparameters\nIn this section, we will take a closer look at some of the hyperparameters you should consider tuning for the Bagging ensemble and their effect on model performance.\nExplore Number of Trees\nAn important hyperparameter for the Bagging algorithm is the number of decision trees used in the ensemble. Typically, the number of trees is increased until the model performance stabilizes. Intuition might suggest that more trees will lead to overfitting, although this is not the case. Bagging and related ensembles of decision trees algorithms (like random forest) appear to be somewhat immune to overfitting the training dataset given the stochastic nature of the learning algorithm.\n\ndef get_models(param):\n    models = dict()\n    \n    if param == 'n_estimators':\n        n_trees = [10,50]\n        for n in n_trees:\n            models[str(n)] = BaggingClassifier(n_estimators=n)\n            \n    if param == 'max_samples':\n        for i in arange(0.1, 1.1, 0.1):\n            key = '%.1f' % i\n            models[key] = BaggingClassifier(max_samples=i)\n            \n    if param == 'base_estimator':\n        models = dict()\n        for i in range(1,21):\n            base = KNeighborsClassifier(n_neighbors=i)\n            models[str(i)] = BaggingClassifier(base_estimator=base)\n            \n    return models\n\n\nX, Y = make_clas_dataset()\n\n\nmodels = get_models('n_estimators')\n\n\nshow_results(models, X, Y)\n\n10:  0.858 ( 0.043)\n50:  0.875 ( 0.038)\n\n\n\n\n\n\npyplot.boxplot(results,  showmeans=True)\npyplot.show()\n\n\n\n\nExplore Number of Samples\nThe size of the bootstrap sample can also be varied. The default is to create a bootstrap sample that has the same number of examples as the original dataset. Using a smaller dataset can increase the variance of the resulting decision trees and could result in better overall performance\n\nX, Y = make_clas_dataset()\n\n\nmodels = get_models(param='max_samples')\n\n\nshow_results(models, X, Y)\n\n0.1:  0.799 ( 0.045)\n0.2:  0.833 ( 0.038)\n0.3:  0.839 ( 0.036)\n0.4:  0.843 ( 0.043)\n0.5:  0.849 ( 0.040)\n0.6:  0.859 ( 0.035)\n0.7:  0.861 ( 0.033)\n0.8:  0.861 ( 0.038)\n0.9:  0.854 ( 0.042)\n1.0:  0.865 ( 0.037)\n\n\n\n\n\nExplore Alternate Algorithm\nDecision trees are the most common algorithm used in a bagging ensemble. The reason for this is that they are easy to configure to have a high variance and because they perform well in general. Other algorithms can be used with bagging and must be configured to have a modestly high variance. One example is the k-nearest neighbors algorithm where the k value can be set to a low value. The algorithm used in the ensemble is specified via the base estimator argument and must be set to an instance of the algorithm and algorithm configuration to use.\n\nX, Y = make_clas_dataset()\n\n\nmodels = get_models('base_estimator')\n\n\nshow_results(models, X, Y)\n\n1:  0.884 ( 0.035)\n2:  0.884 ( 0.034)\n3:  0.887 ( 0.038)\n4:  0.886 ( 0.035)\n5:  0.886 ( 0.037)\n6:  0.882 ( 0.038)\n7:  0.876 ( 0.041)\n8:  0.879 ( 0.038)\n9:  0.876 ( 0.035)\n10:  0.876 ( 0.033)\n11:  0.875 ( 0.035)\n12:  0.872 ( 0.031)\n13:  0.876 ( 0.032)\n14:  0.878 ( 0.029)\n15:  0.872 ( 0.031)\n16:  0.871 ( 0.036)\n17:  0.874 ( 0.032)\n18:  0.873 ( 0.039)\n19:  0.873 ( 0.035)\n20:  0.873 ( 0.036)"
  },
  {
    "objectID": "04-ensemble-learning/bagging.html#evaluate-random-subspace-ensembles",
    "href": "04-ensemble-learning/bagging.html#evaluate-random-subspace-ensembles",
    "title": "Bagging",
    "section": "Evaluate Random Subspace Ensembles",
    "text": "Evaluate Random Subspace Ensembles\nRandom Subspace Ensemble for Classification\n\nX, Y = make_clas_dataset()\n\n\nmodel = BaggingClassifier(bootstrap=False, max_features=10)\n\n\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n\n\nresults = cross_val_score(model, X, Y, scoring='accuracy', cv=cv)\nprint(f'{results.mean(): 0.3f} ({results.std():.3f})')\n\n\nmodel.fit(X,Y)\n\n\nyhat = model.predict([row])\nprint(yhat[0])\n\nRandom Subspace Ensemble for Regression\n\nX, Y = make_reg_dataset()\n\n\nmodel = BaggingRegressor(bootstrap=False, max_features=10)\n\n\ncv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n\n\nresults = cross_val_score(model, X, Y, scoring='neg_mean_absolute_error', cv=cv)\n\n\nprint(f'{results.mean():0.3f} ({results.std():0.3f})')\n\n\nmodel.fit(X, Y)\n\n\nyhat = model.predict([reg_row])\nprint(f'{int(yhat[0])}')"
  },
  {
    "objectID": "04-ensemble-learning/bagging.html#random-subspace-ensemble-hyperparameters",
    "href": "04-ensemble-learning/bagging.html#random-subspace-ensemble-hyperparameters",
    "title": "Bagging",
    "section": "Random Subspace Ensemble Hyperparameters",
    "text": "Random Subspace Ensemble Hyperparameters\nExplore Number of Trees\n\ndef get_models():\n    models = dict()\n    n_trees = [10,50,100]\n    for n in n_trees:\n        models[str(n)] = BaggingClassifier(n_estimators=n, bootstrap=False, max_features=10)\n    return models\n\n\nX, Y = make_clas_dataset()\n\n\nmodels = get_models()\nmodels\n\n\nshow_results(models, X, Y)\n\nExplore Number of Features\n\ndef get_models():\n    models = dict()\n    for n in range(1,5):\n        models[str(n)] = BaggingClassifier(n_estimators=100, bootstrap=False, max_features=n)\n    return models\n\n\nX,Y = make_clas_dataset()\n\n\nmodels = get_models()\n\n\nshow_results(models, X, Y)\n\nExplore Alternate Algorithm\n\nX, Y = make_clas_dataset()\n\n\nmodel = BaggingClassifier(base_estimator=KNeighborsClassifier(), bootstrap=False, max_features=10)\n\n\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n\n\nresults = cross_val_score(model, X, Y, scoring='accuracy', cv=cv)\nprint(f'Mean Accuracy: {results.mean():.3f} ({results.std():.3f})')"
  },
  {
    "objectID": "04-ensemble-learning/bagging.html#single-feature-selection-method-ensembles",
    "href": "04-ensemble-learning/bagging.html#single-feature-selection-method-ensembles",
    "title": "Bagging",
    "section": "Single Feature Selection Method Ensembles",
    "text": "Single Feature Selection Method Ensembles\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import VotingClassifier\n\nANOVA F-statistic Ensemble\n\nfrom sklearn.feature_selection import SelectKBest, f_classif\n\n\ndef get_ensemble(n_features):\n    models = []\n    for i in range(1, n_features+1):\n        feat_selec = SelectKBest(score_func=f_classif, k=i)\n        model = DecisionTreeClassifier()\n        pipe = Pipeline([('fs', feat_selec), ('m', model)])\n        models.append((str(i), pipe))\n    ensemble = VotingClassifier(estimators=models, voting='hard')\n    return ensemble\n\n\nX, Y = make_clas_dataset()\n\n\nensemble = get_ensemble(X.shape[1])\n\n\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n\n\nresults = cross_val_score(ensemble, X, Y, scoring='accuracy', cv=cv)\nprint(f'{results.mean(): .3f} ({results.std(): .3f})')\n\nMutual Information Ensemble\n\nfrom sklearn.feature_selection import SelectKBest, mutual_info_classif\n\n\ndef get_ensemble(n_features):\n    models = []\n    for i in range(1, n_features+1):\n        fs = SelectKBest(score_func=mutual_info_classif, k=i)\n        model = DecisionTreeClassifier()\n        pipe = Pipeline([('fs', fs), ('m', model)])\n        models.append((str(i), pipe))\n    ensemble = VotingClassifier(estimators=models, voting='hard')\n    return ensemble\n\n\nX,Y = make_clas_dataset()\n\n\nensemble = get_ensemble(X.shape[1])\n\n\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n\n\nresults = cross_val_score(ensemble, X, Y, scoring='accuracy', cv=cv)\nprint(f'{results.mean(): .3f} ({results.std(): .3f})')\n\nRecursive Feature Selection Ensemble\n\nfrom sklearn.feature_selection import RFE\n\n\ndef get_ensemble(n_fatures):\n    models = []\n    for i in range(1, n_fatures+1):\n        fs = RFE(estimator=DecisionTreeClassifier(), n_features_to_select=i)\n        model = DecisionTreeClassifier()\n        pipe = Pipeline([('fs', fs), ('m', model)])\n        models.append((str(i), pipe))\n    ensemble = VotingClassifier(estimators=models, voting='hard')\n    return ensemble\n\n\nX, Y = make_clas_dataset()\n\n\nensemble = get_ensemble(X.shape[1])\n\n\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n\n\nresults = cross_val_score(ensemble, X, Y, scoring='accuracy', cv=cv)\nprint(f'{results.mean(): .3f} ({results.std(): .3f})')"
  },
  {
    "objectID": "04-ensemble-learning/bagging.html#combined-feature-selection-ensembles",
    "href": "04-ensemble-learning/bagging.html#combined-feature-selection-ensembles",
    "title": "Bagging",
    "section": "Combined Feature Selection Ensembles",
    "text": "Combined Feature Selection Ensembles\nEnsemble With Fixed Number of Features\nEnsemble With Contiguous Number of Features"
  },
  {
    "objectID": "04-ensemble-learning/bagging.html#evaluate-random-forest-ensembles",
    "href": "04-ensemble-learning/bagging.html#evaluate-random-forest-ensembles",
    "title": "Bagging",
    "section": "Evaluate Random Forest Ensembles",
    "text": "Evaluate Random Forest Ensembles\nRandom Forest for Classification\n\nfrom sklearn.ensemble import RandomForestClassifier\n\n\nX, Y = make_clas_dataset(random_state=3)\n\n\nmodel = RandomForestClassifier()\n\n\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n\n\nresults = cross_val_score(model, X, Y, scoring='accuracy', cv=cv)\nprint(f'Mean Accuracy: {results.mean():.3f} ({results.std():.3f})')\n\n\nmodel.fit(X, Y)\n\n\nrow = [-8.52381793, 5.24451077, -12.14967704, -2.92949242, 0.99314133, 0.67326595,\n-0.38657932, 1.27955683, -0.60712621, 3.20807316, 0.60504151, -1.38706415, 8.92444588,\n-7.43027595, -2.33653219, 1.10358169, 0.21547782, 1.05057966, 0.6975331, 0.26076035]\n\n\nyhat = model.predict([row])\nprint(yhat[0])\n\nRandom Forest for Regression\n\nfrom sklearn.ensemble import RandomForestRegressor\n\n\nX , Y = make_reg_dataset(random_state=2)\n\n\nmodel = RandomForestRegressor()\n\n\ncv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n\n\nresults = cross_val_score(model, X, Y, scoring='neg_mean_absolute_error', cv=cv)\nprint(f'{results.mean():.3f} ({results.std():.3f})')\n\n\nmodel.fit(X, Y)\n\n\nrow = [-0.89483109, -1.0670149, -0.25448694, -0.53850126, 0.21082105, 1.37435592,\n0.71203659, 0.73093031, -1.25878104, -2.01656886, 0.51906798, 0.62767387, 0.96250155,\n1.31410617, -1.25527295, -0.85079036, 0.24129757, -0.17571721, -1.11454339, 0.36268268]\n\n\nyhat = model.predict([row])\nprint(f'{yhat[0]}')"
  },
  {
    "objectID": "04-ensemble-learning/bagging.html#random-forest-hyperparameters",
    "href": "04-ensemble-learning/bagging.html#random-forest-hyperparameters",
    "title": "Bagging",
    "section": "Random Forest Hyperparameters",
    "text": "Random Forest Hyperparameters\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nExplore Number of Samples\n\nfrom numpy import arange\n\n\ndef get_models():\n    models = dict()\n    for i in arange(0.1, 1.1, 0.1):\n        key = '%.1f' % i\n        if i == 1.0:\n            i = None \n        models[key] = RandomForestClassifier(max_samples=i)\n    return models\n\n\nX , y = make_clas_dataset()\n\n\nmodels = get_models()\n\n\nshow_results(models, X, y)\n\nExplore Number of Features\n\ndef get_models():\n    models = dict()\n    for i in range(1,8):\n        models[str(i)] = RandomForestClassifier(max_features=i)\n    return models\n\n\nX, y = make_clas_dataset()\n\n\nmodels = get_models()\n\n\nshow_results(models, X, y)\n\nExplore Number of Trees\n\ndef get_models():\n    models = dict()\n    n_trees = [10, 50, 100]\n    for n in n_trees:\n        models[str(n)] = RandomForestClassifier(n_estimators=n)\n    return models\n\n\nX, y = make_clas_dataset()\n\n\nmodels = get_models()\nmodels\n\n\nshow_results(models, X, y)\n\nExplore Tree Depth\n\ndef get_models():\n    models = dict()\n    depths = [i for i in range(1,4)] + [None]\n    for n in depths:\n        models[str(n)] = RandomForestClassifier(max_depth=n)\n    return models\n\n\nX, y = make_clas_dataset()\n\n\nmodels = get_models()\nmodels\n\n\nshow_results(models, X, y)"
  },
  {
    "objectID": "04-ensemble-learning/bagging.html#evaluate-extra-trees-ensembles",
    "href": "04-ensemble-learning/bagging.html#evaluate-extra-trees-ensembles",
    "title": "Bagging",
    "section": "Evaluate Extra Trees Ensembles",
    "text": "Evaluate Extra Trees Ensembles\nExtra Trees for Classification\n\nfrom sklearn.ensemble import ExtraTreesClassifier\n\n\nX, y = make_clas_dataset(random_state=4)\n\n\nmodel = ExtraTreesClassifier()\n\n\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n\n\nresults = cross_val_score(model, X, y, scoring='accuracy', cv=cv)\nprint(f'{results.mean(): .3f} ({results.std(): .3f})')\n\n 0.907 ( 0.025)\n\n\n\nmodel.fit(X, y)\n\nExtraTreesClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.ExtraTreesClassifierExtraTreesClassifier()\n\n\n\nrow = [-3.52169364, 4.00560592, 2.94756812, -0.09755101, -0.98835896, 1.81021933,\n-0.32657994, 1.08451928, 4.98150546, -2.53855736, 3.43500614, 1.64660497, -4.1557091,\n-1.55301045, -0.30690987, -1.47665577, 6.818756, 0.5132918, 4.3598337, -4.31785495]\n\n\nyhat = model.predict([row])\nprint(f'{yhat[0]}')\n\n0\n\n\nExtra Trees for Regression\n\nfrom sklearn.ensemble import ExtraTreesRegressor\n\n\nX, y = make_reg_dataset(random_state=3)\n\n\nmodel = ExtraTreesRegressor()\n\n\ncv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n\n\nresults = cross_val_score(model, X, y, scoring='neg_mean_absolute_error', cv=cv)\nprint(f'{results.mean(): .3f} ({results.std(): .3f})')\n\n-69.667 ( 4.984)\n\n\n\nmodel.fit(X, y)\n\nExtraTreesRegressor()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.ExtraTreesRegressorExtraTreesRegressor()\n\n\n\nrow = [-0.56996683, 0.80144889, 2.77523539, 1.32554027, -1.44494378, -0.80834175,\n-0.84142896, 0.57710245, 0.96235932, -0.66303907, -1.13994112, 0.49887995, 1.40752035,\n-0.2995842, -0.05708706, -2.08701456, 1.17768469, 0.13474234, 0.09518152, -0.07603207]\n\n\nyhat = model.predict([row])\nprint(f'{int(yhat[0])}')\n\n53"
  },
  {
    "objectID": "04-ensemble-learning/bagging.html#extra-trees-hyperparameters",
    "href": "04-ensemble-learning/bagging.html#extra-trees-hyperparameters",
    "title": "Bagging",
    "section": "Extra Trees Hyperparameters",
    "text": "Extra Trees Hyperparameters\n\nfrom sklearn.ensemble import ExtraTreesClassifier\n\nExplore Number of Trees\n\ndef get_models():\n    models = dict()\n    n_trees = [10, 50, 100]\n    for n in n_trees:\n        models[str(n)] = ExtraTreesClassifier(n_estimators=n)\n    return models\n\n\nX, y = make_clas_dataset(random_state=4)\n\n\nmodels = get_models()\nmodels\n\n{'10': ExtraTreesClassifier(n_estimators=10),\n '50': ExtraTreesClassifier(n_estimators=50),\n '100': ExtraTreesClassifier()}\n\n\n\nshow_results(models, X, y)\n\n10:  0.855 ( 0.025)\n50:  0.902 ( 0.028)\n100:  0.906 ( 0.023)\n\n\n\n\n\nExplore Number of Features\n\ndef get_models():\n    models = dict()\n    for i in range(1, 5):\n        models[str(i)] = ExtraTreesClassifier(max_features=i)\n    return models\n\n\nX , y = make_clas_dataset(random_state=4)\n\n\nmodels = get_models()\nmodels\n\n{'1': ExtraTreesClassifier(max_features=1),\n '2': ExtraTreesClassifier(max_features=2),\n '3': ExtraTreesClassifier(max_features=3),\n '4': ExtraTreesClassifier(max_features=4)}\n\n\n\nshow_results(models, X, y)\n\n1:  0.895 ( 0.025)\n2:  0.901 ( 0.026)\n3:  0.907 ( 0.028)\n4:  0.904 ( 0.025)\n\n\n\n\n\nExplore Minimum Samples per Split\n\ndef get_models():\n    models = dict()\n    for i in range(2, 5):\n        models[str(i)] = ExtraTreesClassifier(min_samples_split=i)\n    return models\n\n\nX, y = make_clas_dataset(random_state=4)\n\n\nmodels = get_models()\nmodels\n\n{'2': ExtraTreesClassifier(),\n '3': ExtraTreesClassifier(min_samples_split=3),\n '4': ExtraTreesClassifier(min_samples_split=4)}\n\n\n\nshow_results(models, X, y)\n\n2:  0.909 ( 0.026)\n3:  0.913 ( 0.025)\n4:  0.909 ( 0.027)"
  },
  {
    "objectID": "04-ensemble-learning/stacking.html",
    "href": "04-ensemble-learning/stacking.html",
    "title": "Voting Ensemble",
    "section": "",
    "text": "Stacking"
  },
  {
    "objectID": "04-ensemble-learning/stacking.html#voting-ensemble-for-classification",
    "href": "04-ensemble-learning/stacking.html#voting-ensemble-for-classification",
    "title": "Voting Ensemble",
    "section": "Voting Ensemble for Classification",
    "text": "Voting Ensemble for Classification\nHard Voting Ensemble for Classification\n\ndef get_models():\n    models = dict()\n    neighbors = [1, 3, 5, 7, 9]\n    for n in neighbors:\n        key = 'knn' + str(n)\n        models[key] = KNeighborsClassifier(n_neighbors=n)\n    members = [(n,m) for n,m in models.items()]\n    models['hard_voting'] = VotingClassifier(estimators=members, voting='hard')\n    return models\n\n\nX, y = make_clas_dataset()\n\n\nmodels = get_models()\nmodels\n\n\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n\n\nshow_results(X, y, models)\n\n\nmodels = []\nneighbors = [1, 3, 5, 7, 9]\n\n\nfor n in neighbors:\n    models.append(('knn'+str(n), KNeighborsClassifier(n_neighbors=n)))\n\n\nensemble = VotingClassifier(estimators=models, voting='hard')\n\n\nensemble.fit(X, y)\n\n\nrow = [5.88891819, 2.64867662, -0.42728226, -1.24988856, -0.00822, -3.57895574, 2.87938412,\n-1.55614691, -0.38168784, 7.50285659, -1.16710354, -5.02492712, -0.46196105,\n-0.64539455, -1.71297469, 0.25987852, -0.193401, -5.52022952, 0.0364453, -1.960039]\n\n\nyhat = ensemble.predict([row])\nprint(f'{yhat[0]}')\n\nSoft Voting Ensemble for Classification\n\nfrom sklearn.svm import SVC\n\n\ndef get_models():\n    models = dict()\n    for n in range(1,6):\n        key = 'svm' + str(n)\n        models[key] = SVC(probability=True, kernel='poly', degree=n)\n    members = [(n,m) for n,m in models.items()]\n    models['soft_voting'] = VotingClassifier(estimators=members, voting='soft')\n    return models\n\n\nX, y = make_clas_dataset()\n\n\nmodels = get_models()\nmodels\n\n\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n\n\nshow_results(X, y, models, cv, 'accuracy')\n\n\nmodels = []\nfor n in range(1,6):\n    models.append(('svm'+str(n), SVC(probability=True, kernel='poly', degree=n)))\nmodels\n\n\nensemble = VotingClassifier(estimators=models, voting='soft')\n\n\nensemble.fit(X, y)\n\n\nrow = [5.88891819, 2.64867662, -0.42728226, -1.24988856, -0.00822, -3.57895574, 2.87938412,\n       -1.55614691, -0.38168784, 7.50285659, -1.16710354, -5.02492712, -0.46196105,-0.64539455, \n       -1.71297469, 0.25987852, -0.193401, -5.52022952, 0.0364453, -1.960039]\n\n\nyhat = ensemble.predict([row])\nprint(f'{yhat[0]}')"
  },
  {
    "objectID": "04-ensemble-learning/stacking.html#voting-ensemble-for-regression",
    "href": "04-ensemble-learning/stacking.html#voting-ensemble-for-regression",
    "title": "Voting Ensemble",
    "section": "Voting Ensemble for Regression",
    "text": "Voting Ensemble for Regression\n\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import VotingRegressor\n\n\ndef get_models():\n    models = dict()\n    for n in range(1,6):\n        key = 'cart' + str(n)\n        models[key] = DecisionTreeRegressor(max_depth=n)\n    members = [(n,m) for n,m in models.items()]\n    models['voting'] = VotingRegressor(estimators=members)\n    return models\n\n\nX, y = make_reg_dataset()\n\n\nmodels = get_models()\nmodels\n\n\ncv = RepeatedKFold(n_splits=10 ,n_repeats=3, random_state=1)\n\n\nshow_results(X, y, models, 'neg_mean_absolute_error', cv)\n\n\nmodels = []\n\n\nfor n in range(1,6):\n    models.append(('cart'+str(n), DecisionTreeRegressor(max_depth=n)))\nmodels\n\n\nensemble = VotingRegressor(estimators=models)\n\n\nensemble.fit(X, y)\n\n\nrow = [0.59332206, -0.56637507, 1.34808718, -0.57054047, -0.72480487, 1.05648449,\n0.77744852, 0.07361796, 0.88398267, 2.02843157, 1.01902732, 0.11227799, 0.94218853,\n       0.26741783, 0.91458143, -0.72759572, 1.08842814, -0.61450942, -0.69387293, 1.69169009]\n\n\nyhat = ensemble.predict([row])\nprint(f'{yhat[0]}')"
  },
  {
    "objectID": "04-ensemble-learning/stacking.html#weighted-average-ensemble-for-classification",
    "href": "04-ensemble-learning/stacking.html#weighted-average-ensemble-for-classification",
    "title": "Voting Ensemble",
    "section": "Weighted Average Ensemble for Classification",
    "text": "Weighted Average Ensemble for Classification\n\nX, y = make_classification()"
  },
  {
    "objectID": "04-ensemble-learning/stacking.html#weighted-average-ensemble-for-regression",
    "href": "04-ensemble-learning/stacking.html#weighted-average-ensemble-for-regression",
    "title": "Voting Ensemble",
    "section": "Weighted Average Ensemble for Regression",
    "text": "Weighted Average Ensemble for Regression"
  },
  {
    "objectID": "notes.html",
    "href": "notes.html",
    "title": "machine-learning",
    "section": "",
    "text": "ML - Phase 1: Being a good operator\nAt this phase the goal is to learn how to use with the tools.\nOutcome: - learn python ecosystem and scikit-learn library - work through end-to-end applied machine learning projects. - get into the Kaggle competitions\nDataset: - Small - Clean - Benchmark\nClassification: - Binary classification - Balanced classification\n\nML - Phase 2: Become an expert\nAt this stage the goal is to learn how things work? and why things work?\nOutcome: - be able to read and implement papers.\nDL - Phase 3"
  },
  {
    "objectID": "03-data-preparation/data-cleaning.html",
    "href": "03-data-preparation/data-cleaning.html",
    "title": "Data Cleaning",
    "section": "",
    "text": "data_path = '/home/naji/Desktop/github-repos/machine-learning/nbs/0-datasets/data/'"
  },
  {
    "objectID": "03-data-preparation/data-cleaning.html#standard-deviation-method",
    "href": "03-data-preparation/data-cleaning.html#standard-deviation-method",
    "title": "Data Cleaning",
    "section": "Standard Deviation Method",
    "text": "Standard Deviation Method\n\ndata = 5 * randn(10000) + 50\ndata\n\narray([49.38763047, 51.14084909, 48.23847435, ..., 62.04216899,\n       54.41392775, 49.50201845])\n\n\n\ndata_mean, data_std = mean(data), std(data)\n\n\ncut_off = data_std * 3\n\n\nlower, upper = data_mean - cut_off, data_mean + cut_off\n\n\noutliers = [x for x in data if x < lower or x > upper]\nprint(f'Identified outliers: {len(outliers)}')\n\nIdentified outliers: 26\n\n\n\noutliers_removed = [x for x in data if x >= lower and x <= upper]\nprint(f'Non-outlier observations: {len(outliers_removed)}')\n\nNon-outlier observations: 9974"
  },
  {
    "objectID": "03-data-preparation/data-cleaning.html#interquartile-range-method",
    "href": "03-data-preparation/data-cleaning.html#interquartile-range-method",
    "title": "Data Cleaning",
    "section": "Interquartile Range Method",
    "text": "Interquartile Range Method\n\nfrom numpy import percentile\n\n\ndata = 5 * randn(10000) + 50\n\n\nq25, q75 = percentile(data, 25), percentile(data, 75)\n\n\niqr = q75 - q25\n\n\nprint(f'Percentiles: 25th={q25:.3f}, 75th={q75:.3f}, IQR={iqr:.3f}')\n\nPercentiles: 25th=46.567, 75th=53.215, IQR=6.647\n\n\n\ncut_off = iqr * 1.5\n\n\nlower, upper = q25 - cut_off, q75 + cut_off\n\n\noutliers = [x for x in data if x < lower or x > upper]\nprint(f'Identified outliers: {len(outliers)}')\n\nIdentified outliers: 75\n\n\n\noutliers_removed = [x for x in data if x >= lower and x <= upper]\nprint(f'Non-outlier observations: {len(outliers_removed)}')\n\nNon-outlier observations: 9925"
  },
  {
    "objectID": "03-data-preparation/data-cleaning.html#automatic-outlier-detection",
    "href": "03-data-preparation/data-cleaning.html#automatic-outlier-detection",
    "title": "Data Cleaning",
    "section": "Automatic Outlier Detection",
    "text": "Automatic Outlier Detection\n\nfrom pandas import read_csv\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_absolute_error\n\nPredicting without removing outliers\n\ndf = read_csv(data_path + 'boston-housing.csv', header=None, delim_whitespace=True)\ndf\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n      3\n      4\n      5\n      6\n      7\n      8\n      9\n      10\n      11\n      12\n      13\n    \n  \n  \n    \n      0\n      0.00632\n      18.0\n      2.31\n      0\n      0.538\n      6.575\n      65.2\n      4.0900\n      1\n      296.0\n      15.3\n      396.90\n      4.98\n      24.0\n    \n    \n      1\n      0.02731\n      0.0\n      7.07\n      0\n      0.469\n      6.421\n      78.9\n      4.9671\n      2\n      242.0\n      17.8\n      396.90\n      9.14\n      21.6\n    \n    \n      2\n      0.02729\n      0.0\n      7.07\n      0\n      0.469\n      7.185\n      61.1\n      4.9671\n      2\n      242.0\n      17.8\n      392.83\n      4.03\n      34.7\n    \n    \n      3\n      0.03237\n      0.0\n      2.18\n      0\n      0.458\n      6.998\n      45.8\n      6.0622\n      3\n      222.0\n      18.7\n      394.63\n      2.94\n      33.4\n    \n    \n      4\n      0.06905\n      0.0\n      2.18\n      0\n      0.458\n      7.147\n      54.2\n      6.0622\n      3\n      222.0\n      18.7\n      396.90\n      5.33\n      36.2\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      501\n      0.06263\n      0.0\n      11.93\n      0\n      0.573\n      6.593\n      69.1\n      2.4786\n      1\n      273.0\n      21.0\n      391.99\n      9.67\n      22.4\n    \n    \n      502\n      0.04527\n      0.0\n      11.93\n      0\n      0.573\n      6.120\n      76.7\n      2.2875\n      1\n      273.0\n      21.0\n      396.90\n      9.08\n      20.6\n    \n    \n      503\n      0.06076\n      0.0\n      11.93\n      0\n      0.573\n      6.976\n      91.0\n      2.1675\n      1\n      273.0\n      21.0\n      396.90\n      5.64\n      23.9\n    \n    \n      504\n      0.10959\n      0.0\n      11.93\n      0\n      0.573\n      6.794\n      89.3\n      2.3889\n      1\n      273.0\n      21.0\n      393.45\n      6.48\n      22.0\n    \n    \n      505\n      0.04741\n      0.0\n      11.93\n      0\n      0.573\n      6.030\n      80.8\n      2.5050\n      1\n      273.0\n      21.0\n      396.90\n      7.88\n      11.9\n    \n  \n\n506 rows  14 columns\n\n\n\n\ndata = df.values\nX, y = data[:, :-1], data[:, -1]\nX.shape, y.shape\n\n((506, 13), (506,))\n\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n\n\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n\nyhat = model.predict(X_test)\n\n\nmae = mean_absolute_error(y_test, yhat)\nprint(f'{mae:0.3f}')\n\n3.417\n\n\nPredicting after removing outliers\n\nfrom sklearn.neighbors import LocalOutlierFactor\n\n\ndf = read_csv(data_path + 'boston-housing.csv', delim_whitespace=True, header=None)\ndf\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n      3\n      4\n      5\n      6\n      7\n      8\n      9\n      10\n      11\n      12\n      13\n    \n  \n  \n    \n      0\n      0.00632\n      18.0\n      2.31\n      0\n      0.538\n      6.575\n      65.2\n      4.0900\n      1\n      296.0\n      15.3\n      396.90\n      4.98\n      24.0\n    \n    \n      1\n      0.02731\n      0.0\n      7.07\n      0\n      0.469\n      6.421\n      78.9\n      4.9671\n      2\n      242.0\n      17.8\n      396.90\n      9.14\n      21.6\n    \n    \n      2\n      0.02729\n      0.0\n      7.07\n      0\n      0.469\n      7.185\n      61.1\n      4.9671\n      2\n      242.0\n      17.8\n      392.83\n      4.03\n      34.7\n    \n    \n      3\n      0.03237\n      0.0\n      2.18\n      0\n      0.458\n      6.998\n      45.8\n      6.0622\n      3\n      222.0\n      18.7\n      394.63\n      2.94\n      33.4\n    \n    \n      4\n      0.06905\n      0.0\n      2.18\n      0\n      0.458\n      7.147\n      54.2\n      6.0622\n      3\n      222.0\n      18.7\n      396.90\n      5.33\n      36.2\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      501\n      0.06263\n      0.0\n      11.93\n      0\n      0.573\n      6.593\n      69.1\n      2.4786\n      1\n      273.0\n      21.0\n      391.99\n      9.67\n      22.4\n    \n    \n      502\n      0.04527\n      0.0\n      11.93\n      0\n      0.573\n      6.120\n      76.7\n      2.2875\n      1\n      273.0\n      21.0\n      396.90\n      9.08\n      20.6\n    \n    \n      503\n      0.06076\n      0.0\n      11.93\n      0\n      0.573\n      6.976\n      91.0\n      2.1675\n      1\n      273.0\n      21.0\n      396.90\n      5.64\n      23.9\n    \n    \n      504\n      0.10959\n      0.0\n      11.93\n      0\n      0.573\n      6.794\n      89.3\n      2.3889\n      1\n      273.0\n      21.0\n      393.45\n      6.48\n      22.0\n    \n    \n      505\n      0.04741\n      0.0\n      11.93\n      0\n      0.573\n      6.030\n      80.8\n      2.5050\n      1\n      273.0\n      21.0\n      396.90\n      7.88\n      11.9\n    \n  \n\n506 rows  14 columns\n\n\n\n\ndata = df.values\nX, y = data[:, :-1], data[:, -1]\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n\n\nprint(X_train.shape, y_train.shape)\n\n(339, 13) (339,)\n\n\n\nlof = LocalOutlierFactor()\n\n\nyhat = lof.fit_predict(X_train)\n\n\nmask = yhat != -1\n\n\nX_train, y_train = X_train[mask, :], y_train[mask]\n\n\nprint(X_train.shape, y_train.shape)\n\n(305, 13) (305,)\n\n\n\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n\nyhat = model.predict(X_test)\n\n\nmae = mean_absolute_error(y_test, yhat)\nprint(f'MAE: {mae:.3f}')\n\nMAE: 3.356"
  },
  {
    "objectID": "03-data-preparation/data-cleaning.html#mark-missing-values",
    "href": "03-data-preparation/data-cleaning.html#mark-missing-values",
    "title": "Data Cleaning",
    "section": "Mark Missing Values",
    "text": "Mark Missing Values\n\ndataset = read_csv(data_path + 'pima-indians-diabetes.csv', header=None)\ndataset.head(20)\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n      3\n      4\n      5\n      6\n      7\n      8\n    \n  \n  \n    \n      0\n      6\n      148\n      72\n      35\n      0\n      33.6\n      0.627\n      50\n      1\n    \n    \n      1\n      1\n      85\n      66\n      29\n      0\n      26.6\n      0.351\n      31\n      0\n    \n    \n      2\n      8\n      183\n      64\n      0\n      0\n      23.3\n      0.672\n      32\n      1\n    \n    \n      3\n      1\n      89\n      66\n      23\n      94\n      28.1\n      0.167\n      21\n      0\n    \n    \n      4\n      0\n      137\n      40\n      35\n      168\n      43.1\n      2.288\n      33\n      1\n    \n    \n      5\n      5\n      116\n      74\n      0\n      0\n      25.6\n      0.201\n      30\n      0\n    \n    \n      6\n      3\n      78\n      50\n      32\n      88\n      31.0\n      0.248\n      26\n      1\n    \n    \n      7\n      10\n      115\n      0\n      0\n      0\n      35.3\n      0.134\n      29\n      0\n    \n    \n      8\n      2\n      197\n      70\n      45\n      543\n      30.5\n      0.158\n      53\n      1\n    \n    \n      9\n      8\n      125\n      96\n      0\n      0\n      0.0\n      0.232\n      54\n      1\n    \n    \n      10\n      4\n      110\n      92\n      0\n      0\n      37.6\n      0.191\n      30\n      0\n    \n    \n      11\n      10\n      168\n      74\n      0\n      0\n      38.0\n      0.537\n      34\n      1\n    \n    \n      12\n      10\n      139\n      80\n      0\n      0\n      27.1\n      1.441\n      57\n      0\n    \n    \n      13\n      1\n      189\n      60\n      23\n      846\n      30.1\n      0.398\n      59\n      1\n    \n    \n      14\n      5\n      166\n      72\n      19\n      175\n      25.8\n      0.587\n      51\n      1\n    \n    \n      15\n      7\n      100\n      0\n      0\n      0\n      30.0\n      0.484\n      32\n      1\n    \n    \n      16\n      0\n      118\n      84\n      47\n      230\n      45.8\n      0.551\n      31\n      1\n    \n    \n      17\n      7\n      107\n      74\n      0\n      0\n      29.6\n      0.254\n      31\n      1\n    \n    \n      18\n      1\n      103\n      30\n      38\n      83\n      43.3\n      0.183\n      33\n      0\n    \n    \n      19\n      1\n      115\n      70\n      30\n      96\n      34.6\n      0.529\n      32\n      1\n    \n  \n\n\n\n\n\ndataset.describe()\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n      3\n      4\n      5\n      6\n      7\n      8\n    \n  \n  \n    \n      count\n      768.000000\n      768.000000\n      768.000000\n      768.000000\n      768.000000\n      768.000000\n      768.000000\n      768.000000\n      768.000000\n    \n    \n      mean\n      3.845052\n      120.894531\n      69.105469\n      20.536458\n      79.799479\n      31.992578\n      0.471876\n      33.240885\n      0.348958\n    \n    \n      std\n      3.369578\n      31.972618\n      19.355807\n      15.952218\n      115.244002\n      7.884160\n      0.331329\n      11.760232\n      0.476951\n    \n    \n      min\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.078000\n      21.000000\n      0.000000\n    \n    \n      25%\n      1.000000\n      99.000000\n      62.000000\n      0.000000\n      0.000000\n      27.300000\n      0.243750\n      24.000000\n      0.000000\n    \n    \n      50%\n      3.000000\n      117.000000\n      72.000000\n      23.000000\n      30.500000\n      32.000000\n      0.372500\n      29.000000\n      0.000000\n    \n    \n      75%\n      6.000000\n      140.250000\n      80.000000\n      32.000000\n      127.250000\n      36.600000\n      0.626250\n      41.000000\n      1.000000\n    \n    \n      max\n      17.000000\n      199.000000\n      122.000000\n      99.000000\n      846.000000\n      67.100000\n      2.420000\n      81.000000\n      1.000000\n    \n  \n\n\n\n\n\nnum_missing = (dataset[[1,2,3,4,5]] == 0).sum()\nprint(num_missing)\n\n1      5\n2     35\n3    227\n4    374\n5     11\ndtype: int64\n\n\n\ndataset[[1,2,3,4,5]] = dataset[[1,2,3,4,5]].replace(0, nan)\n\n\nprint(dataset.isnull().sum())\n\n0      0\n1      5\n2     35\n3    227\n4    374\n5     11\n6      0\n7      0\n8      0\ndtype: int64\n\n\n\nprint(dataset.head(20))\n\n     0      1     2     3      4     5      6   7  8\n0    6  148.0  72.0  35.0    NaN  33.6  0.627  50  1\n1    1   85.0  66.0  29.0    NaN  26.6  0.351  31  0\n2    8  183.0  64.0   NaN    NaN  23.3  0.672  32  1\n3    1   89.0  66.0  23.0   94.0  28.1  0.167  21  0\n4    0  137.0  40.0  35.0  168.0  43.1  2.288  33  1\n5    5  116.0  74.0   NaN    NaN  25.6  0.201  30  0\n6    3   78.0  50.0  32.0   88.0  31.0  0.248  26  1\n7   10  115.0   NaN   NaN    NaN  35.3  0.134  29  0\n8    2  197.0  70.0  45.0  543.0  30.5  0.158  53  1\n9    8  125.0  96.0   NaN    NaN   NaN  0.232  54  1\n10   4  110.0  92.0   NaN    NaN  37.6  0.191  30  0\n11  10  168.0  74.0   NaN    NaN  38.0  0.537  34  1\n12  10  139.0  80.0   NaN    NaN  27.1  1.441  57  0\n13   1  189.0  60.0  23.0  846.0  30.1  0.398  59  1\n14   5  166.0  72.0  19.0  175.0  25.8  0.587  51  1\n15   7  100.0   NaN   NaN    NaN  30.0  0.484  32  1\n16   0  118.0  84.0  47.0  230.0  45.8  0.551  31  1\n17   7  107.0  74.0   NaN    NaN  29.6  0.254  31  1\n18   1  103.0  30.0  38.0   83.0  43.3  0.183  33  0\n19   1  115.0  70.0  30.0   96.0  34.6  0.529  32  1"
  },
  {
    "objectID": "03-data-preparation/data-cleaning.html#missing-values-cause-problems",
    "href": "03-data-preparation/data-cleaning.html#missing-values-cause-problems",
    "title": "Data Cleaning",
    "section": "Missing Values Cause Problems",
    "text": "Missing Values Cause Problems\n\ndataset = read_csv(data_path + 'pima-indians-diabetes.csv', header=None)\ndataset.head(20)\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n      3\n      4\n      5\n      6\n      7\n      8\n    \n  \n  \n    \n      0\n      6\n      148\n      72\n      35\n      0\n      33.6\n      0.627\n      50\n      1\n    \n    \n      1\n      1\n      85\n      66\n      29\n      0\n      26.6\n      0.351\n      31\n      0\n    \n    \n      2\n      8\n      183\n      64\n      0\n      0\n      23.3\n      0.672\n      32\n      1\n    \n    \n      3\n      1\n      89\n      66\n      23\n      94\n      28.1\n      0.167\n      21\n      0\n    \n    \n      4\n      0\n      137\n      40\n      35\n      168\n      43.1\n      2.288\n      33\n      1\n    \n    \n      5\n      5\n      116\n      74\n      0\n      0\n      25.6\n      0.201\n      30\n      0\n    \n    \n      6\n      3\n      78\n      50\n      32\n      88\n      31.0\n      0.248\n      26\n      1\n    \n    \n      7\n      10\n      115\n      0\n      0\n      0\n      35.3\n      0.134\n      29\n      0\n    \n    \n      8\n      2\n      197\n      70\n      45\n      543\n      30.5\n      0.158\n      53\n      1\n    \n    \n      9\n      8\n      125\n      96\n      0\n      0\n      0.0\n      0.232\n      54\n      1\n    \n    \n      10\n      4\n      110\n      92\n      0\n      0\n      37.6\n      0.191\n      30\n      0\n    \n    \n      11\n      10\n      168\n      74\n      0\n      0\n      38.0\n      0.537\n      34\n      1\n    \n    \n      12\n      10\n      139\n      80\n      0\n      0\n      27.1\n      1.441\n      57\n      0\n    \n    \n      13\n      1\n      189\n      60\n      23\n      846\n      30.1\n      0.398\n      59\n      1\n    \n    \n      14\n      5\n      166\n      72\n      19\n      175\n      25.8\n      0.587\n      51\n      1\n    \n    \n      15\n      7\n      100\n      0\n      0\n      0\n      30.0\n      0.484\n      32\n      1\n    \n    \n      16\n      0\n      118\n      84\n      47\n      230\n      45.8\n      0.551\n      31\n      1\n    \n    \n      17\n      7\n      107\n      74\n      0\n      0\n      29.6\n      0.254\n      31\n      1\n    \n    \n      18\n      1\n      103\n      30\n      38\n      83\n      43.3\n      0.183\n      33\n      0\n    \n    \n      19\n      1\n      115\n      70\n      30\n      96\n      34.6\n      0.529\n      32\n      1\n    \n  \n\n\n\n\n\ndataset[[1,2,3,4,5]] = dataset[[1,2,3,4,5]].replace(0, nan)\n\n\nvalues = dataset.values\nX = values[:, 0:8]\ny = values[:, 8]\n\n\nmodel = LinearDiscriminantAnalysis()\n\n\ncv = KFold(n_splits=3, shuffle=True, random_state=1)\n\n\nresult = cross_val_score(model, X, y, cv=cv, scoring='accuracy')\nprint(f'{result:0.3f}')\n\nValueError: \nAll the 3 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n3 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/home/naji/miniconda2/envs/nbdev/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/home/naji/miniconda2/envs/nbdev/lib/python3.9/site-packages/sklearn/discriminant_analysis.py\", line 550, in fit\n    X, y = self._validate_data(\n  File \"/home/naji/miniconda2/envs/nbdev/lib/python3.9/site-packages/sklearn/base.py\", line 596, in _validate_data\n    X, y = check_X_y(X, y, **check_params)\n  File \"/home/naji/miniconda2/envs/nbdev/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 1074, in check_X_y\n    X = check_array(\n  File \"/home/naji/miniconda2/envs/nbdev/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 899, in check_array\n    _assert_all_finite(\n  File \"/home/naji/miniconda2/envs/nbdev/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 146, in _assert_all_finite\n    raise ValueError(msg_err)\nValueError: Input X contains NaN.\nLinearDiscriminantAnalysis does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
  },
  {
    "objectID": "03-data-preparation/data-cleaning.html#remove-rows-with-missing-values",
    "href": "03-data-preparation/data-cleaning.html#remove-rows-with-missing-values",
    "title": "Data Cleaning",
    "section": "Remove Rows With Missing Values",
    "text": "Remove Rows With Missing Values\n\ndataset = read_csv(data_path + 'pima-indians-diabetes.csv', header=None)\n\n\ndataset[[1,2,3,4,5]] = dataset[[1,2,3,4,5]].replace(0, nan)\n\n\ndataset.dropna(inplace=True)\n\n\nvalues = dataset.values\nX = values[:, 0:8]\ny = values[:,8]\n\n\nmodel = LinearDiscriminantAnalysis()\n\n\ncv = KFold(n_splits=3, shuffle=True, random_state=1)\n\n\nresult = cross_val_score(model, X, y, cv=cv, scoring='accuracy')\n\n\nprint(f'Accuracy:{result.mean():0.3f}')\n\nAccuracy:0.781"
  },
  {
    "objectID": "03-data-preparation/data-cleaning.html#statistical-imputation",
    "href": "03-data-preparation/data-cleaning.html#statistical-imputation",
    "title": "Data Cleaning",
    "section": "Statistical Imputation",
    "text": "Statistical Imputation\nStatistical Imputation With SimpleImputer\n\nfrom numpy import isnan\nfrom sklearn.impute import SimpleImputer\n\n\ndataframe = read_csv(data_path + 'horse-colic.csv', header=None, na_values='?')\ndataframe\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n      3\n      4\n      5\n      6\n      7\n      8\n      9\n      ...\n      18\n      19\n      20\n      21\n      22\n      23\n      24\n      25\n      26\n      27\n    \n  \n  \n    \n      0\n      2.0\n      1\n      530101\n      38.5\n      66.0\n      28.0\n      3.0\n      3.0\n      NaN\n      2.0\n      ...\n      45.0\n      8.4\n      NaN\n      NaN\n      2.0\n      2\n      11300\n      0\n      0\n      2\n    \n    \n      1\n      1.0\n      1\n      534817\n      39.2\n      88.0\n      20.0\n      NaN\n      NaN\n      4.0\n      1.0\n      ...\n      50.0\n      85.0\n      2.0\n      2.0\n      3.0\n      2\n      2208\n      0\n      0\n      2\n    \n    \n      2\n      2.0\n      1\n      530334\n      38.3\n      40.0\n      24.0\n      1.0\n      1.0\n      3.0\n      1.0\n      ...\n      33.0\n      6.7\n      NaN\n      NaN\n      1.0\n      2\n      0\n      0\n      0\n      1\n    \n    \n      3\n      1.0\n      9\n      5290409\n      39.1\n      164.0\n      84.0\n      4.0\n      1.0\n      6.0\n      2.0\n      ...\n      48.0\n      7.2\n      3.0\n      5.3\n      2.0\n      1\n      2208\n      0\n      0\n      1\n    \n    \n      4\n      2.0\n      1\n      530255\n      37.3\n      104.0\n      35.0\n      NaN\n      NaN\n      6.0\n      2.0\n      ...\n      74.0\n      7.4\n      NaN\n      NaN\n      2.0\n      2\n      4300\n      0\n      0\n      2\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      295\n      1.0\n      1\n      533886\n      NaN\n      120.0\n      70.0\n      4.0\n      NaN\n      4.0\n      2.0\n      ...\n      55.0\n      65.0\n      NaN\n      NaN\n      3.0\n      2\n      3205\n      0\n      0\n      2\n    \n    \n      296\n      2.0\n      1\n      527702\n      37.2\n      72.0\n      24.0\n      3.0\n      2.0\n      4.0\n      2.0\n      ...\n      44.0\n      NaN\n      3.0\n      3.3\n      3.0\n      1\n      2208\n      0\n      0\n      1\n    \n    \n      297\n      1.0\n      1\n      529386\n      37.5\n      72.0\n      30.0\n      4.0\n      3.0\n      4.0\n      1.0\n      ...\n      60.0\n      6.8\n      NaN\n      NaN\n      2.0\n      1\n      3205\n      0\n      0\n      2\n    \n    \n      298\n      1.0\n      1\n      530612\n      36.5\n      100.0\n      24.0\n      3.0\n      3.0\n      3.0\n      1.0\n      ...\n      50.0\n      6.0\n      3.0\n      3.4\n      1.0\n      1\n      2208\n      0\n      0\n      1\n    \n    \n      299\n      1.0\n      1\n      534618\n      37.2\n      40.0\n      20.0\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      36.0\n      62.0\n      1.0\n      1.0\n      3.0\n      2\n      6112\n      0\n      0\n      2\n    \n  \n\n300 rows  28 columns\n\n\n\n\ndata = dataframe.values\n\n\nix = [i for i in range(data.shape[1]) if i != 23]\nX, y = data[:, ix], data[:, 23]\n\n\nprint(f'Missing: {sum(isnan(X).flatten())}')\n\nMissing: 1605\n\n\n\nimputer = SimpleImputer(strategy='mean')\n\n\nimputer.fit(X)\n\nSimpleImputer()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.SimpleImputerSimpleImputer()\n\n\n\nXtrans = imputer.transform(X)\n\n\nprint(f'Missing: {sum(isnan(Xtrans).flatten())}')\n\nMissing: 0"
  },
  {
    "objectID": "03-data-preparation/data-cleaning.html#knn-imputation",
    "href": "03-data-preparation/data-cleaning.html#knn-imputation",
    "title": "Data Cleaning",
    "section": "KNN Imputation",
    "text": "KNN Imputation\nNearest Neighbor Imputation with KNNImputer\n\nfrom numpy import isnan\nfrom sklearn.impute import KNNImputer\n\n\ndata = dataframe.values\n\n\nix = [i for i in range(data.shape[1]) if i != 23]\n\n\nX, y = data[:, ix], data[:, 23]\n\n\nprint(f'Missing: {sum(isnan(X).flatten())}')\n\nMissing: 1605\n\n\n\nimputer = KNNImputer()\n\n\nimputer.fit(X)\n\nKNNImputer()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.KNNImputerKNNImputer()\n\n\n\nXtrans = imputer.transform(X)\n\n\nprint(f'Missing: {sum(isnan(Xtrans).flatten())}')\n\nMissing: 0\n\n\nKNNImputer and Model Evaluation\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import RepeatedStratifiedKFold, cross_val_score\n\n\ndf = read_csv(data_path + 'horse-colic.csv', header=None, na_values='?')\n\n\ndata = df.values\n\n\nix = [i for i in range(data.shape[1]) if i != 23]\n\n\nX, y = data[:, ix], data[:, 23]\n\n\nmodel = RandomForestClassifier()\nimputer = KNNImputer()\n\n\npipeline = Pipeline(steps=[('i', imputer), ('m', model)])\n\n\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n\n\nscores = cross_val_score(pipeline, X, y, scoring='accuracy', cv=cv)\n\n\nprint(f'{scores.mean():.3f} ({scores.std():.3f})')\n\n0.862 (0.048)\n\n\nKNNImputer and Different Number of Neighbors\n\nfrom matplotlib import pyplot\n\n\ndf = read_csv(data_path + 'horse-colic.csv', header=None, na_values='?')\n\n\ndata = df.values\n\n\nix = [i for i in range(data.shape[1]) if i != 23]\n\n\nX, y = data[:, ix], data[:, 23]\n\n\nresults = list()\nstrategies = [str(i) for i in [1,3,5,7,9,15,18,21]]\n\n\nfor s in strategies:\n    pipeline = Pipeline(steps=[('i', KNNImputer(n_neighbors=int(s))), ('m', RandomForestClassifier())])\n    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n    scores = cross_val_score(pipeline, X, y, scoring='accuracy', cv=cv)\n    results.append(scores)\n    print(f'{s} {scores.mean():.3f} ({scores.std():0.3f})')\n\n\npyplot.boxplot(results, labels=strategies, showmeans=True)\npyplot.show()\n\nKNNImputer Transform When Making a Prediction\n\nfrom numpy import nan\n\n\ndf = read_csv(data_path + 'horse-colic.csv', header=None, na_values='?')\n\n\ndata = df.values\n\n\nix = [i for i in range(data.shape[1]) if i != 23]\n\n\nX, y = data[:, ix], data[:, 23]\n\n\npipeline = Pipeline(steps=[('i', KNNImputer(n_neighbors=21)), ('m', RandomForestClassifier())])\n\n\npipeline.fit(X, y)\n\n\nrow = [2, 1, 530101, 38.50, 66, 28, 3, 3, nan, 2, 5, 4, 4, nan, nan, nan, 3, 5, 45.00,\n8.40, nan, nan, 2, 11300, 00000, 00000, 2]\n\n\nyhat = pipeline.predict([row])\n\n\nprint(f'Predicted Class: {yhat[0]}')"
  },
  {
    "objectID": "03-data-preparation/data-cleaning.html#iterative-imputation",
    "href": "03-data-preparation/data-cleaning.html#iterative-imputation",
    "title": "Data Cleaning",
    "section": "Iterative Imputation",
    "text": "Iterative Imputation\n\nfrom numpy import isnan\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\n\nIterative Imputation With IterativeImputer\nIterativeImputer Data Transform\n\ndf = read_csv(data_path + 'horse-colic.csv', header = None, na_values='?')\n\n\ndata = df.values\n\n\nix = [i for i in range(data.shape[1]) if i != 23]\n\n\nX, y = data[:, ix], data[:, 23]\n\n\nprint(f'Missing: {sum(isnan(X).flatten())}')\n\n\nimputer = IterativeImputer()\n\n\nimputer.fit(X)\n\n\nXtrans = imputer.transform(X)\n\n\nprint(f'Missing: {sum(isnan(Xtrans).flatten())}')\n\nIterativeImputer and Model Evaluation\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import RepeatedStratifiedKFold, cross_val_score\n\n\ndf = read_csv(data_path + 'horse-colic.csv', header=None, na_values='?')\n\n\ndata = df.values\n\n\nix = [i for i in range(data.shape[1]) if i != 23]\n\n\nX, y = data[:, ix], data[:, 23]\n\n\nmodel = RandomForestClassifier()\n\n\nimputer = IterativeImputer()\n\n\npipeline = Pipeline(steps=[('i', imputer), ('m', model)])\n\n\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n\n\nscores = cross_val_score(pipeline, X, y, scoring='accuracy', cv=cv)\n\n\nprint(f'{scores.mean():.3f} ({scores.std():.3f})')\n\nIterativeImputer and Different Imputation Order\n\nfrom matplotlib import pyplot\n\n\ndataframe = read_csv(data_path+'horse-colic.csv', header=None, na_values= '?' )\n\ndata = dataframe.values\nix = [i for i in range(data.shape[1]) if i != 23]\nX, y = data[:, ix], data[:, 23]\n\n\nresults = list()\n\n\nstrategies = ['ascending', 'descending', 'roman', 'arabic', 'random']\n\n\nfor s in strategies:\n    pipeline = Pipeline(steps=[('i', IterativeImputer(imputation_order=s)), ('m', RandomForestClassifier())])\n    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n    scores = cross_val_score(pipeline, X, y, scoring='accuracy', cv=cv)\n    results.append(scores)\n    print(f'{s}: {scores.mean():.3f} ({scores.std():.3f})')\n\n\npyplot.boxplot(results, showmeans=True, labels=strategies)\npyplot.show()\n\nIterativeImputer and Different Number of Iterations\nIterativeImputer Transform When Making a Prediction"
  },
  {
    "objectID": "03-data-preparation/dimensionality-reduction.html",
    "href": "03-data-preparation/dimensionality-reduction.html",
    "title": "Dimensionality Reduction",
    "section": "",
    "text": "from numpy import mean, std\nfrom matplotlib import pyplot\nfrom sklearn.datasets import make_classification\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import cross_val_score, RepeatedStratifiedKFold\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\n\nLDA Dimensionality Reduction\nWorked Example of LDA for Dimensionality\n\nX, y = make_classification(n_samples=1000, n_features=20, n_informative=15, \n                           n_redundant=5, random_state=7, n_classes=10)\n\n\nsteps = [('lda', LinearDiscriminantAnalysis(n_components=5)), ('m', GaussianNB())]\n\n\nmodel = Pipeline(steps=steps)\n\n\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n\n\nn_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv)\n\n\nprint(f'Accuracy: {mean(n_scores): .3f}, {std(n_scores): .3f}')\n\nAccuracy:  0.314,  0.049\n\n\nHow do we know that reducing 20 dimensions of input down to five is good or the best we can do?\n\ndef get_dataset():\n    X, y = make_classification(n_samples=1000, n_features=20, n_informative=15,\n                              n_redundant=5, random_state=7, n_classes=10)\n    return X, y\n\n\ndef get_models():\n    models = dict()\n    for i in range(1,10):\n        steps = [('lda', LinearDiscriminantAnalysis(n_components=i)), ('m', GaussianNB())]\n        models[str(i)] = Pipeline(steps=steps)\n    return models\n\n\ndef evaluate_model(model, X, y):\n    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n    scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv)\n    return scores\n\n\nX, y = get_dataset()\n\n\nmodels = get_models()\n\n\nresults, names = list(), list()\n\n\nfor name, model in models.items():\n    scores = evaluate_model(model, X, y)\n    results.append(scores)\n    names.append(name)\n    print(f'{name} > {mean(scores):.3f}  {std(scores): .3f}')\n\n1 > 0.182   0.032\n2 > 0.235   0.036\n3 > 0.267   0.038\n4 > 0.303   0.037\n5 > 0.314   0.049\n6 > 0.314   0.040\n7 > 0.329   0.042\n8 > 0.343   0.045\n9 > 0.358   0.056\n\n\n\npyplot.boxplot(results, labels=names, showmeans=True)\npyplot.show()\n\n\n\n\nWe may choose to use an LDA transform and Naive Bayes model combination as our final model. This involves fitting the Pipeline on all available data and using the pipeline to make predictions on new data.\n\nX, y = make_classification(n_samples=1000, n_features=20, n_informative=15, \n                           n_redundant=5, random_state=7, n_classes=10)\n\n\nsteps = [('lda', LinearDiscriminantAnalysis(n_components=9)), ('m', GaussianNB())]\n\n\nmodel = Pipeline(steps=steps)\n\n\nmodel.fit(X, y)\n\nPipeline(steps=[('lda', LinearDiscriminantAnalysis(n_components=9)),\n                ('m', GaussianNB())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('lda', LinearDiscriminantAnalysis(n_components=9)),\n                ('m', GaussianNB())])LinearDiscriminantAnalysisLinearDiscriminantAnalysis(n_components=9)GaussianNBGaussianNB()\n\n\n\nrow = [[2.3548775, -1.69674567, 1.6193882, -1.19668862, -2.85422348, -2.00998376,\n16.56128782, 2.57257575, 9.93779782, 0.43415008, 6.08274911, 2.12689336, 1.70100279,\n3.32160983, 13.02048541, -3.05034488, 2.06346747, -3.33390362, 2.45147541, -1.23455205]]\n\n\nyhat = model.predict(row)\n\n\nprint(f'Predicted Class: {yhat[0]}')\n\nPredicted Class: 6\n\n\n\n\nPCA Dimensionality Reduction\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.linear_model import LogisticRegression\n\n\nX, y = make_classification(n_samples=1000, n_features=20, n_informative=15,\n                          n_redundant=5, random_state=7)\n\n\nsteps = [('pca', PCA(n_components=10)), ('m', LogisticRegression())]\n\n\nmodel = Pipeline(steps=steps)\n\n\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n\n\nn_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv)\n\n\nprint(f'Accuracy: {mean(n_scores):.3f} {std(n_scores):.3f}')\n\nAccuracy: 0.816 0.034\n\n\nHow do we know that reducing 20 dimensions of input down to 10 is good or the best we can do?\n\ndef get_models():\n    modesl = dict()\n    for i in range(1, 21):\n        steps = [('pca', PCA(n_components=i)), ('m', LogisticRegression())]\n        models[str(i)] = Pipeline(steps=steps)\n    return models\n\n\nX, y = get_dataset()\n\n\nmodels = get_models()\n\n\nresults, names = list(), list()\n\n\nfor name, model in models.items():\n    scores = evaluate_model(model, X, y)\n    results.append(scores)\n    names.append(name)\n    print(f'{name} > {mean(scores): .3f}  {mean(scores): .3f}')\n\n1 >  0.140   0.140\n2 >  0.147   0.147\n3 >  0.152   0.152\n4 >  0.176   0.176\n5 >  0.171   0.171\n6 >  0.205   0.205\n7 >  0.240   0.240\n8 >  0.263   0.263\n9 >  0.274   0.274\n10 >  0.285   0.285\n11 >  0.287   0.287\n12 >  0.305   0.305\n13 >  0.311   0.311\n14 >  0.306   0.306\n15 >  0.323   0.323\n16 >  0.323   0.323\n17 >  0.323   0.323\n18 >  0.323   0.323\n19 >  0.323   0.323\n20 >  0.323   0.323\n\n\n\npyplot.boxplot(results, labels=names, showmeans=True)\npyplot.xticks(rotation=45)\npyplot.show()\n\n\n\n\nThe example below provides an example of fitting and using a final model with PCA transforms on new data.\n\nX, y = make_classification(n_samples=1000, n_features=20, n_informative=15,\n                          n_redundant=5, random_state=7)\n\n\nsteps = [('pca', PCA(n_components=15)), ('m', LogisticRegression())]\n\n\nmodel = Pipeline(steps=steps)\n\n\nmodel.fit(X, y)\n\nPipeline(steps=[('pca', PCA(n_components=15)), ('m', LogisticRegression())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('pca', PCA(n_components=15)), ('m', LogisticRegression())])PCAPCA(n_components=15)LogisticRegressionLogisticRegression()\n\n\n\nrow = [[0.2929949, -4.21223056, -1.288332, -2.17849815, -0.64527665, 2.58097719,\n0.28422388, -7.1827928, -1.91211104, 2.73729512, 0.81395695, 3.96973717, -2.66939799,\n3.34692332, 4.19791821, 0.99990998, -0.30201875, -4.43170633, -2.82646737, 0.44916808]]\n\n\nyhat = model.predict(row)\n\n\nprint(f'Predicted Class: {yhat[0]}')\n\nPredicted Class: 1\n\n\n\n\nSVD Dimensionality Reduction\nWorked Example of SVD for Dimensionality\n\nfrom sklearn.decomposition import TruncatedSVD\n\n\nX, y = make_classification(n_samples=1000, n_features=20, n_informative=15,\n                          n_redundant=5, random_state=7)\n\n\nsteps = [('svd', TruncatedSVD(n_components=10)), ('m', LogisticRegression())]\n\n\nmodel = Pipeline(steps=steps)\n\n\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n\n\nn_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv)\n\n\nprint(f'Accuracy: {mean(n_scores): .3f}  {std(n_scores): .3f}')\n\nAccuracy:  0.814   0.034\n\n\nHow do we know that reducing 20 dimensions of input down to 10 is good or the best we can do?\n\ndef get_models():\n    models = dict()\n    for i in range(1, 20):\n        steps = [('svd', TruncatedSVD(n_components=i)), ('m', LogisticRegression())]\n        models[str(i)] = Pipeline(steps=steps)\n    return models\n\n\nX, y = get_dataset()\n\n\nmodels = get_models()\n\n\nresults, names = list(), list()\n\n\nfor name, model in models.items():\n    scores = evaluate_model(model, X, y)\n    results.append(scores)\n    names.append(name)\n    print(f'{name}> {mean(scores):.3f}  {std(scores):.3f}')\n\n1> 0.140  0.024\n2> 0.147  0.021\n3> 0.152  0.023\n4> 0.177  0.032\n5> 0.171  0.036\n6> 0.204  0.038\n7> 0.236  0.037\n8> 0.265  0.035\n9> 0.279  0.036\n10> 0.288  0.035\n11> 0.289  0.034\n12> 0.306  0.037\n13> 0.309  0.037\n14> 0.308  0.033\n15> 0.323  0.039\n16> 0.323  0.039\n17> 0.323  0.039\n18> 0.323  0.039\n19> 0.323  0.039\n\n\n\npyplot.boxplot(results, labels=names, showmeans=True)\npyplot.xticks(rotation=45)\npyplot.show()\n\n\n\n\nWe may choose to use an SVD transform and logistic regression model combination as our final model.\n\nX, y = make_classification(n_samples=1000, n_features=20, n_informative=15,\n                          n_redundant=5, random_state=7)\n\n\nsteps = [('svd', TruncatedSVD(n_components=15)), ('m', LogisticRegression())]\n\n\nmodel = Pipeline(steps=steps)\n\n\nmodel.fit(X, y)\n\nPipeline(steps=[('svd', TruncatedSVD(n_components=15)),\n                ('m', LogisticRegression())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('svd', TruncatedSVD(n_components=15)),\n                ('m', LogisticRegression())])TruncatedSVDTruncatedSVD(n_components=15)LogisticRegressionLogisticRegression()\n\n\n\nrow = [[0.2929949, -4.21223056, -1.288332, -2.17849815, -0.64527665, 2.58097719,\n0.28422388, -7.1827928, -1.91211104, 2.73729512, 0.81395695, 3.96973717, -2.66939799,\n3.34692332, 4.19791821, 0.99990998, -0.30201875, -4.43170633, -2.82646737, 0.44916808]]\n\n\nyhat = model.predict(row)\n\n\nprint(f'Predicted Class: {yhat[0]}')\n\nPredicted Class: 1"
  },
  {
    "objectID": "03-data-preparation/data-transforms.html",
    "href": "03-data-preparation/data-transforms.html",
    "title": "Data Transforms",
    "section": "",
    "text": "data_path = '/home/naji/Desktop/github-repos/machine-learning/nbs/0-datasets/data/'\n\n\nScale Numerical Data\nNumerical Data Scaling Methods\n\nfrom pandas import read_csv, DataFrame\nfrom numpy import asarray, mean, std\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import RepeatedStratifiedKFold, cross_val_score, train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.pipeline import Pipeline\nfrom matplotlib import pyplot\n\nData Normalization\n\nfrom sklearn.preprocessing import MinMaxScaler\n\n\ndata = asarray([[100, 0.001],\n                [8, 0.05],\n                [50, 0.005],\n                [88, 0.07],\n                [4, 0.1]])\n\n\nprint(data)\n\n[[1.0e+02 1.0e-03]\n [8.0e+00 5.0e-02]\n [5.0e+01 5.0e-03]\n [8.8e+01 7.0e-02]\n [4.0e+00 1.0e-01]]\n\n\n\nscaler = MinMaxScaler()\n\n\nscaled = scaler.fit_transform(data)\n\n\nprint(scaled)\n\n[[1.         0.        ]\n [0.04166667 0.49494949]\n [0.47916667 0.04040404]\n [0.875      0.6969697 ]\n [0.         1.        ]]\n\n\nData Standardization\n\nfrom sklearn.preprocessing import StandardScaler\n\n\ndata = asarray([[100, 0.001],\n                [8, 0.05],\n                [50, 0.005],\n                [88, 0.07],\n                [4, 0.1]])\n\n\nscaler = StandardScaler()\n\n\nscaled = scaler.fit_transform(data)\n\n\nprint(scaled)\n\n[[ 1.26398112 -1.16389967]\n [-1.06174414  0.12639634]\n [ 0.         -1.05856939]\n [ 0.96062565  0.65304778]\n [-1.16286263  1.44302493]]\n\n\nDiabetes Dataset\n\ndf = read_csv(data_path + 'pima-indians-diabetes.csv', header=None)\ndf\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n      3\n      4\n      5\n      6\n      7\n      8\n    \n  \n  \n    \n      0\n      6\n      148\n      72\n      35\n      0\n      33.6\n      0.627\n      50\n      1\n    \n    \n      1\n      1\n      85\n      66\n      29\n      0\n      26.6\n      0.351\n      31\n      0\n    \n    \n      2\n      8\n      183\n      64\n      0\n      0\n      23.3\n      0.672\n      32\n      1\n    \n    \n      3\n      1\n      89\n      66\n      23\n      94\n      28.1\n      0.167\n      21\n      0\n    \n    \n      4\n      0\n      137\n      40\n      35\n      168\n      43.1\n      2.288\n      33\n      1\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      763\n      10\n      101\n      76\n      48\n      180\n      32.9\n      0.171\n      63\n      0\n    \n    \n      764\n      2\n      122\n      70\n      27\n      0\n      36.8\n      0.340\n      27\n      0\n    \n    \n      765\n      5\n      121\n      72\n      23\n      112\n      26.2\n      0.245\n      30\n      0\n    \n    \n      766\n      1\n      126\n      60\n      0\n      0\n      30.1\n      0.349\n      47\n      1\n    \n    \n      767\n      1\n      93\n      70\n      31\n      0\n      30.4\n      0.315\n      23\n      0\n    \n  \n\n768 rows  9 columns\n\n\n\n\ndf.shape\n\n(768, 9)\n\n\n\nprint(df.describe())\n\n                0           1           2  ...           6           7           8\ncount  768.000000  768.000000  768.000000  ...  768.000000  768.000000  768.000000\nmean     3.845052  120.894531   69.105469  ...    0.471876   33.240885    0.348958\nstd      3.369578   31.972618   19.355807  ...    0.331329   11.760232    0.476951\nmin      0.000000    0.000000    0.000000  ...    0.078000   21.000000    0.000000\n25%      1.000000   99.000000   62.000000  ...    0.243750   24.000000    0.000000\n50%      3.000000  117.000000   72.000000  ...    0.372500   29.000000    0.000000\n75%      6.000000  140.250000   80.000000  ...    0.626250   41.000000    1.000000\nmax     17.000000  199.000000  122.000000  ...    2.420000   81.000000    1.000000\n\n[8 rows x 9 columns]\n\n\n\nfig = df.hist(xlabelsize=4, ylabelsize=4)\n[x.title.set_size(4) for x in fig.ravel()]\npyplot.show()\n\n\n\n\n\ndata = df.values\n\n\nX, y = data[:, :-1], data[:, -1]\n\n\nX = X.astype('float32')\n\n\ny = LabelEncoder().fit_transform(y.astype('str'))\n\n\nmodel = KNeighborsClassifier()\n\n\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n\n\nscores = cross_val_score(model, X, y, scoring='accuracy', cv=cv)\n\n\nprint(f'Accuracy: {mean(scores):0.3f}  {std(scores):0.3f}')\n\nAccuracy: 0.717  0.040\n\n\nMinMaxScaler Transform\n\ndf = read_csv(data_path + 'pima-indians-diabetes.csv', header=None)\n\n\ndata = df.values\n\n\ntrans = MinMaxScaler()\n\n\ndata = trans.fit_transform(data)\n\n\ndf = DataFrame(data)\n\n\nprint(df.describe())\n\n                0           1           2  ...           6           7           8\ncount  768.000000  768.000000  768.000000  ...  768.000000  768.000000  768.000000\nmean     0.226180    0.607510    0.566438  ...    0.168179    0.204015    0.348958\nstd      0.198210    0.160666    0.158654  ...    0.141473    0.196004    0.476951\nmin      0.000000    0.000000    0.000000  ...    0.000000    0.000000    0.000000\n25%      0.058824    0.497487    0.508197  ...    0.070773    0.050000    0.000000\n50%      0.176471    0.587940    0.590164  ...    0.125747    0.133333    0.000000\n75%      0.352941    0.704774    0.655738  ...    0.234095    0.333333    1.000000\nmax      1.000000    1.000000    1.000000  ...    1.000000    1.000000    1.000000\n\n[8 rows x 9 columns]\n\n\n\nfig = df.hist(xlabelsize=4, ylabelsize=4)\n[x.title.set_size(4) for x in fig.ravel()]\npyplot.show()\n\n\n\n\nTrain the model\n\ndf = read_csv(data_path + 'pima-indians-diabetes.csv', header=None)\n\n\ndata = df.values\n\n\nX, y = data[:, :-1], data[:, -1]\n\n\nX = X.astype('float32')\ny = LabelEncoder().fit_transform(y.astype('str'))\n\n\ntrans = MinMaxScaler()\n\n\nmodel = KNeighborsClassifier()\n\n\npipeline = Pipeline(steps=[('t', trans), ('m', model)])\n\n\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n\n\nscores = cross_val_score(pipeline, X, y, scoring='accuracy', cv=cv)\n\n\nprint(f'Accuracy: {mean(scores):0.3f}, {std(scores):.3f}')\n\nAccuracy: 0.739, 0.053\n\n\nStandardScaler Transform\n\ndf = read_csv(data_path + 'pima-indians-diabetes.csv', header=None)\n\n\ndata = df.values[:, :-1]\n\n\ntrans = StandardScaler()\n\n\ndata = trans.fit_transform(data)\n\n\ndf = DataFrame(data)\n\n\nprint(df.describe())\n\n                  0             1  ...             6             7\ncount  7.680000e+02  7.680000e+02  ...  7.680000e+02  7.680000e+02\nmean  -6.476301e-17 -9.251859e-18  ...  2.451743e-16  1.931325e-16\nstd    1.000652e+00  1.000652e+00  ...  1.000652e+00  1.000652e+00\nmin   -1.141852e+00 -3.783654e+00  ... -1.189553e+00 -1.041549e+00\n25%   -8.448851e-01 -6.852363e-01  ... -6.889685e-01 -7.862862e-01\n50%   -2.509521e-01 -1.218877e-01  ... -3.001282e-01 -3.608474e-01\n75%    6.399473e-01  6.057709e-01  ...  4.662269e-01  6.602056e-01\nmax    3.906578e+00  2.444478e+00  ...  5.883565e+00  4.063716e+00\n\n[8 rows x 8 columns]\n\n\n\nfig = df.hist(xlabelsize=4, ylabelsize=4)\n[x.title.set_size(4) for x in fig.ravel()]\npyplot.show()\n\n\n\n\nTrain the model\n\ndf = read_csv(data_path + 'pima-indians-diabetes.csv', header=None)\n\n\ndata = df.values\n\n\nX, y = data[:, :-1], data[:, -1]\n\n\nX = X.astype('float32')\n\n\ny = LabelEncoder().fit_transform(y.astype('str'))\n\n\ntrans = StandardScaler()\n\n\nmodel = KNeighborsClassifier()\n\n\npipeline = Pipeline(steps=[('t', trans), ('m', model)])\n\n\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n\n\nscores = cross_val_score(pipeline, X, y, scoring='accuracy', cv=cv)\n\n\nprint(f'Accuracy: {mean(scores):.3f}  {std(scores):.3f}')\n\nAccuracy: 0.741  0.050\n\n\n\n\nScale Data With Outliers\nIQR Robust Scaler Transform\n\nfrom sklearn.preprocessing import RobustScaler\n\n\ndf = read_csv(data_path + 'pima-indians-diabetes.csv', header=None)\n\n\ndata = df.values[:, :-1]\n\n\ntrans = RobustScaler()\n\n\ndata = trans.fit_transform(data)\n\n\ndf = DataFrame(data)\n\n\nprint(df.describe())\n\n                0           1           2  ...           5           6           7\ncount  768.000000  768.000000  768.000000  ...  768.000000  768.000000  768.000000\nmean     0.169010    0.094413   -0.160807  ...   -0.000798    0.259807    0.249464\nstd      0.673916    0.775094    1.075323  ...    0.847759    0.866219    0.691778\nmin     -0.600000   -2.836364   -4.000000  ...   -3.440860   -0.769935   -0.470588\n25%     -0.400000   -0.436364   -0.555556  ...   -0.505376   -0.336601   -0.294118\n50%      0.000000    0.000000    0.000000  ...    0.000000    0.000000    0.000000\n75%      0.600000    0.563636    0.444444  ...    0.494624    0.663399    0.705882\nmax      2.800000    1.987879    2.777778  ...    3.774194    5.352941    3.058824\n\n[8 rows x 8 columns]\n\n\n\nfig = df.hist(xlabelsize=4, ylabelsize=4)\n[x.title.set_size(4) for x in fig.ravel()]\npyplot.show()\n\n\n\n\nTrain the model\n\ndf = read_csv(data_path + 'pima-indians-diabetes.csv', header=None)\n\n\ndata = df.values\n\n\nX, y = data[:, :-1], data[:, -1]\n\n\nX = X.astype('float32')\n\n\ny = LabelEncoder().fit_transform(y.astype('str'))\n\n\ntrans = RobustScaler()\n\n\nmodel = KNeighborsClassifier()\n\n\npipeline = Pipeline(steps=[('t', trans), ('m', model)])\n\n\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n\n\nscores = cross_val_score(pipeline, X, y, scoring='accuracy', cv=cv)\n\n\nprint(f'Accuracy: {mean(scores):.3f}  {std(scores):.3f}')\n\nAccuracy: 0.734  0.044\n\n\nExplore Robust Scaler Range\n\ndef get_dataset():\n    df = read_csv(data_path + 'pima-indians-diabetes.csv', header=None)\n    data = df.values\n    X, y = data[:, :-1], data[:, -1]\n    X = X.astype('float32')\n    y = LabelEncoder().fit_transform(y.astype('str'))\n    return X, y\n\n\ndef get_models():\n    models = dict()\n    for value in [1, 5, 10, 20, 25, 30]:\n        trans = RobustScaler(quantile_range=(value, 100-value))\n        model = KNeighborsClassifier()\n        models[str(value)] = Pipeline(steps=[('t', trans), ('m', model)])\n    return models\n\n\ndef evaluate_model(model, X, y):\n    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n    scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv)\n    return scores\n\n\nx, y = get_dataset()\n\n\nmodels = get_models()\n\n\nresults, names = list(), list()\n\n\nfor name, model in models.items():\n    scores = evaluate_model(model, X, y)\n    results.append(scores)\n    names.append(name)\n    print(f'{name:} {mean(scores):.3f} {std(scores):.3f}')\n\n1 0.734 0.054\n5 0.736 0.051\n10 0.739 0.047\n20 0.734 0.050\n25 0.734 0.044\n30 0.735 0.042\n\n\n\npyplot.boxplot(results, labels=names, showmeans=True)\npyplot.show()\n\n\n\n\n\n\nEncode Categorical Data\nEncoding Categorical Data\nOrdinal Encoding\n\nfrom sklearn.preprocessing import OrdinalEncoder\n\n\ndata = asarray([['red'], ['green'], ['blue']])\n\n\nprint(data)\n\n[['red']\n ['green']\n ['blue']]\n\n\n\nencoder = OrdinalEncoder()\n\n\nresult = encoder.fit_transform(data)\n\n\nprint(result)\n\n[[2.]\n [1.]\n [0.]]\n\n\nOne Hot Encoding\n\nfrom sklearn.preprocessing import OneHotEncoder\n\n\ndata = asarray([['red'], ['green'], ['blue']])\n\n\nprint(data)\n\n[['red']\n ['green']\n ['blue']]\n\n\n\nencoder = OneHotEncoder(sparse=False)\n\n\nonehot = encoder.fit_transform(data)\n\n\nprint(onehot)\n\n[[0. 0. 1.]\n [0. 1. 0.]\n [1. 0. 0.]]\n\n\nDummy Variable Encoding\n\nfrom sklearn.preprocessing import OneHotEncoder\n\n\ndata = asarray([['red'],['green'],['blue']])\nprint(data)\n\n[['red']\n ['green']\n ['blue']]\n\n\n\nencoder = OneHotEncoder(drop='first', sparse=False)\n\n\nonehot = encoder.fit_transform(data)\nprint(onehot)\n\n[[0. 1.]\n [1. 0.]\n [0. 0.]]\n\n\nBreast Cancer Dataset\n\ndf = read_csv(data_path + 'breast-cancer.csv', header=None)\ndf\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n      3\n      4\n      5\n      6\n      7\n      8\n      9\n    \n  \n  \n    \n      0\n      '40-49'\n      'premeno'\n      '15-19'\n      '0-2'\n      'yes'\n      '3'\n      'right'\n      'left_up'\n      'no'\n      'recurrence-events'\n    \n    \n      1\n      '50-59'\n      'ge40'\n      '15-19'\n      '0-2'\n      'no'\n      '1'\n      'right'\n      'central'\n      'no'\n      'no-recurrence-events'\n    \n    \n      2\n      '50-59'\n      'ge40'\n      '35-39'\n      '0-2'\n      'no'\n      '2'\n      'left'\n      'left_low'\n      'no'\n      'recurrence-events'\n    \n    \n      3\n      '40-49'\n      'premeno'\n      '35-39'\n      '0-2'\n      'yes'\n      '3'\n      'right'\n      'left_low'\n      'yes'\n      'no-recurrence-events'\n    \n    \n      4\n      '40-49'\n      'premeno'\n      '30-34'\n      '3-5'\n      'yes'\n      '2'\n      'left'\n      'right_up'\n      'no'\n      'recurrence-events'\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      281\n      '50-59'\n      'ge40'\n      '30-34'\n      '6-8'\n      'yes'\n      '2'\n      'left'\n      'left_low'\n      'no'\n      'no-recurrence-events'\n    \n    \n      282\n      '50-59'\n      'premeno'\n      '25-29'\n      '3-5'\n      'yes'\n      '2'\n      'left'\n      'left_low'\n      'yes'\n      'no-recurrence-events'\n    \n    \n      283\n      '30-39'\n      'premeno'\n      '30-34'\n      '6-8'\n      'yes'\n      '2'\n      'right'\n      'right_up'\n      'no'\n      'no-recurrence-events'\n    \n    \n      284\n      '50-59'\n      'premeno'\n      '15-19'\n      '0-2'\n      'no'\n      '2'\n      'right'\n      'left_low'\n      'no'\n      'no-recurrence-events'\n    \n    \n      285\n      '50-59'\n      'ge40'\n      '40-44'\n      '0-2'\n      'no'\n      '3'\n      'left'\n      'right_up'\n      'no'\n      'no-recurrence-events'\n    \n  \n\n286 rows  10 columns\n\n\n\n\ndata = df.values\n\n\nX = data[:, :-1].astype(str)\ny = data[:, -1].astype(str)\n\n\nprint('Input', X.shape)\n\nInput (286, 9)\n\n\n\nprint('Output', y.shape)\n\nOutput (286,)\n\n\nOrdinalEncoder Transform\n\nordinal_encoder = OrdinalEncoder()\n\n\nX = ordinal_encoder.fit_transform(X)\n\n\nlabel_encoder = LabelEncoder()\n\n\ny = label_encoder.fit_transform(y)\n\n\nprint('Input', X.shape)\n\nInput (286, 9)\n\n\n\nX[:10, :]\n\narray([[2., 2., 2., 0., 1., 2., 1., 2., 0.],\n       [3., 0., 2., 0., 0., 0., 1., 0., 0.],\n       [3., 0., 6., 0., 0., 1., 0., 1., 0.],\n       [2., 2., 6., 0., 1., 2., 1., 1., 1.],\n       [2., 2., 5., 4., 1., 1., 0., 4., 0.],\n       [3., 2., 4., 4., 0., 1., 1., 2., 1.],\n       [3., 0., 7., 0., 0., 2., 0., 2., 0.],\n       [2., 2., 1., 0., 0., 1., 0., 2., 0.],\n       [2., 2., 0., 0., 0., 1., 1., 3., 0.],\n       [2., 0., 7., 2., 1., 1., 1., 2., 1.]])\n\n\n\nprint('Output', y.shape)\n\nOutput (286,)\n\n\n\ny[:10]\n\narray([1, 0, 1, 0, 1, 0, 0, 0, 0, 0])\n\n\nTraining a model\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\n\n\ndf = read_csv(data_path + 'breast-cancer.csv', header=None)\n\n\ndata = df.values\n\n\nX = data[:, :-1].astype(str)\ny = data[:, -1].astype(str)\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.33, random_state=1)\n\n\nordinal_encoder = OrdinalEncoder()\n\n\nordinal_encoder.fit(X_train)\n\nOrdinalEncoder()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.OrdinalEncoderOrdinalEncoder()\n\n\n\nX_train = ordinal_encoder.transform(X_train)\nX_test = ordinal_encoder.transform(X_test)\n\n\nlabel_encoder = LabelEncoder()\n\n\nlabel_encoder.fit(y_train)\n\nLabelEncoder()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LabelEncoderLabelEncoder()\n\n\n\ny_train = label_encoder.transform(y_train)\n\n\ny_test = label_encoder.transform(y_test)\n\n\nmodel = LogisticRegression()\n\n\nmodel.fit(X_train, y_train)\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression()\n\n\n\nyhat = model.predict(X_test)\n\n\naccuracy = accuracy_score(y_test, yhat)\n\n\nprint(f'Accuracy: {accuracy*100: .3f}')\n\nAccuracy:  75.789\n\n\nOneHotEncoder Transform\n\ndf = read_csv(data_path + 'breast-cancer.csv', header=None)\n\n\ndata = df.values\n\n\nX = data[:, :-1].astype(str)\ny = data[:, -1].astype(str)\n\n\nonehot_encoder = OneHotEncoder(sparse=False)\n\n\nX = onehot_encoder.fit_transform(X)\n\n\nlabel_encoder = LabelEncoder()\n\n\ny = label_encoder.fit_transform(y)\n\n\nprint('Input', X.shape)\n\nInput (286, 43)\n\n\n\nprint(X[:5, :])\n\n[[0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n  0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0.]\n [0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n  0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0.]\n [0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0.\n  0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0.]\n [0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0.\n  0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1.]\n [0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n  1. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0.]]\n\n\nTrain the model\n\ndf = read_csv(data_path + 'breast-cancer.csv', header=None)\n\n\ndata = df.values\n\n\nX = data[:, :-1].astype(str)\ny = data[:, -1].astype(str)\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n\n\nonehot_encoder = OneHotEncoder()\n\n\nonehot_encoder.fit(X_train)\n\nOneHotEncoder()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.OneHotEncoderOneHotEncoder()\n\n\n\nX_train = onehot_encoder.transform(X_train)\nX_test = onehot_encoder.transform(X_test)\n\n\nlabel_encoder = LabelEncoder()\n\n\nlabel_encoder.fit(y_train)\n\nLabelEncoder()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LabelEncoderLabelEncoder()\n\n\n\ny_train = label_encoder.transform(y_train)\n\n\ny_test = label_encoder.transform(y_test)\n\n\nmodel = LogisticRegression()\n\n\nmodel.fit(X_train, y_train)\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression()\n\n\n\nyhat = model.predict(X_test)\n\n\naccuracy = accuracy_score(y_test, yhat)\n\n\nprint(f'Accuracy: {accuracy*100: .3f}')\n\nAccuracy:  70.526\n\n\n\n\nHow to Make Distributions More Gaussian\nPower Transforms\n\nfrom numpy import exp \nfrom numpy.random import randn\nfrom sklearn.preprocessing import PowerTransformer\n\n\ndata = randn(1000)\n\n\ndata = exp(data)\n\n\npyplot.hist(data, bins=25)\npyplot.show()\n\n\n\n\n\ndata = data.reshape(len(data), 1)\n\n\npower = PowerTransformer(method='yeo-johnson', standardize=True)\n\n\ndata_trans = power.fit_transform(data)\n\n\npyplot.hist(data_trans, bins=25)\npyplot.show()\n\n\n\n\nSonar Dataset\n\nfrom sklearn.preprocessing import LabelEncoder\n\n\ndf = read_csv(data_path + 'sonar.csv', header=None)\n\n\ndf\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n      3\n      4\n      5\n      6\n      7\n      8\n      9\n      10\n      11\n      12\n      13\n      14\n      15\n      16\n      17\n      18\n      19\n      20\n      21\n      22\n      23\n      24\n      25\n      26\n      27\n      28\n      29\n      30\n      31\n      32\n      33\n      34\n      35\n      36\n      37\n      38\n      39\n      40\n      41\n      42\n      43\n      44\n      45\n      46\n      47\n      48\n      49\n      50\n      51\n      52\n      53\n      54\n      55\n      56\n      57\n      58\n      59\n      60\n    \n  \n  \n    \n      0\n      0.0200\n      0.0371\n      0.0428\n      0.0207\n      0.0954\n      0.0986\n      0.1539\n      0.1601\n      0.3109\n      0.2111\n      0.1609\n      0.1582\n      0.2238\n      0.0645\n      0.0660\n      0.2273\n      0.3100\n      0.2999\n      0.5078\n      0.4797\n      0.5783\n      0.5071\n      0.4328\n      0.5550\n      0.6711\n      0.6415\n      0.7104\n      0.8080\n      0.6791\n      0.3857\n      0.1307\n      0.2604\n      0.5121\n      0.7547\n      0.8537\n      0.8507\n      0.6692\n      0.6097\n      0.4943\n      0.2744\n      0.0510\n      0.2834\n      0.2825\n      0.4256\n      0.2641\n      0.1386\n      0.1051\n      0.1343\n      0.0383\n      0.0324\n      0.0232\n      0.0027\n      0.0065\n      0.0159\n      0.0072\n      0.0167\n      0.0180\n      0.0084\n      0.0090\n      0.0032\n      R\n    \n    \n      1\n      0.0453\n      0.0523\n      0.0843\n      0.0689\n      0.1183\n      0.2583\n      0.2156\n      0.3481\n      0.3337\n      0.2872\n      0.4918\n      0.6552\n      0.6919\n      0.7797\n      0.7464\n      0.9444\n      1.0000\n      0.8874\n      0.8024\n      0.7818\n      0.5212\n      0.4052\n      0.3957\n      0.3914\n      0.3250\n      0.3200\n      0.3271\n      0.2767\n      0.4423\n      0.2028\n      0.3788\n      0.2947\n      0.1984\n      0.2341\n      0.1306\n      0.4182\n      0.3835\n      0.1057\n      0.1840\n      0.1970\n      0.1674\n      0.0583\n      0.1401\n      0.1628\n      0.0621\n      0.0203\n      0.0530\n      0.0742\n      0.0409\n      0.0061\n      0.0125\n      0.0084\n      0.0089\n      0.0048\n      0.0094\n      0.0191\n      0.0140\n      0.0049\n      0.0052\n      0.0044\n      R\n    \n    \n      2\n      0.0262\n      0.0582\n      0.1099\n      0.1083\n      0.0974\n      0.2280\n      0.2431\n      0.3771\n      0.5598\n      0.6194\n      0.6333\n      0.7060\n      0.5544\n      0.5320\n      0.6479\n      0.6931\n      0.6759\n      0.7551\n      0.8929\n      0.8619\n      0.7974\n      0.6737\n      0.4293\n      0.3648\n      0.5331\n      0.2413\n      0.5070\n      0.8533\n      0.6036\n      0.8514\n      0.8512\n      0.5045\n      0.1862\n      0.2709\n      0.4232\n      0.3043\n      0.6116\n      0.6756\n      0.5375\n      0.4719\n      0.4647\n      0.2587\n      0.2129\n      0.2222\n      0.2111\n      0.0176\n      0.1348\n      0.0744\n      0.0130\n      0.0106\n      0.0033\n      0.0232\n      0.0166\n      0.0095\n      0.0180\n      0.0244\n      0.0316\n      0.0164\n      0.0095\n      0.0078\n      R\n    \n    \n      3\n      0.0100\n      0.0171\n      0.0623\n      0.0205\n      0.0205\n      0.0368\n      0.1098\n      0.1276\n      0.0598\n      0.1264\n      0.0881\n      0.1992\n      0.0184\n      0.2261\n      0.1729\n      0.2131\n      0.0693\n      0.2281\n      0.4060\n      0.3973\n      0.2741\n      0.3690\n      0.5556\n      0.4846\n      0.3140\n      0.5334\n      0.5256\n      0.2520\n      0.2090\n      0.3559\n      0.6260\n      0.7340\n      0.6120\n      0.3497\n      0.3953\n      0.3012\n      0.5408\n      0.8814\n      0.9857\n      0.9167\n      0.6121\n      0.5006\n      0.3210\n      0.3202\n      0.4295\n      0.3654\n      0.2655\n      0.1576\n      0.0681\n      0.0294\n      0.0241\n      0.0121\n      0.0036\n      0.0150\n      0.0085\n      0.0073\n      0.0050\n      0.0044\n      0.0040\n      0.0117\n      R\n    \n    \n      4\n      0.0762\n      0.0666\n      0.0481\n      0.0394\n      0.0590\n      0.0649\n      0.1209\n      0.2467\n      0.3564\n      0.4459\n      0.4152\n      0.3952\n      0.4256\n      0.4135\n      0.4528\n      0.5326\n      0.7306\n      0.6193\n      0.2032\n      0.4636\n      0.4148\n      0.4292\n      0.5730\n      0.5399\n      0.3161\n      0.2285\n      0.6995\n      1.0000\n      0.7262\n      0.4724\n      0.5103\n      0.5459\n      0.2881\n      0.0981\n      0.1951\n      0.4181\n      0.4604\n      0.3217\n      0.2828\n      0.2430\n      0.1979\n      0.2444\n      0.1847\n      0.0841\n      0.0692\n      0.0528\n      0.0357\n      0.0085\n      0.0230\n      0.0046\n      0.0156\n      0.0031\n      0.0054\n      0.0105\n      0.0110\n      0.0015\n      0.0072\n      0.0048\n      0.0107\n      0.0094\n      R\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      203\n      0.0187\n      0.0346\n      0.0168\n      0.0177\n      0.0393\n      0.1630\n      0.2028\n      0.1694\n      0.2328\n      0.2684\n      0.3108\n      0.2933\n      0.2275\n      0.0994\n      0.1801\n      0.2200\n      0.2732\n      0.2862\n      0.2034\n      0.1740\n      0.4130\n      0.6879\n      0.8120\n      0.8453\n      0.8919\n      0.9300\n      0.9987\n      1.0000\n      0.8104\n      0.6199\n      0.6041\n      0.5547\n      0.4160\n      0.1472\n      0.0849\n      0.0608\n      0.0969\n      0.1411\n      0.1676\n      0.1200\n      0.1201\n      0.1036\n      0.1977\n      0.1339\n      0.0902\n      0.1085\n      0.1521\n      0.1363\n      0.0858\n      0.0290\n      0.0203\n      0.0116\n      0.0098\n      0.0199\n      0.0033\n      0.0101\n      0.0065\n      0.0115\n      0.0193\n      0.0157\n      M\n    \n    \n      204\n      0.0323\n      0.0101\n      0.0298\n      0.0564\n      0.0760\n      0.0958\n      0.0990\n      0.1018\n      0.1030\n      0.2154\n      0.3085\n      0.3425\n      0.2990\n      0.1402\n      0.1235\n      0.1534\n      0.1901\n      0.2429\n      0.2120\n      0.2395\n      0.3272\n      0.5949\n      0.8302\n      0.9045\n      0.9888\n      0.9912\n      0.9448\n      1.0000\n      0.9092\n      0.7412\n      0.7691\n      0.7117\n      0.5304\n      0.2131\n      0.0928\n      0.1297\n      0.1159\n      0.1226\n      0.1768\n      0.0345\n      0.1562\n      0.0824\n      0.1149\n      0.1694\n      0.0954\n      0.0080\n      0.0790\n      0.1255\n      0.0647\n      0.0179\n      0.0051\n      0.0061\n      0.0093\n      0.0135\n      0.0063\n      0.0063\n      0.0034\n      0.0032\n      0.0062\n      0.0067\n      M\n    \n    \n      205\n      0.0522\n      0.0437\n      0.0180\n      0.0292\n      0.0351\n      0.1171\n      0.1257\n      0.1178\n      0.1258\n      0.2529\n      0.2716\n      0.2374\n      0.1878\n      0.0983\n      0.0683\n      0.1503\n      0.1723\n      0.2339\n      0.1962\n      0.1395\n      0.3164\n      0.5888\n      0.7631\n      0.8473\n      0.9424\n      0.9986\n      0.9699\n      1.0000\n      0.8630\n      0.6979\n      0.7717\n      0.7305\n      0.5197\n      0.1786\n      0.1098\n      0.1446\n      0.1066\n      0.1440\n      0.1929\n      0.0325\n      0.1490\n      0.0328\n      0.0537\n      0.1309\n      0.0910\n      0.0757\n      0.1059\n      0.1005\n      0.0535\n      0.0235\n      0.0155\n      0.0160\n      0.0029\n      0.0051\n      0.0062\n      0.0089\n      0.0140\n      0.0138\n      0.0077\n      0.0031\n      M\n    \n    \n      206\n      0.0303\n      0.0353\n      0.0490\n      0.0608\n      0.0167\n      0.1354\n      0.1465\n      0.1123\n      0.1945\n      0.2354\n      0.2898\n      0.2812\n      0.1578\n      0.0273\n      0.0673\n      0.1444\n      0.2070\n      0.2645\n      0.2828\n      0.4293\n      0.5685\n      0.6990\n      0.7246\n      0.7622\n      0.9242\n      1.0000\n      0.9979\n      0.8297\n      0.7032\n      0.7141\n      0.6893\n      0.4961\n      0.2584\n      0.0969\n      0.0776\n      0.0364\n      0.1572\n      0.1823\n      0.1349\n      0.0849\n      0.0492\n      0.1367\n      0.1552\n      0.1548\n      0.1319\n      0.0985\n      0.1258\n      0.0954\n      0.0489\n      0.0241\n      0.0042\n      0.0086\n      0.0046\n      0.0126\n      0.0036\n      0.0035\n      0.0034\n      0.0079\n      0.0036\n      0.0048\n      M\n    \n    \n      207\n      0.0260\n      0.0363\n      0.0136\n      0.0272\n      0.0214\n      0.0338\n      0.0655\n      0.1400\n      0.1843\n      0.2354\n      0.2720\n      0.2442\n      0.1665\n      0.0336\n      0.1302\n      0.1708\n      0.2177\n      0.3175\n      0.3714\n      0.4552\n      0.5700\n      0.7397\n      0.8062\n      0.8837\n      0.9432\n      1.0000\n      0.9375\n      0.7603\n      0.7123\n      0.8358\n      0.7622\n      0.4567\n      0.1715\n      0.1549\n      0.1641\n      0.1869\n      0.2655\n      0.1713\n      0.0959\n      0.0768\n      0.0847\n      0.2076\n      0.2505\n      0.1862\n      0.1439\n      0.1470\n      0.0991\n      0.0041\n      0.0154\n      0.0116\n      0.0181\n      0.0146\n      0.0129\n      0.0047\n      0.0039\n      0.0061\n      0.0040\n      0.0036\n      0.0061\n      0.0115\n      M\n    \n  \n\n208 rows  61 columns\n\n\n\n\ndf.shape\n\n(208, 61)\n\n\n\ndf.describe()\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n      3\n      4\n      5\n      6\n      7\n      8\n      9\n      10\n      11\n      12\n      13\n      14\n      15\n      16\n      17\n      18\n      19\n      20\n      21\n      22\n      23\n      24\n      25\n      26\n      27\n      28\n      29\n      30\n      31\n      32\n      33\n      34\n      35\n      36\n      37\n      38\n      39\n      40\n      41\n      42\n      43\n      44\n      45\n      46\n      47\n      48\n      49\n      50\n      51\n      52\n      53\n      54\n      55\n      56\n      57\n      58\n      59\n    \n  \n  \n    \n      count\n      208.000000\n      208.000000\n      208.000000\n      208.000000\n      208.000000\n      208.000000\n      208.000000\n      208.000000\n      208.000000\n      208.000000\n      208.000000\n      208.000000\n      208.000000\n      208.000000\n      208.000000\n      208.000000\n      208.000000\n      208.000000\n      208.000000\n      208.000000\n      208.000000\n      208.000000\n      208.000000\n      208.000000\n      208.000000\n      208.000000\n      208.000000\n      208.000000\n      208.000000\n      208.000000\n      208.000000\n      208.000000\n      208.000000\n      208.000000\n      208.000000\n      208.000000\n      208.000000\n      208.000000\n      208.000000\n      208.000000\n      208.000000\n      208.000000\n      208.000000\n      208.000000\n      208.000000\n      208.000000\n      208.000000\n      208.000000\n      208.000000\n      208.000000\n      208.000000\n      208.000000\n      208.000000\n      208.000000\n      208.000000\n      208.000000\n      208.000000\n      208.000000\n      208.000000\n      208.000000\n    \n    \n      mean\n      0.029164\n      0.038437\n      0.043832\n      0.053892\n      0.075202\n      0.104570\n      0.121747\n      0.134799\n      0.178003\n      0.208259\n      0.236013\n      0.250221\n      0.273305\n      0.296568\n      0.320201\n      0.378487\n      0.415983\n      0.452318\n      0.504812\n      0.563047\n      0.609060\n      0.624275\n      0.646975\n      0.672654\n      0.675424\n      0.699866\n      0.702155\n      0.694024\n      0.642074\n      0.580928\n      0.504475\n      0.439040\n      0.417220\n      0.403233\n      0.392571\n      0.384848\n      0.363807\n      0.339657\n      0.325800\n      0.311207\n      0.289252\n      0.278293\n      0.246542\n      0.214075\n      0.197232\n      0.160631\n      0.122453\n      0.091424\n      0.051929\n      0.020424\n      0.016069\n      0.013420\n      0.010709\n      0.010941\n      0.009290\n      0.008222\n      0.007820\n      0.007949\n      0.007941\n      0.006507\n    \n    \n      std\n      0.022991\n      0.032960\n      0.038428\n      0.046528\n      0.055552\n      0.059105\n      0.061788\n      0.085152\n      0.118387\n      0.134416\n      0.132705\n      0.140072\n      0.140962\n      0.164474\n      0.205427\n      0.232650\n      0.263677\n      0.261529\n      0.257988\n      0.262653\n      0.257818\n      0.255883\n      0.250175\n      0.239116\n      0.244926\n      0.237228\n      0.245657\n      0.237189\n      0.240250\n      0.220749\n      0.213992\n      0.213237\n      0.206513\n      0.231242\n      0.259132\n      0.264121\n      0.239912\n      0.212973\n      0.199075\n      0.178662\n      0.171111\n      0.168728\n      0.138993\n      0.133291\n      0.151628\n      0.133938\n      0.086953\n      0.062417\n      0.035954\n      0.013665\n      0.012008\n      0.009634\n      0.007060\n      0.007301\n      0.007088\n      0.005736\n      0.005785\n      0.006470\n      0.006181\n      0.005031\n    \n    \n      min\n      0.001500\n      0.000600\n      0.001500\n      0.005800\n      0.006700\n      0.010200\n      0.003300\n      0.005500\n      0.007500\n      0.011300\n      0.028900\n      0.023600\n      0.018400\n      0.027300\n      0.003100\n      0.016200\n      0.034900\n      0.037500\n      0.049400\n      0.065600\n      0.051200\n      0.021900\n      0.056300\n      0.023900\n      0.024000\n      0.092100\n      0.048100\n      0.028400\n      0.014400\n      0.061300\n      0.048200\n      0.040400\n      0.047700\n      0.021200\n      0.022300\n      0.008000\n      0.035100\n      0.038300\n      0.037100\n      0.011700\n      0.036000\n      0.005600\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000800\n      0.000500\n      0.001000\n      0.000600\n      0.000400\n      0.000300\n      0.000300\n      0.000100\n      0.000600\n    \n    \n      25%\n      0.013350\n      0.016450\n      0.018950\n      0.024375\n      0.038050\n      0.067025\n      0.080900\n      0.080425\n      0.097025\n      0.111275\n      0.129250\n      0.133475\n      0.166125\n      0.175175\n      0.164625\n      0.196300\n      0.205850\n      0.242075\n      0.299075\n      0.350625\n      0.399725\n      0.406925\n      0.450225\n      0.540725\n      0.525800\n      0.544175\n      0.531900\n      0.534775\n      0.463700\n      0.411400\n      0.345550\n      0.281400\n      0.257875\n      0.217575\n      0.179375\n      0.154350\n      0.160100\n      0.174275\n      0.173975\n      0.186450\n      0.163100\n      0.158900\n      0.155200\n      0.126875\n      0.094475\n      0.068550\n      0.064250\n      0.045125\n      0.026350\n      0.011550\n      0.008425\n      0.007275\n      0.005075\n      0.005375\n      0.004150\n      0.004400\n      0.003700\n      0.003600\n      0.003675\n      0.003100\n    \n    \n      50%\n      0.022800\n      0.030800\n      0.034300\n      0.044050\n      0.062500\n      0.092150\n      0.106950\n      0.112100\n      0.152250\n      0.182400\n      0.224800\n      0.249050\n      0.263950\n      0.281100\n      0.281700\n      0.304700\n      0.308400\n      0.368300\n      0.434950\n      0.542500\n      0.617700\n      0.664900\n      0.699700\n      0.698500\n      0.721100\n      0.754500\n      0.745600\n      0.731900\n      0.680800\n      0.607150\n      0.490350\n      0.429600\n      0.391200\n      0.351050\n      0.312750\n      0.321150\n      0.306300\n      0.312700\n      0.283500\n      0.278050\n      0.259500\n      0.245100\n      0.222550\n      0.177700\n      0.148000\n      0.121350\n      0.101650\n      0.078100\n      0.044700\n      0.017900\n      0.013900\n      0.011400\n      0.009550\n      0.009300\n      0.007500\n      0.006850\n      0.005950\n      0.005800\n      0.006400\n      0.005300\n    \n    \n      75%\n      0.035550\n      0.047950\n      0.057950\n      0.064500\n      0.100275\n      0.134125\n      0.154000\n      0.169600\n      0.233425\n      0.268700\n      0.301650\n      0.331250\n      0.351250\n      0.386175\n      0.452925\n      0.535725\n      0.659425\n      0.679050\n      0.731400\n      0.809325\n      0.816975\n      0.831975\n      0.848575\n      0.872175\n      0.873725\n      0.893800\n      0.917100\n      0.900275\n      0.852125\n      0.735175\n      0.641950\n      0.580300\n      0.556125\n      0.596125\n      0.593350\n      0.556525\n      0.518900\n      0.440550\n      0.434900\n      0.424350\n      0.387525\n      0.384250\n      0.324525\n      0.271750\n      0.231550\n      0.200375\n      0.154425\n      0.120100\n      0.068525\n      0.025275\n      0.020825\n      0.016725\n      0.014900\n      0.014500\n      0.012100\n      0.010575\n      0.010425\n      0.010350\n      0.010325\n      0.008525\n    \n    \n      max\n      0.137100\n      0.233900\n      0.305900\n      0.426400\n      0.401000\n      0.382300\n      0.372900\n      0.459000\n      0.682800\n      0.710600\n      0.734200\n      0.706000\n      0.713100\n      0.997000\n      1.000000\n      0.998800\n      1.000000\n      1.000000\n      1.000000\n      1.000000\n      1.000000\n      1.000000\n      1.000000\n      1.000000\n      1.000000\n      1.000000\n      1.000000\n      1.000000\n      1.000000\n      1.000000\n      0.965700\n      0.930600\n      1.000000\n      0.964700\n      1.000000\n      1.000000\n      0.949700\n      1.000000\n      0.985700\n      0.929700\n      0.899500\n      0.824600\n      0.773300\n      0.776200\n      0.703400\n      0.729200\n      0.552200\n      0.333900\n      0.198100\n      0.082500\n      0.100400\n      0.070900\n      0.039000\n      0.035200\n      0.044700\n      0.039400\n      0.035500\n      0.044000\n      0.036400\n      0.043900\n    \n  \n\n\n\n\n\nfig = df.hist(xlabelsize=4, ylabelsize=4)\n[x.title.set_size(4) for x in fig.ravel()]\npyplot.show()\n\n\n\n\n\ndata = df.values\n\n\nX, y = data[:, :-1], data[:, -1]\n\n\nX = X.astype('float32')\ny = LabelEncoder().fit_transform(y.astype('str'))\n\n\nmodel = KNeighborsClassifier()\n\n\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n\n\nscores = cross_val_score(model, X, y, scoring='accuracy', cv=cv)\n\n\nprint(f'Accuracy: {mean(scores):.3f}   {std(scores):.3f}')\n\nAccuracy: 0.797   0.073\n\n\nBox-Cox Transform\n\ndf = read_csv(data_path + 'sonar.csv', header=None)\ndf\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n      3\n      4\n      5\n      6\n      7\n      8\n      9\n      10\n      11\n      12\n      13\n      14\n      15\n      16\n      17\n      18\n      19\n      20\n      21\n      22\n      23\n      24\n      25\n      26\n      27\n      28\n      29\n      30\n      31\n      32\n      33\n      34\n      35\n      36\n      37\n      38\n      39\n      40\n      41\n      42\n      43\n      44\n      45\n      46\n      47\n      48\n      49\n      50\n      51\n      52\n      53\n      54\n      55\n      56\n      57\n      58\n      59\n      60\n    \n  \n  \n    \n      0\n      0.0200\n      0.0371\n      0.0428\n      0.0207\n      0.0954\n      0.0986\n      0.1539\n      0.1601\n      0.3109\n      0.2111\n      0.1609\n      0.1582\n      0.2238\n      0.0645\n      0.0660\n      0.2273\n      0.3100\n      0.2999\n      0.5078\n      0.4797\n      0.5783\n      0.5071\n      0.4328\n      0.5550\n      0.6711\n      0.6415\n      0.7104\n      0.8080\n      0.6791\n      0.3857\n      0.1307\n      0.2604\n      0.5121\n      0.7547\n      0.8537\n      0.8507\n      0.6692\n      0.6097\n      0.4943\n      0.2744\n      0.0510\n      0.2834\n      0.2825\n      0.4256\n      0.2641\n      0.1386\n      0.1051\n      0.1343\n      0.0383\n      0.0324\n      0.0232\n      0.0027\n      0.0065\n      0.0159\n      0.0072\n      0.0167\n      0.0180\n      0.0084\n      0.0090\n      0.0032\n      R\n    \n    \n      1\n      0.0453\n      0.0523\n      0.0843\n      0.0689\n      0.1183\n      0.2583\n      0.2156\n      0.3481\n      0.3337\n      0.2872\n      0.4918\n      0.6552\n      0.6919\n      0.7797\n      0.7464\n      0.9444\n      1.0000\n      0.8874\n      0.8024\n      0.7818\n      0.5212\n      0.4052\n      0.3957\n      0.3914\n      0.3250\n      0.3200\n      0.3271\n      0.2767\n      0.4423\n      0.2028\n      0.3788\n      0.2947\n      0.1984\n      0.2341\n      0.1306\n      0.4182\n      0.3835\n      0.1057\n      0.1840\n      0.1970\n      0.1674\n      0.0583\n      0.1401\n      0.1628\n      0.0621\n      0.0203\n      0.0530\n      0.0742\n      0.0409\n      0.0061\n      0.0125\n      0.0084\n      0.0089\n      0.0048\n      0.0094\n      0.0191\n      0.0140\n      0.0049\n      0.0052\n      0.0044\n      R\n    \n    \n      2\n      0.0262\n      0.0582\n      0.1099\n      0.1083\n      0.0974\n      0.2280\n      0.2431\n      0.3771\n      0.5598\n      0.6194\n      0.6333\n      0.7060\n      0.5544\n      0.5320\n      0.6479\n      0.6931\n      0.6759\n      0.7551\n      0.8929\n      0.8619\n      0.7974\n      0.6737\n      0.4293\n      0.3648\n      0.5331\n      0.2413\n      0.5070\n      0.8533\n      0.6036\n      0.8514\n      0.8512\n      0.5045\n      0.1862\n      0.2709\n      0.4232\n      0.3043\n      0.6116\n      0.6756\n      0.5375\n      0.4719\n      0.4647\n      0.2587\n      0.2129\n      0.2222\n      0.2111\n      0.0176\n      0.1348\n      0.0744\n      0.0130\n      0.0106\n      0.0033\n      0.0232\n      0.0166\n      0.0095\n      0.0180\n      0.0244\n      0.0316\n      0.0164\n      0.0095\n      0.0078\n      R\n    \n    \n      3\n      0.0100\n      0.0171\n      0.0623\n      0.0205\n      0.0205\n      0.0368\n      0.1098\n      0.1276\n      0.0598\n      0.1264\n      0.0881\n      0.1992\n      0.0184\n      0.2261\n      0.1729\n      0.2131\n      0.0693\n      0.2281\n      0.4060\n      0.3973\n      0.2741\n      0.3690\n      0.5556\n      0.4846\n      0.3140\n      0.5334\n      0.5256\n      0.2520\n      0.2090\n      0.3559\n      0.6260\n      0.7340\n      0.6120\n      0.3497\n      0.3953\n      0.3012\n      0.5408\n      0.8814\n      0.9857\n      0.9167\n      0.6121\n      0.5006\n      0.3210\n      0.3202\n      0.4295\n      0.3654\n      0.2655\n      0.1576\n      0.0681\n      0.0294\n      0.0241\n      0.0121\n      0.0036\n      0.0150\n      0.0085\n      0.0073\n      0.0050\n      0.0044\n      0.0040\n      0.0117\n      R\n    \n    \n      4\n      0.0762\n      0.0666\n      0.0481\n      0.0394\n      0.0590\n      0.0649\n      0.1209\n      0.2467\n      0.3564\n      0.4459\n      0.4152\n      0.3952\n      0.4256\n      0.4135\n      0.4528\n      0.5326\n      0.7306\n      0.6193\n      0.2032\n      0.4636\n      0.4148\n      0.4292\n      0.5730\n      0.5399\n      0.3161\n      0.2285\n      0.6995\n      1.0000\n      0.7262\n      0.4724\n      0.5103\n      0.5459\n      0.2881\n      0.0981\n      0.1951\n      0.4181\n      0.4604\n      0.3217\n      0.2828\n      0.2430\n      0.1979\n      0.2444\n      0.1847\n      0.0841\n      0.0692\n      0.0528\n      0.0357\n      0.0085\n      0.0230\n      0.0046\n      0.0156\n      0.0031\n      0.0054\n      0.0105\n      0.0110\n      0.0015\n      0.0072\n      0.0048\n      0.0107\n      0.0094\n      R\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      203\n      0.0187\n      0.0346\n      0.0168\n      0.0177\n      0.0393\n      0.1630\n      0.2028\n      0.1694\n      0.2328\n      0.2684\n      0.3108\n      0.2933\n      0.2275\n      0.0994\n      0.1801\n      0.2200\n      0.2732\n      0.2862\n      0.2034\n      0.1740\n      0.4130\n      0.6879\n      0.8120\n      0.8453\n      0.8919\n      0.9300\n      0.9987\n      1.0000\n      0.8104\n      0.6199\n      0.6041\n      0.5547\n      0.4160\n      0.1472\n      0.0849\n      0.0608\n      0.0969\n      0.1411\n      0.1676\n      0.1200\n      0.1201\n      0.1036\n      0.1977\n      0.1339\n      0.0902\n      0.1085\n      0.1521\n      0.1363\n      0.0858\n      0.0290\n      0.0203\n      0.0116\n      0.0098\n      0.0199\n      0.0033\n      0.0101\n      0.0065\n      0.0115\n      0.0193\n      0.0157\n      M\n    \n    \n      204\n      0.0323\n      0.0101\n      0.0298\n      0.0564\n      0.0760\n      0.0958\n      0.0990\n      0.1018\n      0.1030\n      0.2154\n      0.3085\n      0.3425\n      0.2990\n      0.1402\n      0.1235\n      0.1534\n      0.1901\n      0.2429\n      0.2120\n      0.2395\n      0.3272\n      0.5949\n      0.8302\n      0.9045\n      0.9888\n      0.9912\n      0.9448\n      1.0000\n      0.9092\n      0.7412\n      0.7691\n      0.7117\n      0.5304\n      0.2131\n      0.0928\n      0.1297\n      0.1159\n      0.1226\n      0.1768\n      0.0345\n      0.1562\n      0.0824\n      0.1149\n      0.1694\n      0.0954\n      0.0080\n      0.0790\n      0.1255\n      0.0647\n      0.0179\n      0.0051\n      0.0061\n      0.0093\n      0.0135\n      0.0063\n      0.0063\n      0.0034\n      0.0032\n      0.0062\n      0.0067\n      M\n    \n    \n      205\n      0.0522\n      0.0437\n      0.0180\n      0.0292\n      0.0351\n      0.1171\n      0.1257\n      0.1178\n      0.1258\n      0.2529\n      0.2716\n      0.2374\n      0.1878\n      0.0983\n      0.0683\n      0.1503\n      0.1723\n      0.2339\n      0.1962\n      0.1395\n      0.3164\n      0.5888\n      0.7631\n      0.8473\n      0.9424\n      0.9986\n      0.9699\n      1.0000\n      0.8630\n      0.6979\n      0.7717\n      0.7305\n      0.5197\n      0.1786\n      0.1098\n      0.1446\n      0.1066\n      0.1440\n      0.1929\n      0.0325\n      0.1490\n      0.0328\n      0.0537\n      0.1309\n      0.0910\n      0.0757\n      0.1059\n      0.1005\n      0.0535\n      0.0235\n      0.0155\n      0.0160\n      0.0029\n      0.0051\n      0.0062\n      0.0089\n      0.0140\n      0.0138\n      0.0077\n      0.0031\n      M\n    \n    \n      206\n      0.0303\n      0.0353\n      0.0490\n      0.0608\n      0.0167\n      0.1354\n      0.1465\n      0.1123\n      0.1945\n      0.2354\n      0.2898\n      0.2812\n      0.1578\n      0.0273\n      0.0673\n      0.1444\n      0.2070\n      0.2645\n      0.2828\n      0.4293\n      0.5685\n      0.6990\n      0.7246\n      0.7622\n      0.9242\n      1.0000\n      0.9979\n      0.8297\n      0.7032\n      0.7141\n      0.6893\n      0.4961\n      0.2584\n      0.0969\n      0.0776\n      0.0364\n      0.1572\n      0.1823\n      0.1349\n      0.0849\n      0.0492\n      0.1367\n      0.1552\n      0.1548\n      0.1319\n      0.0985\n      0.1258\n      0.0954\n      0.0489\n      0.0241\n      0.0042\n      0.0086\n      0.0046\n      0.0126\n      0.0036\n      0.0035\n      0.0034\n      0.0079\n      0.0036\n      0.0048\n      M\n    \n    \n      207\n      0.0260\n      0.0363\n      0.0136\n      0.0272\n      0.0214\n      0.0338\n      0.0655\n      0.1400\n      0.1843\n      0.2354\n      0.2720\n      0.2442\n      0.1665\n      0.0336\n      0.1302\n      0.1708\n      0.2177\n      0.3175\n      0.3714\n      0.4552\n      0.5700\n      0.7397\n      0.8062\n      0.8837\n      0.9432\n      1.0000\n      0.9375\n      0.7603\n      0.7123\n      0.8358\n      0.7622\n      0.4567\n      0.1715\n      0.1549\n      0.1641\n      0.1869\n      0.2655\n      0.1713\n      0.0959\n      0.0768\n      0.0847\n      0.2076\n      0.2505\n      0.1862\n      0.1439\n      0.1470\n      0.0991\n      0.0041\n      0.0154\n      0.0116\n      0.0181\n      0.0146\n      0.0129\n      0.0047\n      0.0039\n      0.0061\n      0.0040\n      0.0036\n      0.0061\n      0.0115\n      M\n    \n  \n\n208 rows  61 columns\n\n\n\n\ndata = df.values[:, :-1]\n\n\npt = PowerTransformer(method='box-cox')\n\n\ndata = pt.fit_transform(data)\n\nValueError: The Box-Cox transformation can only be applied to strictly positive data\n\n\nYeo-Johnson Transform\n\ndf = read_csv(data_path + 'sonar.csv', header=None)\n\n\ndata = df.values[:, :-1]\n\n\npt = PowerTransformer(method='yeo-johnson')\n\n\ndata = pt.fit_transform(data)\n\n\ndf = DataFrame(data)\n\n\nfig = df.hist(xlabelsize=4, ylabelsize=4)\n[x.title.set_size(4) for x in fig.ravel()]\npyplot.show()\n\n\n\n\nTrain the model\n\ndf = read_csv(data_path + 'sonar.csv', header=None)\n\n\ndata = df.values\n\n\nX, y = data[:, :-1], data[:, -1]\n\n\nX = X.astype('float32')\ny = LabelEncoder().fit_transform(y.astype('str'))\n\n\npower = PowerTransformer(method='yeo-johnson')\n\n\nmodel = KNeighborsClassifier()\n\n\npipeline = Pipeline(steps=[('p', power), ('m', model)])\n\n\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n\n\nscores = cross_val_score(pipeline, X, y, scoring='accuracy', cv=cv)\n\n\nprint(f'Accuracy: {mean(scores): .3f}  {std(scores): .3f}')\n\nAccuracy:  0.808   0.082\n\n\nSometimes a lift in performance can be achieved by first standardizing the raw dataset prior to performing a Yeo-Johnson transform. We can explore this by adding a StandardScaler as a first step in the pipeline. The complete example is listed below.\n\ndf = read_csv(data_path + 'sonar.csv', header=None)\n\n\ndata = df.values\n\n\nX, y = data[:, :-1], data[:,-1]\n\n\nX = X.astype('float32')\ny = LabelEncoder().fit_transform(y.astype('str'))\n\n\nscaler = StandardScaler()\n\n\npower = PowerTransformer(method='yeo-johnson')\n\n\nmodel = KNeighborsClassifier()\n\n\npipeline = Pipeline(steps=[('s', scaler), ('p', power), ('m', model)])\n\n\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n\n\nscores = cross_val_score(pipeline, X, y, scoring='accuracy', cv=cv)\n\n\nprint(f'Accuracy: {mean(scores): .3f}, {std(scores): .3f}')\n\nAccuracy:  0.816,  0.077\n\n\n\n\nChange Numerical Data Distributions\nQuantile Transforms\n\nfrom sklearn.preprocessing import QuantileTransformer\n\n\ndata = randn(1000)\n\n\ndata = exp(data)\n\n\npyplot.hist(data, bins=25)\npyplot.show()\n\n\n\n\n\ndata = data.reshape(len(data), 1)\n\n\nquantile = QuantileTransformer(output_distribution='normal')\n\n\ndata_trans = quantile.fit_transform(data)\n\n\npyplot.hist(data_trans, bins=25)\npyplot.show()\n\n\n\n\nSonar Dataset\n\ndf = read_csv(data_path + 'sonar.csv', header=None)\n\n\ndata = df.values[:, :-1]\n\n\ntrans = QuantileTransformer(n_quantiles=100, output_distribution='normal')\n\n\ndata = trans.fit_transform(data)\n\n\ndf = DataFrame(data)\n\n\nfig = df.hist(xlabelsize=4, ylabelsize=4)\n[x.title.set_size(4) for x in fig.ravel()]\npyplot.show()\n\n\n\n\nNext, lets evaluate the same KNN model as the previous section, but in this case on a normal quantile transform of the dataset. The complete example is listed below.\n\ndf = read_csv(data_path + 'sonar.csv', header=None)\n\n\ndata = df.values\n\n\nX, y = data[:, :-1], data[:, -1]\n\n\nX = X.astype('float32')\n\n\ny = LabelEncoder().fit_transform(y.astype('str'))\n\n\ntrans = QuantileTransformer(n_quantiles=100, output_distribution='normal')\n\n\nmodel = KNeighborsClassifier()\n\n\npipeline = Pipeline(steps=[('t', trans), ('m', model)])\n\n\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n\n\nscores = cross_val_score(pipeline, X, y, scoring='accuracy', cv=cv)\n\n\nprint(f'Accuracy: {mean(scores):.3f}  {std(scores): .3f}')\n\nAccuracy: 0.817   0.087\n\n\nUniform Quantile Transform\n\ndf = read_csv(data_path + 'sonar.csv', header=None)\n\n\ndata = df.values[:, :-1]\n\n\ntrans = QuantileTransformer(n_quantiles=100, output_distribution='uniform')\n\n\ndata = trans.fit_transform(data)\n\n\ndf = DataFrame(data)\n\n\nfig = df.hist(xlabelsize=4, ylabelsize=4)\n[x.title.set_size(4) for x in fig.ravel()]\npyplot.show()\n\n\n\n\nNext, lets evaluate the same KNN model as the previous section, but in this case on a uniform quantile transform of the raw dataset.\n\ndf = read_csv(data_path + 'sonar.csv', header=None)\n\n\ndata = df.values\n\n\nX, y = data[:, :-1], data[:, -1]\n\n\nX = X.astype('float32')\n\n\ny = LabelEncoder().fit_transform(y.astype('str'))\n\n\ntrans = QuantileTransformer(n_quantiles=100, output_distribution='uniform')\n\n\nmodel = KNeighborsClassifier()\n\n\npipeline = Pipeline(steps=[('t', trans), ('m', model)])\n\n\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n\n\nscores = cross_val_score(pipeline, X, y, scoring='accuracy', cv=cv)\n\n\nprint(f'Accuracy: {mean(scores):.3f}  {std(scores):.3f}')\n\nAccuracy: 0.845  0.074\n\n\ned to explore the effect of the resolution of the transform on the resulting skill of the model. The example below performs this experiment and plots the mean accuracy for different n quantiles values from 1 to 99.\n\n\nTransform Numerical to Categorical Data\n\nfrom numpy.random import randn\nfrom sklearn.preprocessing import KBinsDiscretizer\nfrom matplotlib import pyplot\n\n\ndata = randn(1000)\n\n\npyplot.hist(data, bins=25)\npyplot.show()\n\n\n\n\n\ndata = data.reshape(len(data), 1)\n\n\nkbins = KBinsDiscretizer(n_bins=10, encode='ordinal', strategy='uniform')\n\n\ndata_trans = kbins.fit_transform(data)\n\n\nprint(data_trans[:10, :])\n\n[[5.]\n [6.]\n [4.]\n [5.]\n [8.]\n [6.]\n [3.]\n [7.]\n [6.]\n [4.]]\n\n\n\npyplot.hist(data_trans, bins=10)\npyplot.show()\n\n\n\n\nSonar Dataset\n\nfrom pandas import read_csv\nfrom matplotlib import pyplot\n\n\ndf = read_csv(data_path + 'sonar.csv', header=None)\n\n\nprint(df.shape)\n\n(208, 61)\n\n\n\ndf.describe()\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n      3\n      4\n      5\n      6\n      7\n      8\n      9\n      10\n      11\n      12\n      13\n      14\n      15\n      16\n      17\n      18\n      19\n      20\n      21\n      22\n      23\n      24\n      25\n      26\n      27\n      28\n      29\n      30\n      31\n      32\n      33\n      34\n      35\n      36\n      37\n      38\n      39\n      40\n      41\n      42\n      43\n      44\n      45\n      46\n      47\n      48\n      49\n      50\n      51\n      52\n      53\n      54\n      55\n      56\n      57\n      58\n      59\n    \n  \n  \n    \n      count\n      208.000000\n      208.000000\n      208.000000\n      208.000000\n      208.000000\n      208.000000\n      208.000000\n      208.000000\n      208.000000\n      208.000000\n      208.000000\n      208.000000\n      208.000000\n      208.000000\n      208.000000\n      208.000000\n      208.000000\n      208.000000\n      208.000000\n      208.000000\n      208.000000\n      208.000000\n      208.000000\n      208.000000\n      208.000000\n      208.000000\n      208.000000\n      208.000000\n      208.000000\n      208.000000\n      208.000000\n      208.000000\n      208.000000\n      208.000000\n      208.000000\n      208.000000\n      208.000000\n      208.000000\n      208.000000\n      208.000000\n      208.000000\n      208.000000\n      208.000000\n      208.000000\n      208.000000\n      208.000000\n      208.000000\n      208.000000\n      208.000000\n      208.000000\n      208.000000\n      208.000000\n      208.000000\n      208.000000\n      208.000000\n      208.000000\n      208.000000\n      208.000000\n      208.000000\n      208.000000\n    \n    \n      mean\n      0.029164\n      0.038437\n      0.043832\n      0.053892\n      0.075202\n      0.104570\n      0.121747\n      0.134799\n      0.178003\n      0.208259\n      0.236013\n      0.250221\n      0.273305\n      0.296568\n      0.320201\n      0.378487\n      0.415983\n      0.452318\n      0.504812\n      0.563047\n      0.609060\n      0.624275\n      0.646975\n      0.672654\n      0.675424\n      0.699866\n      0.702155\n      0.694024\n      0.642074\n      0.580928\n      0.504475\n      0.439040\n      0.417220\n      0.403233\n      0.392571\n      0.384848\n      0.363807\n      0.339657\n      0.325800\n      0.311207\n      0.289252\n      0.278293\n      0.246542\n      0.214075\n      0.197232\n      0.160631\n      0.122453\n      0.091424\n      0.051929\n      0.020424\n      0.016069\n      0.013420\n      0.010709\n      0.010941\n      0.009290\n      0.008222\n      0.007820\n      0.007949\n      0.007941\n      0.006507\n    \n    \n      std\n      0.022991\n      0.032960\n      0.038428\n      0.046528\n      0.055552\n      0.059105\n      0.061788\n      0.085152\n      0.118387\n      0.134416\n      0.132705\n      0.140072\n      0.140962\n      0.164474\n      0.205427\n      0.232650\n      0.263677\n      0.261529\n      0.257988\n      0.262653\n      0.257818\n      0.255883\n      0.250175\n      0.239116\n      0.244926\n      0.237228\n      0.245657\n      0.237189\n      0.240250\n      0.220749\n      0.213992\n      0.213237\n      0.206513\n      0.231242\n      0.259132\n      0.264121\n      0.239912\n      0.212973\n      0.199075\n      0.178662\n      0.171111\n      0.168728\n      0.138993\n      0.133291\n      0.151628\n      0.133938\n      0.086953\n      0.062417\n      0.035954\n      0.013665\n      0.012008\n      0.009634\n      0.007060\n      0.007301\n      0.007088\n      0.005736\n      0.005785\n      0.006470\n      0.006181\n      0.005031\n    \n    \n      min\n      0.001500\n      0.000600\n      0.001500\n      0.005800\n      0.006700\n      0.010200\n      0.003300\n      0.005500\n      0.007500\n      0.011300\n      0.028900\n      0.023600\n      0.018400\n      0.027300\n      0.003100\n      0.016200\n      0.034900\n      0.037500\n      0.049400\n      0.065600\n      0.051200\n      0.021900\n      0.056300\n      0.023900\n      0.024000\n      0.092100\n      0.048100\n      0.028400\n      0.014400\n      0.061300\n      0.048200\n      0.040400\n      0.047700\n      0.021200\n      0.022300\n      0.008000\n      0.035100\n      0.038300\n      0.037100\n      0.011700\n      0.036000\n      0.005600\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000800\n      0.000500\n      0.001000\n      0.000600\n      0.000400\n      0.000300\n      0.000300\n      0.000100\n      0.000600\n    \n    \n      25%\n      0.013350\n      0.016450\n      0.018950\n      0.024375\n      0.038050\n      0.067025\n      0.080900\n      0.080425\n      0.097025\n      0.111275\n      0.129250\n      0.133475\n      0.166125\n      0.175175\n      0.164625\n      0.196300\n      0.205850\n      0.242075\n      0.299075\n      0.350625\n      0.399725\n      0.406925\n      0.450225\n      0.540725\n      0.525800\n      0.544175\n      0.531900\n      0.534775\n      0.463700\n      0.411400\n      0.345550\n      0.281400\n      0.257875\n      0.217575\n      0.179375\n      0.154350\n      0.160100\n      0.174275\n      0.173975\n      0.186450\n      0.163100\n      0.158900\n      0.155200\n      0.126875\n      0.094475\n      0.068550\n      0.064250\n      0.045125\n      0.026350\n      0.011550\n      0.008425\n      0.007275\n      0.005075\n      0.005375\n      0.004150\n      0.004400\n      0.003700\n      0.003600\n      0.003675\n      0.003100\n    \n    \n      50%\n      0.022800\n      0.030800\n      0.034300\n      0.044050\n      0.062500\n      0.092150\n      0.106950\n      0.112100\n      0.152250\n      0.182400\n      0.224800\n      0.249050\n      0.263950\n      0.281100\n      0.281700\n      0.304700\n      0.308400\n      0.368300\n      0.434950\n      0.542500\n      0.617700\n      0.664900\n      0.699700\n      0.698500\n      0.721100\n      0.754500\n      0.745600\n      0.731900\n      0.680800\n      0.607150\n      0.490350\n      0.429600\n      0.391200\n      0.351050\n      0.312750\n      0.321150\n      0.306300\n      0.312700\n      0.283500\n      0.278050\n      0.259500\n      0.245100\n      0.222550\n      0.177700\n      0.148000\n      0.121350\n      0.101650\n      0.078100\n      0.044700\n      0.017900\n      0.013900\n      0.011400\n      0.009550\n      0.009300\n      0.007500\n      0.006850\n      0.005950\n      0.005800\n      0.006400\n      0.005300\n    \n    \n      75%\n      0.035550\n      0.047950\n      0.057950\n      0.064500\n      0.100275\n      0.134125\n      0.154000\n      0.169600\n      0.233425\n      0.268700\n      0.301650\n      0.331250\n      0.351250\n      0.386175\n      0.452925\n      0.535725\n      0.659425\n      0.679050\n      0.731400\n      0.809325\n      0.816975\n      0.831975\n      0.848575\n      0.872175\n      0.873725\n      0.893800\n      0.917100\n      0.900275\n      0.852125\n      0.735175\n      0.641950\n      0.580300\n      0.556125\n      0.596125\n      0.593350\n      0.556525\n      0.518900\n      0.440550\n      0.434900\n      0.424350\n      0.387525\n      0.384250\n      0.324525\n      0.271750\n      0.231550\n      0.200375\n      0.154425\n      0.120100\n      0.068525\n      0.025275\n      0.020825\n      0.016725\n      0.014900\n      0.014500\n      0.012100\n      0.010575\n      0.010425\n      0.010350\n      0.010325\n      0.008525\n    \n    \n      max\n      0.137100\n      0.233900\n      0.305900\n      0.426400\n      0.401000\n      0.382300\n      0.372900\n      0.459000\n      0.682800\n      0.710600\n      0.734200\n      0.706000\n      0.713100\n      0.997000\n      1.000000\n      0.998800\n      1.000000\n      1.000000\n      1.000000\n      1.000000\n      1.000000\n      1.000000\n      1.000000\n      1.000000\n      1.000000\n      1.000000\n      1.000000\n      1.000000\n      1.000000\n      1.000000\n      0.965700\n      0.930600\n      1.000000\n      0.964700\n      1.000000\n      1.000000\n      0.949700\n      1.000000\n      0.985700\n      0.929700\n      0.899500\n      0.824600\n      0.773300\n      0.776200\n      0.703400\n      0.729200\n      0.552200\n      0.333900\n      0.198100\n      0.082500\n      0.100400\n      0.070900\n      0.039000\n      0.035200\n      0.044700\n      0.039400\n      0.035500\n      0.044000\n      0.036400\n      0.043900\n    \n  \n\n\n\n\n\nfig = df.hist(xlabelsize=4, ylabelsize=4)\n[x.title.set_size(4) for x in fig.ravel()]\npyplot.show()\n\n\n\n\nLets fit and evaluate a machine learning model on the raw dataset.\n\nfrom numpy import mean, std\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import RepeatedStratifiedKFold, cross_val_score\n\n\ndf = read_csv(data_path + 'sonar.csv', header=None)\n\n\ndata = df.values\n\n\nX, y = data[:, :-1], data[:, -1]\n\n\nX = X.astype('float32')\ny = LabelEncoder().fit_transform(y.astype('str'))\n\n\nmodel = KNeighborsClassifier()\n\n\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n\n\nn_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv)\n\n\nprint(f'Accuracy: {mean(n_scores): .3f}, {std(n_scores): .3f}')\n\nAccuracy:  0.797,  0.073\n\n\nUniform Discretization Transform\n\nfrom pandas import DataFrame\nfrom sklearn.pipeline import Pipeline\n\n\ndf = read_csv(data_path + 'sonar.csv', header=None)\n\n\ndata = df.values[:, :-1]\n\n\ntrans = KBinsDiscretizer(n_bins=10, encode='ordinal', strategy='uniform')\n\n\ndata = trans.fit_transform(data)\n\n\ndf = DataFrame(data)\n\n\nfig = df.hist(xlabelsize=4, ylabelsize=4)\n[x.title.set_size(4) for x in fig.ravel()]\npyplot.show()\n\n\n\n\nNext, lets evaluate the same KNN model as the previous section\n\ndf = read_csv(data_path + 'sonar.csv', header=None)\n\n\ndata = df.values\n\n\nX, y = data[:, :-1], data[:, -1]\n\n\nX = X.astype('float32')\ny = LabelEncoder().fit_transform(y.astype('str'))\n\n\ntrans = KBinsDiscretizer(n_bins=10, encode='ordinal', strategy='uniform')\n\n\nmodel = KNeighborsClassifier()\n\n\npipeline = Pipeline(steps = [('t', trans), ('m', model)])\n\n\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n\n\nn_scores = cross_val_score(pipeline, X, y, scoring='accuracy', cv=cv)\n\n\nprint(f'Accuracy: {mean(n_scores):.3f}, {std(n_scores):.3f}')\n\nAccuracy: 0.829, 0.079\n\n\nk-Means Discretization Transform\n\ndf = read_csv(data_path + 'sonar.csv', header=None)\n\n\ndata = df.values[:, :-1]\n\n\ntrans = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='kmeans')\n\n\ndata = trans.fit_transform(data)\n\n\ndf = DataFrame(data)\n\n\nfig = df.hist(xlabelsize=4, ylabelsize=4)\n[x.title.set_size(4) for x in fig.ravel()]\npyplot.show()\n\n\n\n\nNext, lets evaluate the same KNN model\n\ndf = read_csv(data_path + 'sonar.csv', header=None)\n\n\ndata = df.values\n\n\nX, y = data[:, :-1], data[:, -1]\n\n\nX = X.astype('float32')\ny = LabelEncoder().fit_transform(y.astype('str'))\n\n\ntrans = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='kmeans')\n\n\nmodel = KNeighborsClassifier()\n\n\npipeline = Pipeline(steps=[('t', trans), ('m', model)])\n\n\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n\n\nn_scores = cross_val_score(pipeline, X, y, scoring='accuracy', cv=cv)\n\n\nprint(f'Accuracy: {mean(n_scores):.3f}, {std(n_scores):.3f}')\n\nAccuracy: 0.814, 0.084\n\n\nQuantile Discretization Transform\n\ndf = read_csv(data_path + 'sonar.csv', header=None)\n\n\ndata = df.values[:, :-1]\n\n\ntrans = KBinsDiscretizer(n_bins=10, encode='ordinal', strategy='quantile')\n\n\ndata = trans.fit_transform(data)\n\n\ndf = DataFrame(data)\n\n\nfig = df.hist(xlabelsize=4, ylabelsize=4)\n[x.title.set_size(4) for x in fig.ravel()]\npyplot.show()\n\n\n\n\nNext, lets evaluate the same KNN model\n\ndf = read_csv(data_path + 'sonar.csv', header=None)\n\n\ndata = df.values\n\n\nX, y = data[:, :-1], data[:, -1]\n\n\nX = X.astype('float32')\ny = LabelEncoder().fit_transform(y.astype('str'))\n\n\ntrans = KBinsDiscretizer(n_bins=10, encode='ordinal', strategy='quantile')\n\n\nmodel = KNeighborsClassifier()\n\n\npipeline = Pipeline(steps=[('t', trans), ('m', model)])\n\n\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n\n\nn_scores = cross_val_score(pipeline, X, y, scoring='accuracy', cv=cv)\n\n\nprint(f'Accuracy: {mean(n_scores):.3f} {std(n_scores): .3f}')\n\nAccuracy: 0.840  0.072\n\n\nWe chose the number of bins as an arbitrary number; in this case, 10. This hyperparameter can be tuned to explore the effect of the resolution of the transform on the resulting skill of the model.\n\ndef get_dataset(filename):\n    df = read_csv(filename, header=None)\n    data = df.values\n    X, y = data[:, :-1], data[:, -1]\n    X = X.astype('float32')\n    y = LabelEncoder().fit_transform(y.astype('str'))\n    return X, y\n\n\ndef get_models():\n    models = dict()\n    for i in range(2, 11):\n        trans = KBinsDiscretizer(n_bins=i, encode='ordinal', strategy='quantile')\n        model = KNeighborsClassifier()\n        models[str(i)] = Pipeline(steps=[('t', trans), ('m', model)])\n    return models\n\n\ndef evaluate_model(model, X, y):\n    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n    scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv)\n    return scores\n\n\nX, y = get_dataset()\n\n\nmodels = get_models()\n\n\nresults, names = list(), list()\n\n\nfor name, model in models.items():\n    scores = evaluate_model(model, X, y)\n    results.append(scores)\n    names.append(name)\n    print(f'{name}: {mean(scores): .3f} {mean(scores): .3f}')\n\n2:  0.822  0.822\n3:  0.870  0.870\n4:  0.838  0.838\n5:  0.838  0.838\n6:  0.844  0.844\n7:  0.852  0.852\n8:  0.838  0.838\n9:  0.841  0.841\n10:  0.840  0.840\n\n\n\npyplot.boxplot(results, labels=names, showmeans=True)\npyplot.show()\n\n\n\n\n\n\nDerive New Input Variables\nPolynomial Feature Transform\n\nfrom numpy import asarray\nfrom sklearn.preprocessing import PolynomialFeatures\n\n\ndata = asarray([[2,3], [2,3], [2,3]])\nprint(data)\n\n[[2 3]\n [2 3]\n [2 3]]\n\n\n\ntrans = PolynomialFeatures(degree=2)\n\n\ndata = trans.fit_transform(data)\n\n\nprint(data)\n\n[[1. 2. 3. 4. 6. 9.]\n [1. 2. 3. 4. 6. 9.]\n [1. 2. 3. 4. 6. 9.]]\n\n\nPolynomial Feature Transform Example\n\ndf = read_csv(data_path + 'sonar.csv', header=None)\n\n\ndata = df.values[:, :-1]\n\n\ntrans = PolynomialFeatures(degree=3)\n\n\ndata = trans.fit_transform(data)\n\n\ndf = DataFrame(data)\n\n\nprint(df.shape)\n\n(208, 39711)\n\n\nNext, lets evaluate the same KNN model\n\ndf = read_csv(data_path + 'sonar.csv', header=None)\n\n\ndata = df.values\n\n\nX, y = data[:, :-1], data[:, -1]\n\n\nX = X.astype('float32')\n\n\ny = LabelEncoder().fit_transform(y.astype('str'))\n\n\ntrans = PolynomialFeatures(degree=3)\n\n\nmodel = KNeighborsClassifier()\n\n\npipeline = Pipeline(steps=[('t', trans), ('m', model)])\n\n\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n\n\nn_scores = cross_val_score(pipeline, X, y, scoring='accuracy', cv=cv)\n\n\nprint(f'Accuracy: {mean(n_scores):.3f} {std(n_scores): .3f}')\n\nAccuracy: 0.800  0.077\n\n\nEffect of Polynomial Degree\n\nX, y = get_dataset(data_path + 'sonar.csv')\n\n\nnum_feature = list()\n\n\ndegrees = [i for i in range(1, 6)]\n\n\nfor d in degrees:\n    trans = PolynomialFeatures(degree=d)\n    data = trans.fit_transform(X)\n    num_feature.append(data.shape[1])\n    print(f'Degree: {d}, Features: {data.shape[1]}')\n\nDegree: 1, Features: 61\nDegree: 2, Features: 1891\nDegree: 3, Features: 39711\nDegree: 4, Features: 635376\nDegree: 5, Features: 8259888\n\n\n\npyplot.plot(degrees, num_feature)\npyplot.show()\n\n\n\n\nIt may be a good idea to treat the degree for the polynomial features transform as a hyperparameter and test different values for your dataset.\n\ndef get_models():\n    models = dict()\n    for d in range(1,5):\n        trans = PolynomialFeatures(degree=d)\n        model = KNeighborsClassifier()\n        models[str(d)] = Pipeline(steps=[('t', trans), ('m', model)])\n    return models\n\n\nX, y = get_dataset(data_path + 'sonar.csv')\n\n\nmodels = get_models()\n\n\nresults, names = list(), list()\n\n\nfor name, model in models.items():\n    scores = evaluate_model(model, X, y)\n    results.append(scores)\n    names.append(name)\n    print(f'{name}: {mean(scores):.3f} {std(scores):.3f}')\n\n1: 0.797 0.073\n2: 0.793 0.085\n3: 0.800 0.077\n4: 0.795 0.079\n\n\n\npyplot.boxplot(results, labels=names, showmeans=True)\npyplot.show()\n\n\n\n\nAdvanced Transforms\nTransform Both Numerical and Categorical Data\nData Preparation for the Abalone Regression Dataset\n\nfrom numpy import absolute\nfrom sklearn.preprocessing import OneHotEncoder, MinMaxScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.svm import SVR\nfrom sklearn.model_selection import KFold\n\n\ndf = read_csv(data_path + 'abalone.csv', header=None)\n\n\nlast_ix = len(df.columns) - 1\n\n\nX, y = df.drop(last_ix, axis=1), df[last_ix]\n\n\nprint(X.shape, y.shape)\n\n(4177, 8) (4177,)\n\n\n\nnumerical_ix = X.select_dtypes(include=['int64', 'float64']).columns\ncategorical_ix = X.select_dtypes(include=['object', 'bool']).columns\n\n\nt = [('cat', OneHotEncoder(), categorical_ix), ('num', MinMaxScaler(), numerical_ix)]\n\n\ncol_transform = ColumnTransformer(transformers=t)\n\n\nmodel = SVR(kernel='rbf', gamma = 'scale', C=100)\n\n\npipeline = Pipeline(steps=[('prep', col_transform), ('m', model)])\n\n\ncv = KFold(n_splits=10, shuffle=True, random_state=1)\n\n\nscores = cross_val_score(pipeline, X, y, scoring='neg_mean_absolute_error', cv=cv)\n\n\nscores = absolute(scores)\n\n\nprint(f'MAE: {mean(scores):.3f} {std(scores): .3f}')\n\nMAE: 1.465  0.047\n\n\nTransform the Target in Regression\nExample of Using the TransformedTargetRegressor\n\nfrom numpy import loadtxt\nfrom sklearn.linear_model import HuberRegressor\nfrom sklearn.compose import TransformedTargetRegressor\nfrom sklearn.model_selection import RepeatedKFold\n\n\ndata = loadtxt(data_path + 'boston-housing.csv')\n\n\nX, y = data[:, :-1], data[:, -1]\n\n\npipeline = Pipeline(steps=[('normalize', MinMaxScaler()), ('model', HuberRegressor())])\n\n\nmodel = TransformedTargetRegressor(regressor=pipeline, transformer=MinMaxScaler())\n\n\ncv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n\n\nscores = cross_val_score(model, X, y, scoring='neg_mean_absolute_error', cv=cv)\n\n\nscores=absolute(scores)\n\n\nprint(f'Mean: {mean(scores):.3f}')\n\nMean: 3.203\n\n\nWe are not restricted to using scaling objects; for example, we can also explore using other data transforms on the target variable, such as the PowerTransformer\n\nfrom sklearn.preprocessing import PowerTransformer\n\n\ndata = loadtxt(data_path + 'boston-housing.csv')\n\n\nX, y = data[:, :-1], data[:, -1]\n\n\nsteps = list()\n\n\nsteps.append(('scale', MinMaxScaler(feature_range=(1e-5, 1))))\nsteps.append(('power', PowerTransformer()))\nsteps.append(('model', HuberRegressor()))\n\n\npipeline = Pipeline(steps=steps)\n\n\nmodel = TransformedTargetRegressor(regressor=pipeline, transformer=PowerTransformer())\n\n\ncv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n\n\nscores = cross_val_score(model, X, y, scoring='neg_mean_absolute_error', cv=cv)\n\n\nscores = absolute(scores)\n\n\nprint(f'Mean: {mean(scores): .3f}')\n\nMean:  2.972\n\n\nHow to Save and Load Data Transforms\nWorked Example of Saving Data Preparatio\nDefine a Dataset\n\nfrom sklearn.datasets import make_blobs\nfrom sklearn.model_selection import train_test_split\n\n\nX, y = make_blobs(n_samples=100, centers=2, n_features=2, random_state=1)\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n\n\nfor i in range(X_test.shape[1]):\n    print(f'{i} > train: min={X_train[:, i].min():.3f}, max={X_train[:,i].max(): .3f}, test: min={X_test[:,i].min():.3f}, max={X_test[:,i].max():.3f}')\n\n0 > train: min=-11.856, max= 0.526, test: min=-11.270, max=0.085\n1 > train: min=-6.388, max= 6.507, test: min=-5.581, max=5.926\n\n\nScale the Dataset\n\nfrom sklearn.linear_model import LogisticRegression\nfrom pickle import dump\n\n\nX, y = make_blobs(n_samples=100, centers=2, n_features=2, random_state=1)\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n\n\nscaler = MinMaxScaler()\n\n\nscaler.fit(X_train)\n\nMinMaxScaler()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.MinMaxScalerMinMaxScaler()\n\n\n\nX_train_scaled = scaler.transform(X_train)\n\n\nX_test_scaled = scaler.transform(X_test)\n\n\nfor i in range(X_test.shape[1]):\n        print(f'{i} > train: min={X_train_scaled[:, i].min():.3f}, max={X_train_scaled[:,i].max(): .3f}, test: min={X_test_scaled[:,i].min():.3f}, max={X_test_scaled[:,i].max():.3f}')\n\n0 > train: min=0.000, max= 1.000, test: min=0.047, max=0.964\n1 > train: min=0.000, max= 1.000, test: min=0.063, max=0.955\n\n\nSave Model and Data Scaler\n\nX, y = make_blobs(n_samples=100, centers=2, n_features=2, random_state=1)\n\n\nX_tarin, _, y_train, _ = train_test_split(X, y, test_size=0.33, random_state=1)\n\n\nscaler = MinMaxScaler()\n\n\nscaler.fit(X_tarin)\n\nMinMaxScaler()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.MinMaxScalerMinMaxScaler()\n\n\n\nX_train_scaled = scaler.transform(X_train)\n\n\nmodel = LogisticRegression(solver='lbfgs')\n\n\nmodel.fit(X_train_scaled, y_train)\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression()\n\n\n\ndump(model, open('model.pkl', 'wb'))\n\n\ndump(scaler, open('scaler.pkl', 'wb'))\n\nLoad Model and Data Scaler\n\nfrom pickle import load\n\n\nX, y = make_blobs(n_samples=100, centers=2, n_features=2, random_state=1)\n\n\n_, X_test, _, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n\n\nscaler = load(open('model.pkl'))"
  },
  {
    "objectID": "03-data-preparation/feature-selection.html",
    "href": "03-data-preparation/feature-selection.html",
    "title": "Feature Selection",
    "section": "",
    "text": "data_path = '/home/naji/Desktop/github-repos/machine-learning/nbs/0-datasets/data/'"
  },
  {
    "objectID": "03-data-preparation/feature-selection.html#categorical-feature-selection",
    "href": "03-data-preparation/feature-selection.html#categorical-feature-selection",
    "title": "Feature Selection",
    "section": "Categorical Feature Selection",
    "text": "Categorical Feature Selection\n\nChi-Squared Feature Selection\n\nfrom sklearn.feature_selection import SelectKBest, chi2\n\n\ndef select_features(X_train, y_train, X_test, k='all'):\n    fs = SelectKBest(score_func=chi2, k=k)\n    fs.fit(X_train, y_train)\n    X_train_fs = fs.transform(X_train)\n    X_test_fs = fs.transform(X_test)\n    return X_train_fs, X_test_fs, fs\n\n\nX, y = load_data('breast-cancer.csv')\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n\n\nX_train_enc, X_test_enc = prepare_inputs(X_train, X_test)\n\n\ny_train_enc, y_test_enc = prepare_targets(y_train, y_test)\n\n\nX_train_fs, X_test_fs, fs = select_features(X_train_enc, y_train_enc, X_test_enc)\n\n\nfor i in range(len(fs.scores_)):\n    print(f'Features {i}: {fs.scores_[i]:.3f}')\n\n\npyplot.bar([i for i in range(len(fs.scores_))], fs.scores_)\npyplot.show()\n\nModel Built Using Chi-Squared Features\n\nmodel = LogisticRegression(solver='lbfgs')\n\n\nmodel.fit(X_train_fs, y_train_enc)\n\n\nyhat = model.predict(X_test_fs)\n\n\naccuracy = accuracy_score(y_test_enc, yhat)\n\n\nprint(f'Accuracy: {accuracy*100:.3f}%')\n\n\n\nMutual Information Feature Selection\n\nfrom sklearn.feature_selection import SelectKBest, mutual_info_classif\n\n\ndef select_features(X_train, y_train, X_test, k='all'):\n    fs = SelectKBest(score_func=mutual_info_classif, k=k)\n    fs.fit(X_train, y_train)\n    X_train_fs = fs.transform(X_train)\n    X_test_fs = fs.transform(X_test)\n    return X_train_fs, X_test_fs, fs\n\n\nX, y = load_data('breast-cancer.csv')\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n\n\nX_train_enc, X_test_enc = prepare_inputs(X_train, X_test)\n\n\ny_train_enc, y_test_enc = prepare_targets(y_train, y_test)\n\n\nX_train_fs, X_test_fs, fs = select_features(X_train_enc, y_train_enc, X_test_enc)\n\n\nfor i in range(len(fs.scores_)):\n    print(f'Feature {i}: {fs.scores_[i]:.3f}')\n\n\npyplot.bar([i for i in range(len(fs.scores_))], fs.scores_)\npyplot.show()\n\nModel Built Using Mutual Information Features\n\nmodel = LogisticRegression(solver='lbfgs')\n\n\nmodel.fit(X_train_fs, y_train_enc)\n\n\nyhat = model.predict(X_test_fs)\n\n\naccuracy = accuracy_score(y_test_enc, yhat)\n\n\nprint(f'Accuracy: {accuracy*100: 0.2f}%')"
  },
  {
    "objectID": "03-data-preparation/feature-selection.html#numerical-feature-selection",
    "href": "03-data-preparation/feature-selection.html#numerical-feature-selection",
    "title": "Feature Selection",
    "section": "Numerical Feature Selection",
    "text": "Numerical Feature Selection\n\nfrom sklearn.feature_selection import SelectKBest, f_classif\n\n\nANOVA F-test Feature Selection\n\ndef select_features(X_train, y_train, X_test, k='all'):\n    fs = SelectKBest(score_func=f_classif, k=k)\n    fs.fit(X_train, y_train)\n    X_train_fs = fs.transform(X_train)\n    X_test_fs = fs.transform(X_test)\n    return X_train_fs, X_test_fs, fs\n\n\nX, y = load_data('pima-indians-diabetes.csv')\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n\n\nX_train_fs, X_test_fs, fs = select_features(X_train, y_train, X_test)\n\n\nfor i in range(len(fs.scores_)):\n    print(f'Feature {i}: {fs.scores_[i]: .3f}')\n\n\npyplot.bar([i for i in range(len(fs.scores_))], fs.scores_)\npyplot.show()\n\nModel Built Using ANOVA F-test Features\n\nmodel = LogisticRegression(solver='liblinear')\n\n\nmodel.fit(X_train_fs, y_train)\n\n\nyhat = model.predict(X_test_fs)\n\n\naccuracy = accuracy_score(y_test, yhat)\n\n\nprint(f'Accuracy: {accuracy*100:.3f}')\n\n\n\nMutual Information Feature Selection\n\nfrom sklearn.feature_selection import SelectKBest, mutual_info_classif\n\n\ndef select_features(X_train, y_tarin, X_test, k='all'):\n    fs = SelectKBest(score_func=mutual_info_classif, k=k)\n    fs.fit(X_train, y_train)\n    X_train_fs = fs.transform(X_train)\n    X_test_fs = fs.transform(X_test)\n    return X_train_fs, X_test_fs, fs"
  },
  {
    "objectID": "03-data-preparation/data-leakage.html",
    "href": "03-data-preparation/data-leakage.html",
    "title": "Data Leakage",
    "section": "",
    "text": "from sklearn.datasets import make_classification\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.linear_model import LogisticRegression"
  },
  {
    "objectID": "03-data-preparation/data-leakage.html#data-preparation-with-train-and-test-sets",
    "href": "03-data-preparation/data-leakage.html#data-preparation-with-train-and-test-sets",
    "title": "Data Leakage",
    "section": "Data Preparation With Train and Test Sets",
    "text": "Data Preparation With Train and Test Sets\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\nTrain-Test Evaluation With Naive Data Preparation\n\nX, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=7)\n\n\n# standardize the dataset\nscaler = MinMaxScaler()\nX = scaler.fit_transform(X)\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n\n\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train)\n\n\nyhat = model.predict(X_test)\n\n\naccuracy = accuracy_score(y_test, yhat)\n\n\nprint(f'{accuracy*100: 0.3f}%')\n\nTrain-Test Evaluation With Correct Data Preparation\n\nX, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=7)\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n\n\nscaler = MinMaxScaler()\n\n\nscaler.fit(X_train)\n\n\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)\n\n\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train)\n\n\nyhat = model.predict(X_test)\n\n\naccuracy = accuracy_score(y_test, yhat)\nprint(f'{accuracy*100: .3f}%')"
  },
  {
    "objectID": "03-data-preparation/data-leakage.html#data-preparation-with-k-fold-cross-validation",
    "href": "03-data-preparation/data-leakage.html#data-preparation-with-k-fold-cross-validation",
    "title": "Data Leakage",
    "section": "Data Preparation With k-fold Cross-Validation",
    "text": "Data Preparation With k-fold Cross-Validation\n\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.pipeline import Pipeline\n\nCross-Validation Evaluation With Naive Data Preparation\n\nX, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=7)\n\n\nscaler = MinMaxScaler()\n\n\nX = scaler.fit_transform(X)\n\n\nmodel = LogisticRegression()\n\n\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n\n\nscores = cross_val_score(model, X, y, scoring='accuracy', cv=cv)\n\n\nprint(f'{scores.mean()*100: .3f} ({scores.std()*100: .3f})')\n\n 85.300 ( 3.607)\n\n\nCross-Validation Evaluation With Correct Data Preparation\n\nX, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=7)\n\n\nsteps = list()\nsteps.append(('scaler', MinMaxScaler()))\nsteps.append(('model', LogisticRegression()))\n\n\npipeline = Pipeline(steps=steps)\n\n\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n\n\nscores = cross_val_score(pipeline, X, y, scoring='accuracy', cv=cv)\n\n\nprint(f'{scores.mean()*100: .3f} ({scores.std()*100:.3f})')\n\n 85.433 (3.471)"
  },
  {
    "objectID": "02-end-to-end/steps.html",
    "href": "02-end-to-end/steps.html",
    "title": "Machine Learning Steps",
    "section": "",
    "text": "filepath = '/home/naji/Desktop/github-repos/machine-learning/nbs/data/'\npima = 'pima-indians-diabetes.csv'\nhousing = 'housing.csv'"
  },
  {
    "objectID": "02-end-to-end/steps.html#analyze-data",
    "href": "02-end-to-end/steps.html#analyze-data",
    "title": "Machine Learning Steps",
    "section": "Analyze Data",
    "text": "Analyze Data\n\nLoad Machine Learning Data\nYou must be able to load your data before you can start your machine learning project. The most common format for machine learning data is CSV files. There are a number of ways to load a CSV file in Python. In this lesson you will learn three ways that you can use to load your CSV data in Python: 1. Load CSV Files with the Python Standard Library. 2. Load CSV Files with NumPy. 3. Load CSV Files with Pandas.\nConsiderations When Loading CSV Data\nFile Header\nDoes your data have a file header? If so this can help in automatically assigning names to each column of data. If not, you may need to name your attributes manually. Either way, you should explicitly specify whether or not your CSV file has a file header when loading your data.\nComments\nDoes your data have comments? Comments in a CSV file are indicated by a hash (#) at the start of a line. If you have comments in your file, depending on the method used to load your data, you may need to indicate whether or not to expect comments and the character to expect to signify a comment line.\nDelimiter\nThe standard delimiter that separates values in fields is the comma (,) character. Your file could use a different delimiter like tab or white space in which case you must specify it explicitly.\nQuotes\nSometimes field values can have spaces. In these CSV files the values are often quoted. The default quote character is the double quotation marks character. Other characters can be used, and you must specify the quote character used in your file.\nPima Indians Dataset\nhttps://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\n\nLoad CSV Files with the Python Standard Library\nThe Python API provides the module CSV and the function reader() that can be used to load CSV files. Once loaded, you can convert the CSV data to a NumPy array and use it for machine learning.\n\nimport csv\nimport numpy\n\n\nraw_data = open(filepath+filename, 'rt')\nreader = csv.reader(raw_data, delimiter=',', quoting=csv.QUOTE_NONE)\n\n\nx = list(reader)\nx[:10]\n\n\ndata = numpy.array(x).astype('float')\nprint(data.shape)\n\n\ndata\n\n\n\nLoad CSV Files with NumPy\nYou can load your CSV data using NumPy and the numpy.loadtxt() function. This function assumes no header row and all data has the same format.\n\nfrom numpy import loadtxt\n\n\nraw_data = open(filepath + filename, 'rt')\n\n\ndata = loadtxt(raw_data, delimiter=',')\n\n\nprint(data.shape)\n\nThis example can be modified to load the same dataset directly from a URL as follows:\n\nfrom urllib.request import urlopen\n\n\nurl = 'https://goo.gl/bDdBiA'\n\n\nraw_data = urlopen(url)\n\n\ndata = loadtxt(raw_data, delimiter=',')\n\n\nprint(data.shape)\n\n\n\nLoad CSV Files with Pandas\nYou can load your CSV data using Pandas and the pandas.read csv() function. This function is very flexible and is perhaps my recommended approach for loading your machine learning data. The function returns a pandas.DataFrame 6 that you can immediately start summarizing and plotting.\n\nfrom pandas import read_csv\n\n\nnames = ['preg','plas','pres','skin','test','mass','pedi','age','class']\n\n\ndata = read_csv(filepath + filename, names=names)\n\n\ndata.shape\n\n\ndata\n\nWe can also modify this example to load CSV data directly from a URL.\n\nurl='https://goo.gl/bDdBiA'\n\n\ndata = read_csv(url, names=names)\n\n\ndata.shape\n\nGenerally I recommend that you load your data with Pandas in practice and all subsequent examples in this book will use this method.\n\n\n\nUnderstand Your Data With Descriptive Statistics\nYou must understand your data in order to get the best results. In this chapter you will discover 7 recipes that you can use in Python to better understand your machine learning data. After reading this lesson you will know how to: 1. Take a peek at your raw data. 2. Review the dimensions of your dataset. 3. Review the data types of attributes in your data. 4. Summarize the distribution of instances across classes in your dataset. 5. Summarize your data using descriptive statistics. 6. Understand the relationships in your data using correlations. 7. Review the skew of the distributions of each attribute.\n\nPeek at Your Data\nThere is no substitute for looking at the raw data. Looking at the raw data can reveal insights that you cannot get any other way. It can also plant seeds that may later grow into ideas on how to better pre-process and handle the data for machine learning tasks.\n\nfrom pandas import read_csv\n\n\nnames = ['preg','plas','pres','skin','test','mass','pedi','age','class']\n\n\ndata = read_csv(filepath + filename, names=names)\n\n\ndata.head(20)\n\n\n\nDimensions of Your Data\nYou must have a very good handle on how much data you have, both in terms of rows and columns.  Too many rows and algorithms may take too long to train. Too few and perhaps you do not have enough data to train the algorithms.  Too many features and some algorithms can be distracted or suffer poor performance due to the curse of dimensionality.\n\nprint(data.shape)\n\n\n\nData Type For Each Attribute\nThe type of each attribute is important. Strings may need to be converted to floating point values or integers to represent categorical or ordinal values. You can get an idea of the types of attributes by peeking at the raw data, as above.\n\ndata.dtypes\n\n\n\nDescriptive Statistics\nDescriptive statistics can give you great insight into the properties of each attribute. Often you can create more summaries than you have time to review. The describe() function on the Pandas DataFrame lists 8 statistical properties of each attribute. They are:  Count.  Mean.  Standard Deviation.  Minimum Value.  25th Percentile.  50th Percentile (Median).  75th Percentile.  Maximum Value.\n\nfrom pandas import set_option\n\n\nset_option('display.width', 100)\nset_option('display.precision', 3)\n\n\ndata.describe()\n\n\n\nClass Distribution (Classification Only)\nOn classification problems you need to know how balanced the class values are. Highly imbalanced problems (a lot more observations for one class than another) are common and may need special handling in the data preparation stage of your project.\n\nclass_coutns = data.groupby('class').size()\n\n\nprint(class_coutns)\n\n\n\nCorrelations Between Attributes\nCorrelation refers to the relationship between two variables and how they may or may not change together. The most common method for calculating correlation is Pearsons Correlation Coefficient, that assumes a normal distribution of the attributes involved. A correlation of -1 or 1 shows a full negative or positive correlation respectively. Whereas a value of 0 shows no correlation at all. Some machine learning algorithms like linear and logistic regression can suffer poor performance if there are highly correlated attributes in your dataset. As such, it is a good idea to review all of the pairwise correlations of the attributes in your dataset.\n\nset_option('display.width', 100)\nset_option('display.precision', 3)\n\n\ndata.corr(method='pearson')\n\n\n\nSkew of Univariate Distributions\nSkew refers to a distribution that is assumed Gaussian (normal or bell curve) that is shifted or squashed in one direction or another. Many machine learning algorithms assume a Gaussian distribution. Knowing that an attribute has a skew may allow you to perform data preparation to correct the skew and later improve the accuracy of your models.\n\ndata.skew()\n\nThe skew results show a positive (right) or negative (left) skew. Values closer to zero show less skew.\nTips To Remember\nThis section gives you some tips to remember when reviewing your data using summary statistics.  Review the numbers. Generating the summary statistics is not enough. Take a moment to pause, read and really think about the numbers you are seeing.  Ask why. Review your numbers and ask a lot of questions. How and why are you seeing specific values. Think about how the numbers relate to the problem domain in general and specific entities that observations relate to.  Write down ideas. Write down your observations and ideas. Keep a small text file or note pad and jot down all of the ideas for how variables may relate, for what numbers mean, and ideas for techniques to try later. The things you write down now while the data is fresh will be very valuable later when you are trying to think up new things to try.\nSummary\nIn this chapter you discovered the importance of describing your dataset before you start work on your machine learning project. You discovered 7 different ways to summarize your dataset using Python and Pandas:  Peek At Your Data.  Dimensions of Your Data.  Data Types.  Class Distribution.  Data Summary.  Correlations.  Skewness.\n\n\n\nUnderstand Your Data With Visualization\nYou must understand your data in order to get the best results from machine learning algorithms. The fastest way to learn more about your data is to use data visualization. In this chapter you will discover exactly how you can visualize your machine learning data in Python using Pandas.\n\nUnivariate Plots\nIn this section we will look at three techniques that you can use to understand each attribute of your dataset independently.\n\nHistograms.\nDensity Plots.\nBox and Whisker Plots.\n\n\nHistograms\n\nfrom matplotlib import pyplot\n\nA fast way to get an idea of the distribution of each attribute is to look at histograms. Histograms group data into bins and provide you a count of the number of observations in each bin. From the shape of the bins you can quickly get a feeling for whether an attribute is Gaussian, skewed or even has an exponential distribution. It can also help you see possible outliers.\n\ndata\n\n\ndata.hist(figsize=[10,10])\npyplot.show()\n\nWe can see that perhaps the attributes age, pedi and test may have an exponential distribution. We can also see that perhaps the mass and pres and plas attributes may have a Gaussian or nearly Gaussian distribution. This is interesting because many machine learning techniques assume a Gaussian univariate distribution on the input variables.\n\n\nDensity Plots\nDensity plots are another way of getting a quick idea of the distribution of each attribute. The plots look like an abstracted histogram with a smooth curve drawn through the top of each bin, much like your eye tried to do with the histograms.\n\ndata.plot(kind='density', subplots=True, layout=(3,3), sharex=False)\npyplot.show()\n\n\n\nBox and Whisker Plots\nAnother useful way to review the distribution of each attribute is to use Box and Whisker Plots or boxplots for short. Boxplots summarize the distribution of each attribute, drawing a line for the median (middle value) and a box around the 25th and 75th percentiles (the middle 50% of the data). The whiskers give an idea of the spread of the data and dots outside of the whiskers show candidate outlier values (values that are 1.5 times greater than the size of spread of the middle 50% of the data).\n\ndata.plot(kind='box', subplots=True, layout=(3,3), sharex=False, sharey=False)\npyplot.show()\n\nWe can see that the spread of attributes is quite different. Some like age, test and skin appear quite skewed towards smaller values.\n\n\n\nMultivariate Plots\nThis section provides examples of two plots that show the interactions between multiple variables in your dataset.  Correlation Matrix Plot.  Scatter Plot Matrix.\n\nCorrelation Matrix Plot\nCorrelation gives an indication of how related the changes are between two variables. If two variables change in the same direction they are positively correlated. If they change in opposite directions together (one goes up, one goes down), then they are negatively correlated. You can calculate the correlation between each pair of attributes. This is called a correlation matrix. You can then plot the correlation matrix and get an idea of which variables have a high correlation with each other. This is useful to know, because some machine learning algorithms like linear and logistic regression can have poor performance if there are highly correlated input variables in your data.\n\n# plot correlation matrix\n\nfig = pyplot.figure()\nax = fig.add_subplot(111)\ncax = ax.matshow(data.corr(), vmin=-1, vmax=1)\nfig.colorbar(cax)\nticks = numpy.arange(0,9,1)\nax.set_xticks(ticks)\nax.set_yticks(ticks)\nax.set_xticklabels(names)\nax.set_yticklabels(names)\npyplot.show()\n\nWe can see that the matrix is symmetrical, i.e.the bottom left of the matrix is the same as the top right. This is useful as we can see two different views on the same data in one plot. We can also see that each variable is perfectly positively correlated with itself (as you would have expected) in the diagonal line from top left to bottom right.\nThe example is not generic in that it specifies the names for the attributes along the axes as well as the number of ticks. This recipe can be made more generic by removing these aspects as follows:\n\n# plot correlation matrix\nfig = pyplot.figure()\nax = fig.add_subplot(111)\ncax = ax.matshow(data.corr(), vmin=-1, vmax=1)\nfig.colorbar(cax)\npyplot.show()\n\nGenerating the plot, you can see that it gives the same information although making it a little harder to see what attributes are correlated by name. Use this generic plot as a first cut to understand the correlations in your dataset and customize it like the first example in order to read off more specific data if needed.\n\n\nScatter Plot Matrix\nA scatter plot shows the relationship between two variables as dots in two dimensions, one axis for each attribute. You can create a scatter plot for each pair of attributes in your data. Drawing all these scatter plots together is called a scatter plot matrix. Scatter plots are useful for spotting structured relationships between variables, like whether you could summarize the relationship between two variables with a line. Attributes with structured relationships may also be correlated and good candidates for removal from your dataset.\n\nfrom pandas.plotting import scatter_matrix\n\n\nscatter_matrix(data)\npyplot.show()\n\nLike the Correlation Matrix Plot above, the scatter plot matrix is symmetrical. This is useful to look at the pairwise relationships from different perspectives. Because there is little point of drawing a scatter plot of each variable with itself, the diagonal shows histograms of each attribute.\nIn this chapter you discovered a number of ways that you can better understand your machine learning data in Python using Pandas. Specifically, you learned how to plot your data using:  Histograms.  Density Plots.  Box and Whisker Plots.  Correlation Matrix Plot.  Scatter Plot Matrix."
  },
  {
    "objectID": "02-end-to-end/steps.html#prepare-data",
    "href": "02-end-to-end/steps.html#prepare-data",
    "title": "Machine Learning Steps",
    "section": "Prepare Data",
    "text": "Prepare Data\nMany machine learning algorithms make assumptions about your data. It is often a very good idea to prepare your data in such a way to best expose the structure of the problem to the machine learning algorithms that you intend to use. In this chapter you will discover how to prepare your data for machine learning in Python using scikit-learn. After completing this lesson you will know how to: 1. Rescale data. 2. Standardize data. 3. Normalize data. 4. Binarize data.\nNeed For Data Pre-processing\nYou almost always need to pre-process your data. It is a required step. A difficulty is that different algorithms make different assumptions about your data and may require different transforms. Further, when you follow all of the rules and prepare your data, sometimes algorithms can deliver better results without pre-processing.\nGenerally, I would recommend creating many different views and transforms of your data, then exercise a handful of algorithms on each view of your dataset. This will help you to flush out which data transforms might be better at exposing the structure of your problem in general.\nData Transforms\nIn this lesson you will work through 4 different data pre-processing recipes for machine learning. The Pima Indian diabetes dataset is used in each recipe. Each recipe follows the same structure:\n Load the dataset.  Split the dataset into the input and output variables for machine learning.  Apply a pre-processing transform to the input variables.  Summarize the data to show the change.\nThe scikit-learn library provides two standard idioms for transforming data. Each are useful in different circumstances. The transforms are calculated in such a way that they can be applied to your training data and any samples of data you may have in the future. The scikit-learn documentation has some information on how to use various different pre-processing methods:\n Fit and Multiple Transform.  Combined Fit-And-Transform.\nThe Fit and Multiple Transform method is the preferred approach. You call the fit() function to prepare the parameters of the transform once on your data. Then later you can use the transform() function on the same data to prepare it for modeling and again on the test or validation dataset or new data that you may see in the future. The Combined Fit-And-Transform is a convenience that you can use for one off tasks. This might be useful if you are interested in plotting or summarizing the transformed data. You can review the preprocess API in scikit-learn here.\n\nRescale Data\nWhen your data is comprised of attributes with varying scales, many machine learning algorithms can benefit from rescaling the attributes to all have the same scale. Often this is referred to as normalization and attributes are often rescaled into the range between 0 and 1. This is useful for optimization algorithms used in the core of machine learning algorithms like gradient descent. It is also useful for algorithms that weight inputs like regression and neural networks and algorithms that use distance measures like k-Nearest Neighbors. You can rescale your data using scikit-learn using the MinMaxScaler class.\n\nfrom pandas import read_csv\nfrom numpy import set_printoptions\nfrom sklearn.preprocessing import MinMaxScaler\n\n\nnames = ['preg','plas','pres','skin','test','mass','pedi','age','class']\n\n\ndataframe = read_csv(filepath+filename, names=names)\ndataframe\n\n\narray = dataframe.values\narray\n\n\n# separate array into input and output components\n\nX = array[:, 0:8]\nY = array[:, 8]\nscaler = MinMaxScaler(feature_range=(0,1))\nrescaledX = scaler.fit_transform(X)\n\n\nset_printoptions(precision=3)\n\n\nrescaledX[0:5, :]\n\n\n\nStandardize Data\nStandardization is a useful technique to transform attributes with a Gaussian distribution and differing means and standard deviations to a standard Gaussian distribution with a mean of 0 and a standard deviation of 1. It is most suitable for techniques that assume a Gaussian distribution in the input variables and work better with rescaled data, such as linear regression, logistic regression and linear discriminate analysis. You can standardize data using scikit-learn with the StandardScaler class\n\nfrom sklearn.preprocessing import StandardScaler\nfrom pandas import read_csv\nfrom numpy import set_printoptions\n\n\nnames = ['preg','plas','pres','skin','test','mass','pedi','age','class']\n\n\ndataframe = read_csv(filepath+filename, names=names)\n\n\narray = dataframe.values\n\n\n# separate array into input and output components\nX = array[:, 0:8]\nY = array[:, 8]\nscaler = StandardScaler().fit(X)\nrescaledX = scaler.transform(X)\n\n\nset_printoptions(precision=3)\n\n\nrescaledX[0:5, :]\n\nThe values for each attribute now have a mean value of 0 and a standard deviation of 1.\n\n\nNormalize Data\nNormalizing in scikit-learn refers to rescaling each observation (row) to have a length of 1 (called a unit norm or a vector with the length of 1 in linear algebra). This pre-processing method can be useful for sparse datasets (lots of zeros) with attributes of varying scales when using algorithms that weight input values such as neural networks and algorithms that use distance measures such as k-Nearest Neighbors. You can normalize data in Python with scikit-learn using the Normalizer class\n\nfrom sklearn.preprocessing import Normalizer\nfrom pandas import read_csv\nfrom numpy import set_printoptions\n\n\nnames = ['preg','plas','pres','skin','test','mass','pedi','age','class']\n\n\ndataframe = read_csv(filepath+filename, names=names)\narray = dataframe.values\n\n\nX = array[:, 0:8]\nY = array[:, 8]\nscaler = Normalizer().fit(X)\nnormalizedX = scaler.transform(X)\n\n\nset_printoptions(precision=3)\n\n\nnormalizedX[0:5, :]\n\n\n\nBinarize Data (Make Binary)\nYou can transform your data using a binary threshold. All values above the threshold are marked 1 and all equal to or below are marked as 0. This is called binarizing your data or thresholding your data. It can be useful when you have probabilities that you want to make into crisp values. It is also useful when feature engineering and you want to add new features that indicate something meaningful. You can create new binary attributes in Python using scikit-learn with the Binarizer class\n\nfrom sklearn.preprocessing import Binarizer\nfrom pandas import read_csv\nfrom numpy import set_printoptions\n\n\nnames = ['preg','plas','pres','skin','test','mass','pedi','age','class']\n\n\ndataframe = read_csv(filepath+filename, names=names)\narray = dataframe.values\n\n\nX = array[:, 0:8]\nY = array[:, 8]\nbinarizer = Binarizer(threshold=0).fit(X)\nbinaryX = binarizer.transform(X)\n\n\nset_printoptions(precision=3)\nbinaryX[0:5, :]\n\nIn this chapter you discovered how you can prepare your data for machine learning in Python using scikit-learn. You now have recipes to:\n Rescale data.  Standardize data.  Normalize data.  Binarize data.\nYou now know how to transform your data to best expose the structure of your problem to the modeling algorithms. In the next lesson you will discover how to select the features of your data that are most relevant to making predictions."
  },
  {
    "objectID": "02-end-to-end/steps.html#evaluate-algorithms",
    "href": "02-end-to-end/steps.html#evaluate-algorithms",
    "title": "Machine Learning Steps",
    "section": "Evaluate Algorithms",
    "text": "Evaluate Algorithms\n\nEvaluate the Performance of Machine Learning Algorithms with Resampling\nYou need to know how well your algorithms perform on unseen data. The best way to evaluate the performance of an algorithm would be to make predictions for new data to which you already know the answers. The second best way is to use clever techniques from statistics called resampling methods that allow you to make accurate estimates for how well your algorithm will perform on new data. In this chapter you will discover how you can estimate the accuracy of your machine learning algorithms using resampling methods in Python and scikit-learn on the Pima Indians dataset.\nEvaluate Machine Learning Algorithms\nWhy cant you prepare your machine learning algorithm on your training dataset and use predictions from this same dataset to evaluate performance? The simple answer is overfitting. Imagine an algorithm that remembers every observation it is shown during training. If you evaluated your machine learning algorithm on the same dataset used to train the algorithm, then an algorithm like this would have a perfect score on the training dataset. But the predictions it made on new data would be terrible. We must evaluate our machine learning algorithms on data that is not used to train the algorithm.\nA model evaluation is an estimate that we can use to talk about how well we think the method may actually do in practice. It is not a guarantee of performance. Once we estimate the performance of our algorithm, we can then re-train the final algorithm on the entire training dataset and get it ready for operational use. Next up we are going to look at four different techniques that we can use to split up our training dataset and create useful estimates of performance for our machine learning algorithms:\n\nTrain and Test Sets.\nk-fold Cross-Validation.\nLeave One Out Cross-Validation.\nRepeated Random Test-Train Splits.\n\n\nSplit into Train and Test Sets\nThe simplest method that we can use to evaluate the performance of a machine learning algorithm is to use different training and testing datasets. We can take our original dataset and split it into two parts. Train the algorithm on the first part, make predictions on the second part and evaluate the predictions against the expected results. The size of the split can depend on the size and specifics of your dataset, although it is common to use 67% of the data for training and the remaining 33% for testing.\nThis algorithm evaluation technique is very fast. It is ideal for large datasets (millions of records) where there is strong evidence that both splits of the data are representative of the underlying problem. Because of the speed, it is useful to use this approach when the algorithm you are investigating is slow to train. A downside of this technique is that it can have a high variance. This means that differences in the training and test dataset can result in meaningful differences in the estimate of accuracy. In the example below we split the Pima Indians dataset into 67%/33% splits for training and test and evaluate the accuracy of a Logistic Regression model.\n\nfrom pandas import read_csv\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\n\n\nnames = ['preg','plas','pres','skin','test','mass','pedi','age','class']\n\n\ndataframe = read_csv(filepath+filename, names=names)\narray = dataframe.values\nX = array[:, 0:8]\nY = array[:, 8]\n\n\ntest_size = 0.33\nseed = 7\n\n\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size, random_state=seed)\n\n\nmodel = LogisticRegression(solver='liblinear')\nmodel.fit(X_train, Y_train)\n\n\nresult = model.score(X_test, Y_test)\nprint(f'Accuracy: {result*100: 0.3f}%')\n\nWe can see that the estimated accuracy for the model was approximately 75%. Note that in addition to specifying the size of the split, we also specify the random seed. Because the split of the data is random, we want to ensure that the results are reproducible. By specifying the random seed we ensure that we get the same random numbers each time we run the code and in turn the same split of data. This is important if we want to compare this result to the estimated accuracy of another machine learning algorithm or the same algorithm with a different configuration. To ensure the comparison was apples-for-apples, we must ensure that they are trained and tested on exactly the same data.\n\n\nK-fold Cross-Validation\nCross-validation is an approach that you can use to estimate the performance of a machine learning algorithm with less variance than a single train-test set split. It works by splitting the dataset into k-parts (e.g.k = 5 or k = 10). Each split of the data is called a fold. The algorithm is trained on k  1 folds with one held back and tested on the held back fold. This is repeated so that each fold of the dataset is given a chance to be the held back test set. After running cross-validation you end up with k different performance scores that you can summarize using a mean and a standard deviation.\nThe result is a more reliable estimate of the performance of the algorithm on new data. It is more accurate because the algorithm is trained and evaluated multiple times on different data. The choice of k must allow the size of each test partition to be large enough to be a reasonable sample of the problem, whilst allowing enough repetitions of the train-test evaluation of the algorithm to provide a fair estimate of the algorithms performance on unseen data. For modest sized datasets in the thousands or tens of thousands of records, k values of 3, 5 and 10 are common. In the example below we use 10-fold cross-validation\n\nfrom pandas import read_csv\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\n\n\nnames = ['preg','plas','pres','skin','test','mass','pedi','age','class']\n\n\ndataframe = read_csv(filepath+filename, names=names)\narray = dataframe.values\nX = array[:, 0:8]\nY = array[:, 8]\n\n\nkfold = KFold(n_splits=10, random_state=7, shuffle=True)\n\n\nmodel = LogisticRegression(solver='liblinear')\n\n\nresults = cross_val_score(model, X, Y, cv=kfold)\n\n\nprint(f'Accuracy: {results.mean()*100 : .3f} ({results.std()*100 : .3f})')\n\nYou can see that we report both the mean and the standard deviation of the performance measure. When summarizing performance measures, it is a good practice to summarize the distribution of the measures, in this case assuming a Gaussian distribution of performance (a very reasonable assumption) and recording the mean and standard deviation.\n\n\nLeave One Out Cross-Validation\nYou can configure cross-validation so that the size of the fold is 1 (k is set to the number of observations in your dataset). This variation of cross-validation is called leave-one-out cross- validation. The result is a large number of performance measures that can be summarized in an effort to give a more reasonable estimate of the accuracy of your model on unseen data. A downside is that it can be a computationally more expensive procedure than k-fold cross-validation.\n\nfrom pandas import read_csv\nfrom sklearn.model_selection import LeaveOneOut\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\n\n\nnames = ['preg','plas','pres','skin','test','mass','pedi','age','class']\n\n\ndataframe = read_csv(filepath+filename, names=names)\n\n\narray = dataframe.values\nX = array[:, 0:8]\nY = array[:, 8]\n\n\nloov = LeaveOneOut()\nmodel = LogisticRegression(solver='liblinear')\nresults = cross_val_score(model,X, Y, cv=loov)\n\n\nprint(f'Accuracy: {results.mean()*100: 0.3f}% ({results.std()*100: 0.3f})')\n\nYou can see in the standard deviation that the score has more variance than the k-fold cross-validation results described above\n\n\nRepeated Random Test-Train Splits\nAnother variation on k-fold cross-validation is to create a random split of the data like the train/test split described above, but repeat the process of splitting and evaluation of the algorithm multiple times, like cross-validation. This has the speed of using a train/test split and the reduction in variance in the estimated performance of k-fold cross-validation. You can also repeat the process many more times as needed to improve the accuracy. A down side is that repetitions may include much of the same data in the train or the test split from run to run, introducing redundancy into the evaluation. The example below splits the data into a 67%/33% train/test split and repeats the process 10 times.\n\nfrom pandas import read_csv\nfrom sklearn.model_selection import ShuffleSplit\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\n\n\nnames = ['preg','plas','pres','skin','test','mass','pedi','age','class']\n\n\ndataframe = read_csv(filepath+filename, names=names)\narray = dataframe.values\nX = array[:, 0:8]\nY = array[:, 8]\n\n\nn_splits = 10\ntest_size = 0.33\nseed = 7\n\n\nkfold = ShuffleSplit(n_splits=n_splits, test_size=test_size, random_state=seed)\n\n\nmodel = LogisticRegression(solver='liblinear')\nresults = cross_val_score(model, X, Y, cv=kfold)\n\n\nprint(f'Accuracy: {results.mean()*100 : 0.3f}% ({results.std()*100 : 0.3f})')\n\nWhat Techniques to Use When\nThis section lists some tips to consider what resampling technique to use in different circumstances.\n\nGenerally k-fold cross-validation is the gold standard for evaluating the performance of a machine learning algorithm on unseen data with k set to 3, 5, or 10.\nUsing a train/test split is good for speed when using a slow algorithm and produces performance estimates with lower bias when using large datasets.\nTechniques like leave-one-out cross-validation and repeated random splits can be useful intermediates when trying to balance variance in the estimated performance, model training speed and dataset size.\n\nThe best advice is to experiment and find a technique for your problem that is fast and produces reasonable estimates of performance that you can use to make decisions. If in doubt, use 10-fold cross-validation.\n\n\n\nMachine Learning Algorithm Performance Metrics\nThe metrics that you choose to evaluate your machine learning algorithms are very important. Choice of metrics influences how the performance of machine learning algorithms is measured and compared. They influence how you weight the importance of different characteristics in the results and your ultimate choice of which algorithm to choose. In this chapter you will discover how to select and use different machine learning performance metrics in Python with scikit-learn.\n\nClassification Metrics\nClassification problems are perhaps the most common type of machine learning problem and as such there is a myriad of metrics that can be used to evaluate predictions for these problems. In this section we will review how to use the following metrics:\n\nClassification Accuracy.\nLogistic Loss.\nArea Under ROC Curve.\nConfusion Matrix.\nClassification Report.\n\n\nfrom pandas import read_csv\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\n\nClassification Accuracy\nClassification accuracy is the number of correct predictions made as a ratio of all predictions made. This is the most common evaluation metric for classification problems, it is also the most misused. It is really only suitable when there are an equal number of observations in each class (which is rarely the case) and that all predictions and prediction errors are equally important, which is often not the case. Below is an example of calculating classification accuracy.\n\nfrom pandas import read_csv\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\n\n\nnames = ['preg','plas','pres','skin','test','mass','pedi','age','class']\n\n\ndataframe = read_csv(filepath+filename, names=names )\ndataframe\n\n\narray = dataframe.values\narray\n\n\nX = array[:, 0:8]\nY = array[:, 8]\n\n\nkfold = KFold(n_splits=10, shuffle=True, random_state=7)\n\n\nmodel = LogisticRegression(solver='liblinear')\n\n\nresults = cross_val_score(model, X, Y, cv=kfold, scoring='accuracy')\n\n\nprint(f'Accuracy is: {results.mean()*100: .3f} ({results.std(): .3f})')\n\nLogistic Loss\nLogistic loss (or logloss) is a performance metric for evaluating the predictions of probabilities of membership to a given class. The scalar probability between 0 and 1 can be seen as a measure of confidence for a prediction by an algorithm. Predictions that are correct or incorrect are rewarded or punished proportionally to the confidence of the prediction.\n\nfrom pandas import read_csv\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.linear_model import LogisticRegression\n\n\nnames = ['preg','plas','pres','skin','test','mass','pedi','age','class']\n\n\ndataframe = read_csv(filepath+filename, names=names)\narray = dataframe.values\nX = array[:,0:8]\nY = array[:, 8]\n\n\ncv = KFold(n_splits=10, random_state=7, shuffle=True)\n\n\nmodel = LogisticRegression(solver='liblinear')\n\n\nresults = cross_val_score(model, X, Y, cv=cv, scoring='neg_log_loss')\n\n\nprint(f'Accuracy is: {results.mean()*100: .3f} ({results.std(): .3f})')\n\nSmaller logloss is better with 0 representing a perfect logloss. As mentioned above, the measure is inverted to be ascending when using the cross val score() function.\nArea Under ROC Curve\nArea under ROC Curve (or ROC AUC for short) is a performance metric for binary classification problems. The AUC represents a models ability to discriminate between positive and negative classes. An area of 1.0 represents a model that made all predictions perfectly. An area of 0.5 represents a model that is as good as random. A ROC Curve is a plot of the true positive rate and the false positive rate for a given set of probability predictions at different thresholds used to map the probabilities to class labels. The area under the curve is then the approximate integral under the ROC Curve.\n\nfrom pandas import read_csv\nfrom sklearn.model_selection import KFold,cross_val_score\nfrom sklearn.linear_model import LogisticRegression\n\n\nnames = ['preg','plas','pres','skin','test','mass','pedi','age','class']\n\n\ndataframe = read_csv(filepath+filename, names=names)\narray = dataframe.values\nX = array[:, 0:8]\nY = array[:, 8]\n\n\ncv = KFold(n_splits=10, random_state=7, shuffle=True)\nmodel = LogisticRegression(solver='liblinear')\n\n\nresults = cross_val_score(model, X, Y, cv=cv, scoring='roc_auc')\nprint(f'AUC: {results.mean(): .3f} ({results.std(): 0.3f})')\n\nYou can see the AUC is relatively close to 1 and greater than 0.5, suggesting some skill in the predictions.\nConfusion Matrix\nThe confusion matrix is a handy presentation of the accuracy of a model with two or more classes. The table presents predictions on the x-axis and true outcomes on the y-axis. The cells of the table are the number of predictions made by a machine learning algorithm. For example, a machine learning algorithm can predict 0 or 1 and each prediction may actually have been a 0 or 1. Predictions for 0 that were actually 0 appear in the cell for prediction = 0 and actual = 0, whereas predictions for 0 that were actually 1 appear in the cell for prediction = 0 and actual = 1. And so on.\n\nfrom pandas import read_csv\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\n\n\nnames = ['preg','plas','pres','skin','test','mass','pedi','age','class']\n\n\ndataframe = read_csv(filepath+filename, names=names)\narray = dataframe.values\nX = array[:, 0:8]\nY = array[:,8]\n\n\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.33, random_state=7)\n\n\nmodel = LogisticRegression(solver='liblinear')\n\n\nmodel.fit(X_train, Y_train)\n\n\npredicted_Y = model.predict(X_test)\n\n\nmatrix = confusion_matrix(Y_test, predicted_Y)\nprint(matrix)\n\nAlthough the array is printed without headings, you can see that the majority of the predictions fall on the diagonal line of the matrix (which are correct predictions).\nClassification Report\nThe scikit-learn library provides a convenience report when working on classification problems to give you a quick idea of the accuracy of a model using a number of measures. The classification report() function displays the precision, recall, F1-score and support for each class.\n\nfrom pandas import read_csv\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\n\n\nnames = ['preg','plas','pres','skin','test','mass','pedi','age','class']\n\n\ndataframe = read_csv(filepath+filename, names=names)\narray = dataframe.values\nX = array[:,0:8]\nY = array[:,8]\n\n\nmodel = LogisticRegression(solver='liblinear')\n\n\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.33, random_state=7)\n\n\nmodel.fit(X_train, Y_train)\n\n\npredicted_Y = model.predict(X_test)\n\n\nreport = classification_report(Y_test, predicted_Y)\nprint(report)\n\n\n\nRegression Metrics\nIn this section will review 3 of the most common metrics for evaluating predictions on regression machine learning problems:\n\nMean Absolute Error.\nMean Squared Error.\nR 2 .\n\nMean Absolute Error\nThe Mean Absolute Error (or MAE) is the average of the absolute differences between predictions and actual values. It gives an idea of how wrong the predictions were. The measure gives an idea of the magnitude of the error, but no idea of the direction (e.g.over or under predicting).\n\nfrom pandas import read_csv\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import KFold, cross_val_score\n\n\nnames = ['CRIM','ZN','INDUS','CHAS','NOX','RM','AGE','DIS','RAD','TAX','PTRATIO',\n'B','LSTAT','MEDV']\n\n\ndataframe = read_csv(filepath+housing, delim_whitespace=True, names=names)\ndataframe\n\n\narray = dataframe.values\nX = array[:, 0:13]\nY = array[:, 13]\n\n\ncv = KFold(n_splits=10, random_state=7, shuffle=True)\n\n\nmodel = LinearRegression()\n\n\nresults = cross_val_score(model, X, Y, cv=cv, scoring='neg_mean_absolute_error')\n\n\nprint(f'MAE: {results.mean(): 0.3f} ({results.std(): .3f})')\n\n\n\nMean Squared Error\n\nfrom pandas import read_csv\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.linear_model import LinearRegression\n\n\nnames = ['CRIM','ZN','INDUS','CHAS','NOX','RM','AGE','DIS','RAD','TAX','PTRATIO',\n'B','LSTAT','MEDV']\n\n\ndataframe = read_csv(filepath+housing, delim_whitespace=True,names=names)\narray = dataframe.values\nX = array[:, 0:13]\nY = array[:, 13]\n\n\nmodel = LinearRegression()\n\n\ncv = KFold(n_splits=10, random_state=7, shuffle=True)\n\n\nresults = cross_val_score(model, X, Y, cv=cv, scoring='neg_mean_squared_error')\n\n\nprint(f'MSE: {results.mean(): 0.3f} ({results.std(): 0.3f})')\n\nThis metric too is inverted so that the results are increasing. Remember to take the absolute value before taking the square root if you are interested in calculating the RMSE.\n\n\nR2 Metric\nThe R 2 (or R Squared) metric provides an indication of the goodness of fit of a set of predictions to the actual values. In statistical literature this measure is called the coefficient of determination. This is a value between 0 and 1 for no-fit and perfect fit respectively.\n\nfrom pandas import read_csv\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.linear_model import LinearRegression\n\n\nnames = ['CRIM','ZN','INDUS','CHAS','NOX','RM','AGE','DIS','RAD','TAX','PTRATIO',\n'B','LSTAT','MEDV']\n\n\ndataframe = read_csv(filepath+housing, delim_whitespace=True, names=names)\narray = dataframe.values\nX = array[:, 0:13]\nY = array[:, 13]\n\n\ncv = KFold(n_splits=10, shuffle=True, random_state=7)\n\n\nmodel = LinearRegression()\n\n\nresults = cross_val_score(model, X, Y, cv=cv, scoring='r2')\n\n\nprint(f'R Squared: {results.mean():0.3f} ({results.std(): .3f})')\n\n\n\n\nSpot-Check Classification Algorithms\nSpot-checking is a way of discovering which algorithms perform well on your machine learning problem. You cannot know which algorithms are best suited to your problem beforehand. You must trial a number of methods and focus attention on those that prove themselves the most promising. In this chapter you will discover six machine learning algorithms that you can use when spot-checking your classification problem in Python with scikit-learn. After completing this lesson you will know: 1. How to spot-check machine learning algorithms on a classification problem. 2. How to spot-check two linear classification algorithms. 3. How to spot-check four nonlinear classification algorithms.\nAlgorithm Spot-Checking\nYou cannot know which algorithm will work best on your dataset beforehand. You must use trial and error to discover a shortlist of algorithms that do well on your problem that you can then double down on and tune further. I call this process spot-checking. The question is not: What algorithm should I use on my dataset? Instead it is: What algorithms should I spot-check on my dataset? You can guess at what algorithms might do well on your dataset, and this can be a good starting point. I recommend trying a mixture of algorithms and see what is good at picking out the structure in your data. Below are some suggestions when spot-checking algorithms on your dataset:\n\nTry a mixture of algorithm representations (e.g.instances and trees).\nTry a mixture of learning algorithms (e.g.different algorithms for learning the same type of representation).\nTry a mixture of modeling types (e.g.linear and nonlinear functions or parametric and nonparametric).\n\nLets get specific. In the next section, we will look at algorithms that you can use to spot-check on your next classification machine learning project in Python.\nAlgorithms Overview\nWe are going to take a look at six classification algorithms that you can spot-check on your dataset. Starting with two linear machine learning algorithms:\n\nLogistic Regression.\nLinear Discriminant Analysis.\n\nThen looking at four nonlinear machine learning algorithms: * k-Nearest Neighbors. * Naive Bayes. * Classification and Regression Trees. * Support Vector Machines.\nEach recipe is demonstrated on the Pima Indians onset of Diabetes dataset. A test harness using 10-fold cross-validation is used to demonstrate how to spot-check each machine learning algorithm and mean accuracy measures are used to indicate algorithm performance. The recipes assume that you know about each machine learning algorithm and how to use them. We will not go into the API or parameterization of each algorithm.\n\nLinear Machine Learning Algorithms\nLogistic Regression\nLogistic regression assumes a Gaussian distribution for the numeric input variables and can model binary classification problems.\n\nfrom pandas import read_csv\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.linear_model import LogisticRegression\n\n\nnames = ['preg','plas','pres','skin','test','mass','pedi','age','class']\n\n\ndataframe = read_csv(filepath+pima, names=names)\narray = dataframe.values\nX = array[:, 0:8]\nY = array[:, 8]\n\n\nkfodl = KFold(n_splits=10, random_state=7, shuffle=True)\n\n\nmodel = LogisticRegression(solver='liblinear')\n\n\nresults = cross_val_score(model, X, Y, cv=kfodl)\nprint(f'{results.mean(): .3f}')\n\nLinear Discriminant Analysis\nLinear Discriminant Analysis or LDA is a statistical technique for binary and multiclass classification. It too assumes a Gaussian distribution for the numerical input variables.\n\nfrom pandas import read_csv\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\n\nnames = ['preg','plas','pres','skin','test','mass','pedi','age','class']\n\n\ndataframe = read_csv(filepath+pima, names=names)\narray = dataframe.values\nX = array[:, 0:8]\nY = array[:,8]\n\n\nkfold = KFold(n_splits=10, random_state=7, shuffle=True)\nmodel = LinearDiscriminantAnalysis()\n\n\nresults = cross_val_score(model , X ,Y, cv=kfold)\nprint(f'{results.mean()}')\n\n\n\nNonlinear Machine Learning Algorithms\nk-Nearest Neighbors\nThe k-Nearest Neighbors algorithm (or KNN) uses a distance metric to find the k most similar instances in the training data for a new instance and takes the mean outcome of the neighbors as the prediction. You can construct a KNN model using the KNeighborsClassifier class.\n\nfrom pandas import read_csv\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.neighbors import KNeighborsClassifier\n\n\nnames = ['preg','plas','pres','skin','test','mass','pedi','age','class']\n\n\ndataframe = read_csv(filepath+pima, names=names)\narray = dataframe.values\nX = array[:,0:8]\nY = array[:,8]\n\n\nkfold = KFold(n_splits=10, random_state=7, shuffle=True)\n\n\nmodel = KNeighborsClassifier()\n\n\nresults = cross_val_score(model, X, Y, cv=kfold)\nprint(f'{results.mean()}')\n\nNaive Bayes\nNaive Bayes calculates the probability of each class and the conditional probability of each class given each input value. These probabilities are estimated for new data and multiplied together, assuming that they are all independent (a simple or naive assumption). When working with real-valued data, a Gaussian distribution is assumed to easily estimate the probabilities for input variables using the Gaussian Probability Density Function.\n\nfrom pandas import read_csv\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.naive_bayes import GaussianNB\n\n\nnames = ['preg','plas','pres','skin','test','mass','pedi','age','class']\n\n\ndataframe = read_csv(filepath+pima, names=names)\narray = dataframe.values\nX = array[:, 0:8]\nY = array[:,8]\n\n\nkfod = KFold(n_splits=10, random_state=7, shuffle=True)\nmodel = GaussianNB()\n\n\nresults = cross_val_score(model, X, Y, cv=kfod)\nprint(f'{results.mean()}')\n\nClassification and Regression Trees\nClassification and Regression Trees (CART or just decision trees) construct a binary tree from the training data. Split points are chosen greedily by evaluating each attribute and each value of each attribute in the training data in order to minimize a cost function (like the Gini index).\n\nfrom pandas import read_csv\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.tree import DecisionTreeClassifier\n\n\nnames = ['preg','plas','pres','skin','test','mass','pedi','age','class']\n\n\ndataframe = read_csv(filepath+pima, names=names)\narray = dataframe.values\nX = array[:, 0:8]\nY = array[:, 8]\n\n\nkfold = KFold(n_splits=10, random_state=7, shuffle=True)\nmodel = DecisionTreeClassifier()\n\n\nresults = cross_val_score(model, X, Y, cv=kfod)\nprint(f'{results.mean(): .3f}')\n\nSupport Vector Machines\nSupport Vector Machines (or SVM) seek a line that best separates two classes. Those data instances that are closest to the line that best separates the classes are called support vectors and influence where the line is placed. SVM has been extended to support multiple classes. Of particular importance is the use of different kernel functions via the kernel parameter. A powerful Radial Basis Function is used by default.\n\nfrom pandas import read_csv\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.svm import SVC\n\n\nnames = ['preg','plas','pres','skin','test','mass','pedi','age','class']\n\n\ndataframe = read_csv(filepath+pima, names=names)\narray = dataframe.values\nX = array[:, 0:8]\nY = array[:, 8]\n\n\nkfold = KFold(n_splits=10, random_state=7, shuffle=True)\n\n\nmodel = SVC()\n\n\nresults = cross_val_score(model, X, Y, cv=kfod)\nprint(f'{results.mean()}')\n\n\n\n\nSpot-Check Regression Algorithms\nAlgorithms Overview\nIn this lesson we are going to take a look at seven regression algorithms that you can spot-check on your dataset.\nStarting with four linear machine learning algorithms: * Linear Regression. * Ridge Regression. * LASSO Linear Regression. * Elastic Net Regression.\nThen looking at three nonlinear machine learning algorithms: * k-Nearest Neighbors. * Classification and Regression Trees. * Support Vector Machines.\n\nLinear Machine Learning Algorithms\nLinear Regression\nLinear regression assumes that the input variables have a Gaussian distribution. It is also assumed that input variables are relevant to the output variable and that they are not highly correlated with each other (a problem called collinearity).\n\nfrom pandas import read_csv\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.linear_model import LinearRegression\n\n\nnames = ['CRIM','ZN','INDUS','CHAS','NOX','RM','AGE','DIS','RAD','TAX','PTRATIO','B','LSTAT','MEDV']\n\n\ndataframe = read_csv(filepath+housing, names=names, delim_whitespace=True)\narray = dataframe.values\nX = array[:, 0:13]\nY = array[:, 13]\n\n\nkfold = KFold(n_splits=10, random_state=7, shuffle=True)\n\n\nmodel = LinearRegression()\n\n\nresults = cross_val_score(model, X, Y, cv=kfold, scoring='neg_mean_squared_error')\nprint(f'{results.mean()}')\n\nRidge Regression\nRidge regression is an extension of linear regression where the loss function is modified to minimize the complexity of the model measured as the sum squared value of the coefficient values (also called the L2-norm).\n\nfrom pandas import read_csv\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.linear_model import Ridge\n\n\nnames = ['CRIM','ZN','INDUS','CHAS','NOX','RM','AGE','DIS','RAD','TAX','PTRATIO','B','LSTAT','MEDV']\n\n\ndataframe = read_csv(filepath+housing, names=names, delim_whitespace=True)\narray = dataframe.values\nX = array[:, 0:13]\nY = array[:,13]\n\n\nkfold = KFold(n_splits=10, random_state=7, shuffle=True)\n\n\nmodel = Ridge()\n\n\nresults = cross_val_score(model, X, Y, cv=kfold, scoring='neg_mean_squared_error')\nprint(f'{results.mean()}')\n\nLASSO Regression\nThe Least Absolute Shrinkage and Selection Operator (or LASSO for short) is a modification of linear regression, like ridge regression, where the loss function is modified to minimize the complexity of the model measured as the sum absolute value of the coefficient values (also called the L1-norm).\n\nfrom pandas import read_csv\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.linear_model import Lasso\n\n\nnames = ['CRIM','ZN','INDUS','CHAS','NOX','RM','AGE','DIS','RAD','TAX','PTRATIO','B','LSTAT','MEDV']\n\n\ndataframe = read_csv(filepath+housing, delim_whitespace=True, names=names)\narray = dataframe.values\nX = array[:, 0:13]\nY = array[:, 13]\n\n\nkfold = KFold(n_splits=10, random_state=7, shuffle=True)\n\n\nmodel = Lasso()\n\n\nresults = cross_val_score(model, X, Y, cv=kfold, scoring='neg_mean_squared_error')\nprint(f'{results.mean()}')\n\nElasticNet Regression\nElasticNet is a form of regularization regression that combines the properties of both Ridge Regression and LASSO regression. It seeks to minimize the complexity of the regression model (magnitude and number of regression coefficients) by penalizing the model using both the L2-norm (sum squared coefficient values) and the L1-norm (sum absolute coefficient values).\n\nfrom pandas import read_csv\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.linear_model import ElasticNet\n\n\nnames = ['CRIM','ZN','INDUS','CHAS','NOX','RM','AGE','DIS','RAD','TAX','PTRATIO','B','LSTAT','MEDV']\n\n\ndataframe = read_csv(filepath+housing, names=names, delim_whitespace=True)\narray = dataframe.values\nX = array[:, 0:13]\nY = array[:, 13]\n\n\nkfold = KFold(n_splits=10, random_state=7, shuffle=True)\n\n\nmodel = ElasticNet()\n\n\nresults = cross_val_score(model, X, Y, cv=kfold, scoring='neg_mean_squared_error')\nprint(f'{results.mean()}')\n\n\n\nNonlinear Machine Learning Algorithms\nK-Nearest Neighbors\nThe k-Nearest Neighbors algorithm (or KNN) locates the k most similar instances in the training dataset for a new data instance. From the k neighbors, the mean or median output variable is taken as the prediction. Of note is the distance metric used (the metric argument). The Minkowski distance is used by default, which is a generalization of both the Euclidean distance (used when all inputs have the same scale) and Manhattan distance (used when the scales of the input variables differ).\n\nfrom pandas import read_csv\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.neighbors import KNeighborsRegressor\n\n\nnames = ['CRIM','ZN','INDUS','CHAS','NOX','RM','AGE','DIS','RAD','TAX','PTRATIO','B','LSTAT','MEDV']\n\n\ndataframe = read_csv(filepath+housing, names=names, delim_whitespace=True)\narray = dataframe.values\nX = array[:, 0:13]\nY = array[: ,13]\n\n\nkfold = KFold(n_splits=10, random_state=7, shuffle=True)\n\n\nmodel = KNeighborsRegressor()\n\n\nresults = cross_val_score(model, X, Y, cv=kfold, scoring='neg_mean_squared_error')\nprint(f'{results.mean()}')\n\nClassification and Regression Trees\nDecision trees or the Classification and Regression Trees (CART as they are known) use the train- ing data to select the best points to split the data in order to minimize a cost metric. The default cost metric for regression decision trees is the mean squared error, specified in the criterion parameter.\n\nfrom pandas import read_csv\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.tree import DecisionTreeRegressor\n\n\nnames = ['CRIM','ZN','INDUS','CHAS','NOX','RM','AGE','DIS','RAD','TAX','PTRATIO','B','LSTAT','MEDV']\n\n\ndataframe = read_csv(filepath+housing, names=names, delim_whitespace=True)\narray = dataframe.values\nX = array[:, 0:13]\nY = array[:, 13]\n\n\nkfold = KFold(n_splits=10, shuffle=True, random_state=7)\n\n\nmodel = DecisionTreeRegressor()\n\n\nresults = cross_val_score(model, X, Y, cv=kfold, scoring='neg_mean_squared_error')\nprint(f'{results.mean()}')\n\nSupport Vector Machines\nSupport Vector Machines (SVM) were developed for binary classification. The technique has been extended for the prediction real-valued problems called Support Vector Regression (SVR). Like the classification example, SVR is built upon the LIBSVM library.\n\nfrom pandas import read_csv\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.svm import SVR\n\n\nnames = ['CRIM','ZN','INDUS','CHAS','NOX','RM','AGE','DIS','RAD','TAX','PTRATIO','B','LSTAT','MEDV']\n\n\ndataframe = read_csv(filepath+housing, names=names, delim_whitespace=True)\narray = dataframe.values\nX = array[:,0:13]\nY = array[:, 13]\n\n\nkfold = KFold(n_splits=10, random_state=7, shuffle=True)\n\n\nmodel = SVR(gamma='auto')\n\n\nresults = cross_val_score(model, X, Y, cv=kfold, scoring='neg_mean_squared_error')\nprint(f'{results.mean()}')\n\n\n\n\nCompare Machine Learning Algorithms\nIt is important to compare the performance of multiple different machine learning algorithms consistently. In this chapter you will discover how you can create a test harness to compare multiple different machine learning algorithms in Python with scikit-learn. You can use this test harness as a template on your own machine learning problems and add more and different algorithms to compare. After completing this lesson you will know:\n\nHow to formulate an experiment to directly compare machine learning algorithms.\nA reusable template for evaluating the performance of multiple algorithms on one dataset.\nHow to report and visualize the results when comparing algorithm performance.\n\nChoose The Best Machine Learning Model\nWhen you work on a machine learning project, you often end up with multiple good models to choose from. Each model will have different performance characteristics. Using resampling methods like cross-validation, you can get an estimate for how accurate each model may be on unseen data. You need to be able to use these estimates to choose one or two best models from the suite of models that you have created.\nWhen you have a new dataset, it is a good idea to visualize the data using different techniques in order to look at the data from different perspectives. The same idea applies to model selection. You should use a number of different ways of looking at the estimated accuracy of your machine learning algorithms in order to choose the one or two algorithms to finalize. A way to do this is to use visualization methods to show the average accuracy, variance and other properties of the distribution of model accuracies. In the next section you will discover exactly how you can do that in Python with scikit-learn.\nCompare Machine Learning Algorithms Consistently\nThe key to a fair comparison of machine learning algorithms is ensuring that each algorithm is evaluated in the same way on the same data. You can achieve this by forcing each algorithm to be evaluated on a consistent test harness. In the example below six different classification algorithms are compared on a single dataset: * Logistic Regression. * Linear Discriminant Analysis. * k-Nearest Neighbors. * Classification and Regression Trees. * Naive Bayes. * Support Vector Machines.\n\nfrom matplotlib import pyplot\nfrom pandas import read_csv\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\n\n\nnames = ['preg','plas','pres','skin','test','mass','pedi','age','class']\n\n\ndataframe = read_csv(filepath+pima, names=names)\narray = dataframe.values\nX = array[:, 0:8]\nY = array[:,8]\n\n\nmodels = []\n\n\nmodels.append(('LR', LogisticRegression(solver='liblinear')))\nmodels.append(('LDA', LinearDiscriminantAnalysis()))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('CART', DecisionTreeClassifier()))\nmodels.append(('NB', GaussianNB()))\nmodels.append(('SVM', SVC()))\n\n\nresults = []\nnames = []\n\n\nfor name, model in models:\n    kfold = KFold(n_splits=10, random_state=7, shuffle=True)\n    cv_results = cross_val_score(model, X, Y, cv=kfold, scoring='accuracy')\n    results.append(cv_results)\n    names.append(name)\n    print(f'{name}: {cv_results.mean():.3f} ({cv_results.std():.3f})')\n\n\nfig = pyplot.figure()\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(111)\npyplot.boxplot(results)\nax.set_xticklabels(names)\npyplot.show()\n\nThe example also provides a box and whisker plot showing the spread of the accuracy scores across each cross-validation fold for each algorithm.\nIn this lesson you learned how to compare the performance of machine learning algorithms to each other. But what if you need to prepare your data as part of the comparison process? In the next lesson you will discover Pipelines in scikit-learn and how they overcome the common problems of data leakage when comparing machine learning algorithms.\n\n\nFeature Selection For Machine Learning\nThe data features that you use to train your machine learning models have a huge influence on the performance you can achieve. Irrelevant or partially relevant features can negatively impact model performance. In this chapter you will discover automatic feature selection techniques that you can use to prepare your machine learning data in Python with scikit-learn.\nAfter completing this lesson you will know how to use:\n\nUnivariate Selection.\nRecursive Feature Elimination.\nPrinciple Component Analysis.\nFeature Importance.\n\nUnivariate Selection\nStatistical tests can be used to select those features that have the strongest relationship with the output variable. The scikit-learn library provides the SelectKBest class 2 that can be used with a suite of different statistical tests to select a specific number of features. Many different statistical tests can be used with this selection method. For example the ANOVA F-value method is appropriate for numerical inputs and categorical data, as we see in the Pima dataset.\n\nfrom sklearn.feature_selection import SelectKBest, f_classif\nfrom numpy import set_printoptions\n\n\nX, Y = load_pima_data()\n\n\nX[0:5, :]\n\narray([[6.000e+00, 1.480e+02, 7.200e+01, 3.500e+01, 0.000e+00, 3.360e+01,\n        6.270e-01, 5.000e+01],\n       [1.000e+00, 8.500e+01, 6.600e+01, 2.900e+01, 0.000e+00, 2.660e+01,\n        3.510e-01, 3.100e+01],\n       [8.000e+00, 1.830e+02, 6.400e+01, 0.000e+00, 0.000e+00, 2.330e+01,\n        6.720e-01, 3.200e+01],\n       [1.000e+00, 8.900e+01, 6.600e+01, 2.300e+01, 9.400e+01, 2.810e+01,\n        1.670e-01, 2.100e+01],\n       [0.000e+00, 1.370e+02, 4.000e+01, 3.500e+01, 1.680e+02, 4.310e+01,\n        2.288e+00, 3.300e+01]])\n\n\n\n# feature extraction\ntest = SelectKBest(score_func=f_classif, k=4)\nfit = test.fit(X,Y)\n\n\n# summarize scores\nset_printoptions(precision=3)\nfit.scores_\n\narray([ 39.67 , 213.162,   3.257,   4.304,  13.281,  71.772,  23.871,\n        46.141])\n\n\n\nfeatures = fit.transform(X)\n\n\nfeatures[0:5, :]\n\narray([[  6. , 148. ,  33.6,  50. ],\n       [  1. ,  85. ,  26.6,  31. ],\n       [  8. , 183. ,  23.3,  32. ],\n       [  1. ,  89. ,  28.1,  21. ],\n       [  0. , 137. ,  43.1,  33. ]])\n\n\nYou can see the scores for each attribute and the 4 attributes chosen (those with the highest scores). Specifically features with indexes 0 (preq), 1 (plas), 5 (mass), and 7 (age).\nRecursive Feature Elimination\nThe Recursive Feature Elimination (or RFE) works by recursively removing attributes and building a model on those attributes that remain. It uses the model accuracy to identify which attributes (and combination of attributes) contribute the most to predicting the target attribute.\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_selection import RFE\n\n\nX, Y = load_pima_data()\n\n\nmodel = LogisticRegression(solver='liblinear')\n\n\nrfe = RFE(model, n_features_to_select=3)\nfit = rfe.fit(X,Y)\n\n\nprint(f'Num Features: {fit.n_features_}')\n\nNum Features: 3\n\n\n\nprint(f'Selected Features: {fit.support_}')\n\nSelected Features: [ True False False False False  True  True False]\n\n\n\nprint(f'Feature Ranking: {fit.ranking_}')\n\nFeature Ranking: [1 2 3 5 6 1 1 4]\n\n\nPrincipal Component Analysis\nPrincipal Component Analysis (or PCA) uses linear algebra to transform the dataset into a compressed form. Generally this is called a data reduction technique. A property of PCA is that you can choose the number of dimensions or principal components in the transformed result.\n\nfrom sklearn.decomposition import PCA\n\n\nX, Y = load_pima_data()\n\n\npca = PCA(n_components=3)\n\n\nfit = pca.fit(X)\n\n\nprint(f'Explained Variance: {fit.explained_variance_ratio_}')\n\nExplained Variance: [0.889 0.062 0.026]\n\n\n\nprint(fit.components_)\n\n[[-2.022e-03  9.781e-02  1.609e-02  6.076e-02  9.931e-01  1.401e-02\n   5.372e-04 -3.565e-03]\n [-2.265e-02 -9.722e-01 -1.419e-01  5.786e-02  9.463e-02 -4.697e-02\n  -8.168e-04 -1.402e-01]\n [-2.246e-02  1.434e-01 -9.225e-01 -3.070e-01  2.098e-02 -1.324e-01\n  -6.400e-04 -1.255e-01]]\n\n\nFeature Importance\nBagged decision trees like Random Forest and Extra Trees can be used to estimate the importance of features.\n\nfrom sklearn.ensemble import ExtraTreesClassifier\n\n\nX,Y = load_pima_data()\n\n\nmodel = ExtraTreesClassifier(n_estimators=100)\n\n\nmodel.fit(X,Y)\n\nExtraTreesClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.ExtraTreesClassifierExtraTreesClassifier()\n\n\n\nprint(model.feature_importances_)\n\n[0.112 0.237 0.098 0.081 0.075 0.139 0.12  0.139]\n\n\n\n\nAutomate Machine Learning Workflows with Pipelines\nThere are standard workflows in a machine learning project that can be automated. In Python scikit-learn, Pipelines help to clearly define and automate these workflows. In this chapter you will discover Pipelines in scikit-learn and how you can automate common machine learningworkflows. After completing this lesson you will know: 1. How to use pipelines to minimize data leakage. 2. How to construct a data preparation and modeling pipeline. 3. How to construct a feature extraction and modeling pipeline.\nAutomating Machine Learning Workflows\nThere are standard workflows in applied machine learning. Standard because they overcome common problems like data leakage in your test harness. Python scikit-learn provides a Pipeline utility to help automate machine learning workflows. Pipelines work by allowing for a linear sequence of data transforms to be chained together culminating in a modeling process that can be evaluated.\nThe goal is to ensure that all of the steps in the pipeline are constrained to the data available for the evaluation, such as the training dataset or each fold of the cross-validation procedure. You can learn more about Pipelines in scikit-learn by reading the Pipeline section 1 of the user guide. You can also review the API documentation for the Pipeline and FeatureUnion classes and the pipeline module.\n\nData Preparation and Modeling Pipeline\nAn easy trap to fall into in applied machine learning is leaking data from your training dataset to your test dataset. To avoid this trap you need a robust test harness with strong separation of training and testing. This includes data preparation. Data preparation is one easy way to leak knowledge of the whole training dataset to the algorithm. For example, preparing your data using normalization or standardization on the entire training dataset before learning would not be a valid test because the training dataset would have been influenced by the scale of the data in the test set.\nPipelines help you prevent data leakage in your test harness by ensuring that data preparation like standardization is constrained to each fold of your cross-validation procedure. The example below demonstrates this important data preparation and model evaluation workflow on the Pima Indians onset of diabetes dataset. The pipeline is defined with two steps: 1. Standardize the data. 2. Learn a Linear Discriminant Analysis model.\n\n# Create a pipeline that standardizes the data then creates a model\n\n\nfrom pandas import read_csv\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.pipeline import Pipeline\n\n\ndataframe = read_csv(filepath+pima, names=names)\narray = dataframe.values\nX = array[:, 0:8]\nY = array[:, 8]\n\n\nkfold = KFold(n_splits=10, random_state=7, shuffle=True)\n\n\n# create pipeline\npipline_items = []\npipline_items.append(('standarize', StandardScaler()))\npipline_items.append(('lda', LinearDiscriminantAnalysis()))\npipeline = Pipeline(pipline_items)\n\n\nresults = cross_val_score(pipeline, X, Y, cv=kfold)\nprint(f'{results.mean()}')\n\nNotice how we create a Python list of steps that are provided to the Pipeline for processing the data. Also notice how the Pipeline itself is treated like an estimator and is evaluated in its entirety by the k-fold cross-validation procedure.\n\n\nFeature Extraction and Modeling Pipeline\nFeature extraction is another procedure that is susceptible to data leakage. Like data preparation, feature extraction procedures must be restricted to the data in your training dataset. The pipeline provides a handy tool called the FeatureUnion which allows the results of multiple feature selection and extraction procedures to be combined into a larger dataset on which a model can be trained. Importantly, all the feature extraction and the feature union occurs within each fold of the cross-validation procedure. The example below demonstrates the pipeline defined with four steps:\n\nFeature Extraction with Principal Component Analysis (3 features).\nFeature Extraction with Statistical Selection (6 features).\nFeature Union.\nLearn a Logistic Regression Model.\n\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_selection import SelectKBest\n\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.linear_model import LogisticRegression\n\n\n# load data\ndatafarem = read_csv(filepath+pima, names=pima_names)\narray = datafarem.values\nX = array[:,0:8]\nY = array[:, 8]\n\n\n# create feature union\nfeatures = []\nfeatures.append(('pca', PCA(n_components=3)))\nfeatures.append(('select_best', SelectKBest(k=6)))\nfeature_union = FeatureUnion(features)\n\n\n# create pipeline\npipeline_items = []\npipeline_items.append(('feature_union', feature_union)) \npipeline_items.append(('logistic', LogisticRegression(solver='liblinear')))\npipeline = Pipeline(pipeline_items)\n\n\n# evaluate pipeline\nkfold = KFold(n_splits=10, random_state=seed, shuffle=True)\nresults = cross_val_score(pipeline, X, Y, cv=kfold)\nprint(f'{results.mean()}')\n\nNotice how the FeatureUnion is its own Pipeline that in turn is a single step in the final Pipeline used to feed Logistic Regression. This might get you thinking about how you can start embedding pipelines within pipelines.\nIn this chapter you discovered the difficulties of data leakage in applied machine learning. You discovered the Pipeline utilities in Python scikit-learn and how they can be used to automate standard applied machine learning workflows. You learned how to use Pipelines in two important use cases:\n\nData preparation and modeling constrained to each fold of the cross-validation procedure.\nFeature extraction and feature union constrained to each fold of the cross-validation procedure."
  },
  {
    "objectID": "02-end-to-end/steps.html#improve-results",
    "href": "02-end-to-end/steps.html#improve-results",
    "title": "Machine Learning Steps",
    "section": "Improve Results",
    "text": "Improve Results\n\nImprove Performance with Ensembles\nEnsembles can give you a boost in accuracy on your dataset. In this chapter you will discover how you can create some of the most powerful types of ensembles in Python using scikit-learn. This lesson will step you through Boosting, Bagging and Majority Voting and show you how you can continue to ratchet up the accuracy of the models on your own datasets. After completing this lesson you will know:\n\nHow to use bagging ensemble methods such as bagged decision trees, random forest and extra trees.\nHow to use boosting ensemble methods such as AdaBoost and stochastic gradient boosting.\nHow to use voting ensemble methods to combine the predictions from multiple algorithms.\n\nThe three most popular methods for combining the predictions from different models are:\n\nBagging. Building multiple models (typically of the same type) from different subsamples of the training dataset.\nBoosting. Building multiple models (typically of the same type) each of which learns to fix the prediction errors of a prior model in the sequence of models.\nVoting. Building multiple models (typically of differing types) and simple statistics (like calculating the mean) are used to combine predictions. This assumes you are generally familiar with machine learning algorithms and ensemble methods and will not go into the details of how the algorithms work or their parameters.\n\n\nBagging Algorithms\nBootstrap Aggregation (or Bagging) involves taking multiple samples from your training dataset (with replacement) and training a model for each sample. The final output prediction is averaged across the predictions of all of the sub-models. The three bagging models covered in this section are as follows: * Bagged Decision Trees. * Random Forest. * Extra Trees.\nBagged Decision Trees\nBagging performs best with algorithms that have high variance. A popular example are decision trees, often constructed without pruning. In the example below is an example of using the BaggingClassifier with the Classification and Regression Trees algorithm (DecisionTreeClassifier).\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import BaggingClassifier\n\n\ndataframe = read_csv(filepath+pima, names=pima_names)\narray = datafarem.values\nX = array[:, 0:8]\nY = array[:, 8]\n\n\nkfold = KFold(n_splits=10, random_state=seed, shuffle=True)\n\n\ncart = DecisionTreeClassifier()\n\n\nmodel = BaggingClassifier(base_estimator=cart, n_estimators=100, random_state=seed)\n\n\nresults = cross_val_score(model, X, Y, cv=kfold)\nprint(f'{results.mean()}')\n\nRandom Forest\nRandom Forests is an extension of bagged decision trees. Samples of the training dataset are taken with replacement, but the trees are constructed in a way that reduces the correlation between individual classifiers. Specifically, rather than greedily choosing the best split point in the construction of each tree, only a random subset of features are considered for each split.\n\nfrom sklearn.ensemble import RandomForestClassifier\n\n\nX, Y = load_pima_data()\n\n\nmodel = RandomForestClassifier(n_estimators=100, max_features=3)\n\n\nresults = cross_val_score(model, X, Y, \n                          cv=KFold(n_splits=10, random_state=7, shuffle=True))\n\n\nprint(f'{results.mean()}')\n\n0.7577922077922078\n\n\nExtra Trees\nExtra Trees are another modification of bagging where random trees are constructed from samples of the training dataset. You can construct an Extra Trees model for classification using the ExtraTreesClassifier class\n\nfrom sklearn.ensemble import ExtraTreesClassifier\n\n\nX, Y = load_pima_data()\n\n\nmodel = ExtraTreesClassifier(n_estimators=100, max_features=7)\n\n\nresults = cross_val_score(model, X, Y, \n                         cv= KFold(n_splits=10, random_state=7, shuffle=True))\n\n\nprint(f'{results.mean()}')\n\n0.7656527682843473\n\n\n\n\nBoosting Algorithms\nBoosting ensemble algorithms create a sequence of models that attempt to correct the mistakes of the models before them in the sequence. Once created, the models make predictions which may be weighted by their demonstrated accuracy and the results are combined to create a final output prediction. The two most common boosting ensemble machine learning algorithms are: * AdaBoost. * Stochastic Gradient Boosting.\nAdaBoost\nAdaBoost was perhaps the first successful boosting ensemble algorithm. It generally works by weighting instances in the dataset by how easy or difficult they are to classify, allowing the algorithm to pay less or more attention to them in the construction of subsequent models\n\nfrom sklearn.ensemble import AdaBoostClassifier\n\n\nX, Y = load_pima_data()\n\n\nmodel = AdaBoostClassifier(n_estimators=30, random_state=seed)\n\n\nresults = cross_val_score(model, X, Y, \n                         cv=KFold(n_splits=10, random_state=seed, shuffle=True))\n\n\nprint(f'{results.mean()}')\n\n0.7552802460697198\n\n\nStochastic Gradient Boosting\nStochastic Gradient Boosting (also called Gradient Boosting Machines) are one of the most sophisticated ensemble techniques. It is also a technique that is proving to be perhaps one of the best techniques available for improving performance via ensembles.\n\nfrom sklearn.ensemble import GradientBoostingClassifier\n\n\nX, Y = load_pima_data()\n\n\nmodel = GradientBoostingClassifier(n_estimators=100, random_state=seed)\n\n\nresults = cross_val_score(model, X, Y, \n                         cv=KFold(n_splits=10, random_state=seed, shuffle=True))\n\n\nprint(f'{results.mean()}')\n\n0.7604921394395079\n\n\n\n\nVoting Ensemble\nVoting is one of the simplest ways of combining the predictions from multiple machine learning algorithms. It works by first creating two or more standalone models from your training dataset. A Voting Classifier can then be used to wrap your models and average the predictions of the sub-models when asked to make predictions for new data. The predictions of the sub-models can be weighted, but specifying the weights for classifiers manually or even heuristically is difficult. More advanced methods can learn how to best weight the predictions from sub-models, but this is called stacking (stacked aggregation) and is currently not provided in scikit-learn.\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import VotingClassifier\n\n\nX, Y = load_pima_data()\n\n\nmodels = []\nmodels.append(('logistic', LogisticRegression(solver='liblinear')))\nmodels.append(('cart', DecisionTreeClassifier()))\nmodels.append(('svm', SVC(gamma='auto')))\n\n\nensemble = VotingClassifier(models)\n\n\nresults = cross_val_score(ensemble, X, Y, \n                         cv=KFold(n_splits=10, random_state=seed, shuffle=True))\n\n\nprint(f'{results.mean()}')\n\n0.7461893369788107\n\n\n\n\n\nImprove Performance with Algorithm Tuning\nachine learning models are parameterized so that their behavior can be tuned for a given problem. Models can have many parameters and finding the best combination of parameters can be treated as a search problem. In this chapter you will discover how to tune the parameters of machine learning algorithms in Python using the scikit-learn. After completing this lesson you will know: 1. The importance of algorithm parameter tuning to improve algorithm performance. 2. How to use a grid search algorithm tuning strategy. 3. How to use a random search algorithm tuning strategy.\nMachine Learning Algorithm Parameters\nAlgorithm tuning is a final step in the process of applied machine learning before finalizing your model. It is sometimes called hyperparameter optimization where the algorithm parameters are referred to as hyperparameters, whereas the coefficients found by the machine learning algorithm itself are referred to as parameters. Optimization suggests the search-nature of the problem. Phrased as a search problem, you can use different search strategies to find a good and robust parameter or set of parameters for an algorithm on a given problem. Python scikit-learn provides two simple methods for algorithm parameter tuning:\n\nGrid Search Parameter Tuning.\nRandom Search Parameter Tuning.\n\n\nGrid Search Parameter Tuning\nGrid search is an approach to parameter tuning that will methodically build and evaluate a model for each combination of algorithm parameters specified in a grid.\n\nimport numpy\nfrom sklearn.linear_model import RidgeClassifier\nfrom sklearn.model_selection import GridSearchCV\n\n\nX, Y = load_pima_data()\n\n\nalphas = numpy.array([1, 0.1, 0.01, 0.001, 0.0001, 0])\n\n\nparam_grid = dict(alpha=alphas)\n\n\nmodel = RidgeClassifier()\n\n\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, cv=3)\n\n\ngrid.fit(X, Y)\n\nGridSearchCV(cv=3, estimator=RidgeClassifier(),\n             param_grid={'alpha': array([1.e+00, 1.e-01, 1.e-02, 1.e-03, 1.e-04, 0.e+00])})In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(cv=3, estimator=RidgeClassifier(),\n             param_grid={'alpha': array([1.e+00, 1.e-01, 1.e-02, 1.e-03, 1.e-04, 0.e+00])})estimator: RidgeClassifierRidgeClassifier()RidgeClassifierRidgeClassifier()\n\n\n\nprint(grid.best_score_)\n\n0.7708333333333334\n\n\n\nprint(grid.best_estimator_.alpha)\n\n1.0\n\n\n\n\nRandom Search Parameter Tuning\nRandom search is an approach to parameter tuning that will sample algorithm parameters from a random distribution (i.e.uniform) for a fixed number of iterations. A model is constructed and evaluated for each combination of parameters chosen.\n\nfrom scipy.stats import uniform\nfrom sklearn.linear_model import RidgeClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\n\n\nX, Y = load_pima_data()\n\n\nparam_grid = {'alpha': uniform()}\n\n\nmodel = RidgeClassifier()\n\n\nrsearch = RandomizedSearchCV(estimator=model, \n                            param_distributions=param_grid,\n                           n_iter=100, cv=3, random_state=7)\n\n\nrsearch.fit(X, Y)\n\nRandomizedSearchCV(cv=3, estimator=RidgeClassifier(), n_iter=100,\n                   param_distributions={'alpha': <scipy.stats._distn_infrastructure.rv_continuous_frozen object at 0x7ff9ffd58100>},\n                   random_state=7)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomizedSearchCVRandomizedSearchCV(cv=3, estimator=RidgeClassifier(), n_iter=100,\n                   param_distributions={'alpha': <scipy.stats._distn_infrastructure.rv_continuous_frozen object at 0x7ff9ffd58100>},\n                   random_state=7)estimator: RidgeClassifierRidgeClassifier()RidgeClassifierRidgeClassifier()\n\n\n\nprint(rsearch.best_score_)\n\n0.7708333333333334\n\n\n\nprint(rsearch.best_estimator_.alpha)\n\n0.07630828937395717\n\n\nAlgorithm parameter tuning is an important step for improving algorithm performance right before presenting results or preparing a system for production. In this chapter you discovered algorithm parameter tuning and two methods that you can use right now in Python and scikit-learn to improve your algorithm results:\n\nGrid Search Parameter Tuning\nRandom Search Parameter Tuning"
  },
  {
    "objectID": "02-end-to-end/steps.html#present-results",
    "href": "02-end-to-end/steps.html#present-results",
    "title": "Machine Learning Steps",
    "section": "Present Results",
    "text": "Present Results\nFinding an accurate machine learning model is not the end of the project. In this chapter you will discover how to save and load your machine learning model in Python using scikit-learn. This allows you to save your model to file and load it later in order to make predictions. After completing this lesson you will know:\n\nThe importance of serializing models for reuse.\nHow to use pickle to serialize and deserialize machine learning models.\nHow to use Joblib to serialize and deserialize machine learning models.\n\n\nFinalize Your Model with pickle\nPickle is the standard way of serializing objects in Python. You can use the pickle 1 operation to serialize your machine learning algorithms and save the serialized format to a file. Later you can load this file to deserialize your model and use it to make new predictions.\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom pickle import dump, load\n\n\nX, Y = load_pima_data()\n\n\nX_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size=0.33, random_state=seed)\n\n\nmodel = LogisticRegression(solver='liblinear')\nmodel.fit(X_train, Y_train)\n\nLogisticRegression(solver='liblinear')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression(solver='liblinear')\n\n\n\ndump(model, open('models/model1.sav', 'wb'))\n\n\nloaded_model = load(open('models/model1.sav', 'rb'))\n\n\nresult = loaded_model.score(X_test, Y_test)\nprint(result)\n\n0.7559055118110236\n\n\n\n\nFinalize Your Model with Joblib\nThe Joblib 2 library is part of the SciPy ecosystem and provides utilities for pipelining Python jobs. It provides utilities for saving and loading Python objects that make use of NumPy data structures, efficiently 3 . This can be useful for some machine learning algorithms that require a lot of parameters or store the entire dataset (e.g.k-Nearest Neighbors).\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom joblib import dump, load\n\n\nX, Y = load_pima_data()\n\n\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.33, random_state=7)\n\n\nmodel = LogisticRegression(solver='liblinear')\n\n\nmodel.fit(X_train, Y_train)\n\nLogisticRegression(solver='liblinear')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression(solver='liblinear')\n\n\n\ndump(model, 'models/model2.sav')\n\n['models/model2.sav']\n\n\n\nloaded_model = load('models/model2.sav')\n\n\nresult = loaded_model.score(X_test, Y_test)\nprint(result)\n\n0.7559055118110236\n\n\nTips for Finalizing Your Model\nThis section lists some important considerations when finalizing your machine learning models. * Python Version. Take note of the Python version. You almost certainly require the same major (and maybe minor) version of Python used to serialize the model when you later load it and deserialize it. * Library Versions. The version of all major libraries used in your machine learning project almost certainly need to be the same when deserializing a saved model. This is not limited to the version of NumPy and the version of scikit-learn. * Manual Serialization. You might like to manually output the parameters of your learned model so that you can use them directly in scikit-learn or another platform in the future. Often the techniques used internally by machine learning algorithms to make predictions are a lot simpler than those used to learn the parameters and can be easy to implement in custom code that you have control over.\nTake note of the version so that you can re-create the environment if for some reason you cannot reload your model on another machine or another platform at a later time.\nSummary\nIn this chapter you discovered how to persist your machine learning algorithms in Python with scikit-learn. You learned two techniques that you can use: * The pickle API for serializing standard Python objects. * The Joblib API for efficiently serializing Python objects with NumPy arrays."
  },
  {
    "objectID": "02-end-to-end/intro.html",
    "href": "02-end-to-end/intro.html",
    "title": "Introduction",
    "section": "",
    "text": "This book focuses on a specific sub-field of machine learning called predictive modeling. This is the field of machine learning that is the most useful in industry and the type of machine learning that the scikit-learn library in Python excels at facilitating. Unlike statistics, where models are used to understand data, predictive modeling is laser focused on developing models that make the most accurate predictions at the expense of explaining why predictions are made. Unlike the broader field of machine learning that could feasibly be used with data in any format, predictive modeling is primarily focused on tabular data (e.g.tables of numbers like in a spreadsheet).\nA predictive modeling machine learning project can be broken down into 6 top-level tasks: 1. Define Problem: Investigate and characterize the problem in order to better understand the goals of the project. 2. Analyze Data: Use descriptive statistics and visualization to better understand the data you have available. 3. Prepare Data: Use data transforms in order to better expose the structure of the prediction problem to modeling algorithms. 4. Evaluate Algorithms: Design a test harness to evaluate a number of standard algorithms on the data and select the top few to investigate further. 5. Improve Results: Use algorithm tuning and ensemble methods to get the most out of well-performing algorithms on your data. 6. Present Results: Finalize the model, make predictions and present results.\nYou need to piece the recipes together into end-to-end projects. This will show you how to actually deliver a model or make predictions on new data using Python. This book uses small well-understood machine learning datasets from the UCI Machine learning repository 1 in both the lessons and in the example projects. These datasets are available for free as CSV downloads. These datasets are excellent for practicing applied machine learning because: * They are small, meaning they fit into memory and algorithms can model them in reasonable time. * They are well behaved, meaning you often dont need to do a lot of feature engineering to get a good result. * They are benchmarks, meaning that many people have used them before and you can get ideas of good algorithms to try and accuracy levels you should expect.\nIn Part III you will work through three projects:\nHello World Project (Iris flowers dataset) : This is a quick pass through the project steps without much tuning or optimizing on a dataset that is widely used as the hello world of machine learning.\nRegression (Boston House Price dataset) : Work through each step of the project process with a regression problem.\nBinary Classification (Sonar dataset) : Work through each step of the project process using all of the methods on a binary classification problem."
  }
]