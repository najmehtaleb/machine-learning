{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65e2431d",
   "metadata": {},
   "source": [
    "# Machine Learning Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473ee4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = '/home/naji/Desktop/github-repos/machine-learning/nbs/data/'\n",
    "pima = 'pima-indians-diabetes.csv'\n",
    "housing = 'housing.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1d6c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 7\n",
    "pima_names = ['preg','plas','pres','skin','test','mass','pedi','age','class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a234acf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold, cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c0bbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pima_data():\n",
    "    dataframe = read_csv(filepath+pima, names=pima_names)\n",
    "    array = dataframe.values\n",
    "    X = array[:, 0:8]\n",
    "    Y = array[:, 8]\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c340ea",
   "metadata": {},
   "source": [
    "## Analyze Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497a10b7",
   "metadata": {},
   "source": [
    "### Load Machine Learning Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eddff9b",
   "metadata": {},
   "source": [
    "You must be able to load your data before you can start your machine learning project. The\n",
    "most common format for machine learning data is CSV files. There are a number of ways to\n",
    "load a CSV file in Python. In this lesson you will learn three ways that you can use to load\n",
    "your CSV data in Python:\n",
    "1. Load CSV Files with the Python Standard Library.\n",
    "2. Load CSV Files with NumPy.\n",
    "3. Load CSV Files with Pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41d6477",
   "metadata": {},
   "source": [
    "**Considerations When Loading CSV Data**\n",
    "\n",
    "**File Header** \\\n",
    "Does your data have a file header? If so this can help in automatically assigning names to each column of data. If not, you may need to name your attributes manually. Either way, you should explicitly specify whether or not your CSV file has a file header when loading your data.\n",
    "\n",
    "\n",
    "**Comments**\\\n",
    "Does your data have comments? Comments in a CSV file are indicated by a hash (#) at the\n",
    "start of a line. If you have comments in your file, depending on the method used to load your data, you may need to indicate whether or not to expect comments and the character to expect to signify a comment line.\n",
    "\n",
    "**Delimiter**\\\n",
    "The standard delimiter that separates values in fields is the comma (,) character. Your file could use a different delimiter like tab or white space in which case you must specify it explicitly.\n",
    "\n",
    "**Quotes**\\\n",
    "Sometimes field values can have spaces. In these CSV files the values are often quoted. The default quote character is the double quotation marks character. Other characters can be used, and you must specify the quote character used in your file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1f4798",
   "metadata": {},
   "source": [
    "**Pima Indians Dataset**\n",
    "\n",
    "https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3194cfc9",
   "metadata": {},
   "source": [
    "#### Load CSV Files with the Python Standard Library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310a55bb",
   "metadata": {},
   "source": [
    "The Python API provides the module CSV and the function reader() that can be used to load\n",
    "CSV files. Once loaded, you can convert the CSV data to a NumPy array and use it for machine\n",
    "learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092fbe92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aeee15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = open(filepath+filename, 'rt')\n",
    "reader = csv.reader(raw_data, delimiter=',', quoting=csv.QUOTE_NONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8677d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = list(reader)\n",
    "x[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed2fa17",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = numpy.array(x).astype('float')\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e708f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993c7d1a",
   "metadata": {},
   "source": [
    "#### Load CSV Files with NumPy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26287c66",
   "metadata": {},
   "source": [
    "You can load your CSV data using NumPy and the numpy.loadtxt() function. This function\n",
    "assumes no header row and all data has the same format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef288bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import loadtxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda01fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = open(filepath + filename, 'rt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b0ccc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = loadtxt(raw_data, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3bf318",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b67585",
   "metadata": {},
   "source": [
    "This example can be modified to load the same dataset directly from a URL as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c787f239",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e03392",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://goo.gl/bDdBiA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8478958",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = urlopen(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa93208",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = loadtxt(raw_data, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f61a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47199162",
   "metadata": {},
   "source": [
    "#### Load CSV Files with Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c00884",
   "metadata": {},
   "source": [
    "You can load your CSV data using Pandas and the pandas.read csv() function. This function is very flexible and is perhaps my recommended approach for loading your machine learning data. The function returns a pandas.DataFrame 6 that you can immediately start summarizing and plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3f2dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9002414",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['preg','plas','pres','skin','test','mass','pedi','age','class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec73dc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_csv(filepath + filename, names=names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec186fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d5b127",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b19591",
   "metadata": {},
   "source": [
    "We can also modify this example to load CSV data directly from a URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34218aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "url='https://goo.gl/bDdBiA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54485aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_csv(url, names=names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9801232e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a95b494",
   "metadata": {},
   "source": [
    "Generally I recommend that you load your data with Pandas in practice and all subsequent examples in this book will use this method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9fd5042",
   "metadata": {},
   "source": [
    "### Understand Your Data With Descriptive Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3124c240",
   "metadata": {},
   "source": [
    "You must understand your data in order to get the best results. In this chapter you will discover\n",
    "7 recipes that you can use in Python to better understand your machine learning data. After\n",
    "reading this lesson you will know how to:\n",
    "1. Take a peek at your raw data.\n",
    "2. Review the dimensions of your dataset.\n",
    "3. Review the data types of attributes in your data.\n",
    "4. Summarize the distribution of instances across classes in your dataset.\n",
    "5. Summarize your data using descriptive statistics.\n",
    "6. Understand the relationships in your data using correlations.\n",
    "7. Review the skew of the distributions of each attribute."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff62a54e",
   "metadata": {},
   "source": [
    "#### Peek at Your Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65458d82",
   "metadata": {},
   "source": [
    "There is no substitute for looking at the raw data. Looking at the raw data can reveal insights\n",
    "that you cannot get any other way. It can also plant seeds that may later grow into ideas on\n",
    "how to better pre-process and handle the data for machine learning tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17070d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086dd7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['preg','plas','pres','skin','test','mass','pedi','age','class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefe95ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_csv(filepath + filename, names=names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d623055",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5378e656",
   "metadata": {},
   "source": [
    "#### Dimensions of Your Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99ed471",
   "metadata": {},
   "source": [
    "You must have a very good handle on how much data you have, both in terms of rows and\n",
    "columns.\n",
    " Too many rows and algorithms may take too long to train. Too few and perhaps you do\n",
    "not have enough data to train the algorithms.\n",
    " Too many features and some algorithms can be distracted or suffer poor performance due\n",
    "to the curse of dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311f6e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23995e3",
   "metadata": {},
   "source": [
    "#### Data Type For Each Attribute"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60114e8d",
   "metadata": {},
   "source": [
    "The type of each attribute is important. Strings may need to be converted to floating point\n",
    "values or integers to represent categorical or ordinal values. You can get an idea of the types of\n",
    "attributes by peeking at the raw data, as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4bab93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9b75fa",
   "metadata": {},
   "source": [
    "#### Descriptive Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7ce081",
   "metadata": {},
   "source": [
    "Descriptive statistics can give you great insight into the properties of each attribute. Often you\n",
    "can create more summaries than you have time to review. The describe() function on the\n",
    "Pandas DataFrame lists 8 statistical properties of each attribute. They are:\n",
    " Count.\n",
    " Mean.\n",
    " Standard Deviation.\n",
    " Minimum Value.\n",
    " 25th Percentile.\n",
    " 50th Percentile (Median).\n",
    " 75th Percentile.\n",
    " Maximum Value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed3c508",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import set_option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f8634f",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_option('display.width', 100)\n",
    "set_option('display.precision', 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1240680",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1918471",
   "metadata": {},
   "source": [
    "#### Class Distribution (Classification Only)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d608c0",
   "metadata": {},
   "source": [
    "On classification problems you need to know how balanced the class values are. Highly imbalanced\n",
    "problems (a lot more observations for one class than another) are common and may need special\n",
    "handling in the data preparation stage of your project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5514185c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_coutns = data.groupby('class').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08674ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(class_coutns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8ff866",
   "metadata": {},
   "source": [
    "#### Correlations Between Attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1be0047",
   "metadata": {},
   "source": [
    "Correlation refers to the relationship between two variables and how they may or may not\n",
    "change together. The most common method for calculating correlation is Pearson’s Correlation\n",
    "Coefficient, that assumes a normal distribution of the attributes involved. A correlation of -1\n",
    "or 1 shows a full negative or positive correlation respectively. Whereas a value of 0 shows no\n",
    "correlation at all. Some machine learning algorithms like linear and logistic regression can suffer\n",
    "poor performance if there are highly correlated attributes in your dataset. As such, it is a good\n",
    "idea to review all of the pairwise correlations of the attributes in your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603f4244",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_option('display.width', 100)\n",
    "set_option('display.precision', 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b530350",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.corr(method='pearson')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768133b4",
   "metadata": {},
   "source": [
    "#### Skew of Univariate Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae53ce6a",
   "metadata": {},
   "source": [
    "Skew refers to a distribution that is assumed Gaussian (normal or bell curve) that is shifted or\n",
    "squashed in one direction or another. Many machine learning algorithms assume a Gaussian\n",
    "distribution. Knowing that an attribute has a skew may allow you to perform data preparation\n",
    "to correct the skew and later improve the accuracy of your models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac17f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.skew()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0425659",
   "metadata": {},
   "source": [
    "The skew results show a positive (right) or negative (left) skew. Values closer to zero show\n",
    "less skew."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a049f875",
   "metadata": {},
   "source": [
    "Tips To Remember\n",
    "\n",
    "This section gives you some tips to remember when reviewing your data using summary statistics.\n",
    " Review the numbers. Generating the summary statistics is not enough. Take a moment\n",
    "to pause, read and really think about the numbers you are seeing.\n",
    " Ask why. Review your numbers and ask a lot of questions. How and why are you seeing\n",
    "specific values. Think about how the numbers relate to the problem domain in general\n",
    "and specific entities that observations relate to.\n",
    " Write down ideas. Write down your observations and ideas. Keep a small text file or\n",
    "note pad and jot down all of the ideas for how variables may relate, for what numbers\n",
    "mean, and ideas for techniques to try later. The things you write down now while the\n",
    "data is fresh will be very valuable later when you are trying to think up new things to try."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee86b6e",
   "metadata": {},
   "source": [
    "Summary\n",
    "\n",
    "In this chapter you discovered the importance of describing your dataset before you start work\n",
    "on your machine learning project. You discovered 7 different ways to summarize your dataset\n",
    "using Python and Pandas:\n",
    " Peek At Your Data.\n",
    " Dimensions of Your Data.\n",
    " Data Types.\n",
    " Class Distribution.\n",
    " Data Summary.\n",
    " Correlations.\n",
    " Skewness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbd13de",
   "metadata": {},
   "source": [
    "### Understand Your Data With Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb1804f",
   "metadata": {},
   "source": [
    "You must understand your data in order to get the best results from machine learning algorithms.\n",
    "The fastest way to learn more about your data is to use data visualization. In this chapter you\n",
    "will discover exactly how you can visualize your machine learning data in Python using Pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22387c9c",
   "metadata": {},
   "source": [
    "#### Univariate Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79af52ae",
   "metadata": {},
   "source": [
    "In this section we will look at three techniques that you can use to understand each attribute of\n",
    "your dataset independently.\n",
    "\n",
    "* Histograms.\n",
    "* Density Plots.\n",
    "* Box and Whisker Plots."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f95cbb",
   "metadata": {},
   "source": [
    "##### Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79a3b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33fcfc22",
   "metadata": {},
   "source": [
    "A fast way to get an idea of the distribution of each attribute is to look at histograms. Histograms\n",
    "group data into bins and provide you a count of the number of observations in each bin. From\n",
    "the shape of the bins you can quickly get a feeling for whether an attribute is Gaussian, skewed\n",
    "or even has an exponential distribution. It can also help you see possible outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ce6cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da11e608",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.hist(figsize=[10,10])\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28317a6",
   "metadata": {},
   "source": [
    "We can see that perhaps the attributes age, pedi and test may have an exponential\n",
    "distribution. We can also see that perhaps the mass and pres and plas attributes may have a\n",
    "Gaussian or nearly Gaussian distribution. This is interesting because many machine learning\n",
    "techniques assume a Gaussian univariate distribution on the input variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64af02ca",
   "metadata": {},
   "source": [
    "##### Density Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2fbf8c",
   "metadata": {},
   "source": [
    "Density plots are another way of getting a quick idea of the distribution of each attribute. The\n",
    "plots look like an abstracted histogram with a smooth curve drawn through the top of each bin,\n",
    "much like your eye tried to do with the histograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6e3c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.plot(kind='density', subplots=True, layout=(3,3), sharex=False)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36bce6af",
   "metadata": {},
   "source": [
    "##### Box and Whisker Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6b4546",
   "metadata": {},
   "source": [
    "Another useful way to review the distribution of each attribute is to use Box and Whisker Plots\n",
    "or boxplots for short. Boxplots summarize the distribution of each attribute, drawing a line for\n",
    "the median (middle value) and a box around the 25th and 75th percentiles (the middle 50% of\n",
    "the data). The whiskers give an idea of the spread of the data and dots outside of the whiskers\n",
    "show candidate outlier values (values that are 1.5 times greater than the size of spread of the\n",
    "middle 50% of the data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfcf4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.plot(kind='box', subplots=True, layout=(3,3), sharex=False, sharey=False)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb4aa0e",
   "metadata": {},
   "source": [
    "We can see that the spread of attributes is quite different. Some like age, test and skin\n",
    "appear quite skewed towards smaller values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42675aa7",
   "metadata": {},
   "source": [
    "#### Multivariate Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee9470e",
   "metadata": {},
   "source": [
    "This section provides examples of two plots that show the interactions between multiple variables\n",
    "in your dataset.\n",
    " Correlation Matrix Plot.\n",
    " Scatter Plot Matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb752b62",
   "metadata": {},
   "source": [
    "##### Correlation Matrix Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85236d6b",
   "metadata": {},
   "source": [
    "Correlation gives an indication of how related the changes are between two variables. If two\n",
    "variables change in the same direction they are positively correlated. If they change in opposite\n",
    "directions together (one goes up, one goes down), then they are negatively correlated. You can\n",
    "calculate the correlation between each pair of attributes. This is called a correlation matrix. You\n",
    "can then plot the correlation matrix and get an idea of which variables have a high correlation with each other. This is useful to know, because some machine learning algorithms like linear\n",
    "and logistic regression can have poor performance if there are highly correlated input variables\n",
    "in your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa12ff07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot correlation matrix\n",
    "\n",
    "fig = pyplot.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(data.corr(), vmin=-1, vmax=1)\n",
    "fig.colorbar(cax)\n",
    "ticks = numpy.arange(0,9,1)\n",
    "ax.set_xticks(ticks)\n",
    "ax.set_yticks(ticks)\n",
    "ax.set_xticklabels(names)\n",
    "ax.set_yticklabels(names)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a59d622",
   "metadata": {},
   "source": [
    "We can see that the matrix is symmetrical, i.e. the bottom left of the matrix is the same as\n",
    "the top right. This is useful as we can see two different views on the same data in one plot. We\n",
    "can also see that each variable is perfectly positively correlated with itself (as you would have\n",
    "expected) in the diagonal line from top left to bottom right."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b96bf7",
   "metadata": {},
   "source": [
    "The example is not generic in that it specifies the names for the attributes along the axes as\n",
    "well as the number of ticks. This recipe can be made more generic by removing these aspects as\n",
    "follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a2b962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot correlation matrix\n",
    "fig = pyplot.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(data.corr(), vmin=-1, vmax=1)\n",
    "fig.colorbar(cax)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79c006d",
   "metadata": {},
   "source": [
    "Generating the plot, you can see that it gives the same information although making it a\n",
    "little harder to see what attributes are correlated by name. Use this generic plot as a first cut\n",
    "to understand the correlations in your dataset and customize it like the first example in order\n",
    "to read off more specific data if needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde35716",
   "metadata": {},
   "source": [
    "##### Scatter Plot Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ec9438",
   "metadata": {},
   "source": [
    "A scatter plot shows the relationship between two variables as dots in two dimensions, one\n",
    "axis for each attribute. You can create a scatter plot for each pair of attributes in your data.\n",
    "Drawing all these scatter plots together is called a scatter plot matrix. Scatter plots are useful\n",
    "for spotting structured relationships between variables, like whether you could summarize the\n",
    "relationship between two variables with a line. Attributes with structured relationships may\n",
    "also be correlated and good candidates for removal from your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb38acb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import scatter_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c888e14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_matrix(data)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0332c586",
   "metadata": {},
   "source": [
    "Like the Correlation Matrix Plot above, the scatter plot matrix is symmetrical. This is\n",
    "useful to look at the pairwise relationships from different perspectives. Because there is little point of drawing a scatter plot of each variable with itself, the diagonal shows histograms of\n",
    "each attribute."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996d88fe",
   "metadata": {},
   "source": [
    "In this chapter you discovered a number of ways that you can better understand your machine\n",
    "learning data in Python using Pandas. Specifically, you learned how to plot your data using:\n",
    " Histograms.\n",
    " Density Plots.\n",
    " Box and Whisker Plots.\n",
    " Correlation Matrix Plot.\n",
    " Scatter Plot Matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b276de",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be67ae32",
   "metadata": {},
   "source": [
    "Many machine learning algorithms make assumptions about your data. It is often a very good\n",
    "idea to prepare your data in such a way to best expose the structure of the problem to the\n",
    "machine learning algorithms that you intend to use. In this chapter you will discover how to\n",
    "prepare your data for machine learning in Python using scikit-learn. After completing this\n",
    "lesson you will know how to:\n",
    "1. Rescale data.\n",
    "2. Standardize data.\n",
    "3. Normalize data.\n",
    "4. Binarize data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82abe6e7",
   "metadata": {},
   "source": [
    "**Need For Data Pre-processing**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e677391",
   "metadata": {},
   "source": [
    "You almost always need to pre-process your data. It is a required step. A difficulty is that\n",
    "different algorithms make different assumptions about your data and may require different\n",
    "transforms. Further, when you follow all of the rules and prepare your data, sometimes algorithms\n",
    "can deliver better results without pre-processing.\n",
    "\n",
    "Generally, I would recommend creating many different views and transforms of your data,\n",
    "then exercise a handful of algorithms on each view of your dataset. This will help you to flush\n",
    "out which data transforms might be better at exposing the structure of your problem in general."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7e6831",
   "metadata": {},
   "source": [
    "**Data Transforms**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7a718e",
   "metadata": {},
   "source": [
    "In this lesson you will work through 4 different data pre-processing recipes for machine learning.\n",
    "The Pima Indian diabetes dataset is used in each recipe. Each recipe follows the same structure:\n",
    "\n",
    " Load the dataset.\n",
    " Split the dataset into the input and output variables for machine learning.\n",
    " Apply a pre-processing transform to the input variables.\n",
    " Summarize the data to show the change."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeff219c",
   "metadata": {},
   "source": [
    "The scikit-learn library provides two standard idioms for transforming data. Each are useful\n",
    "in different circumstances. The transforms are calculated in such a way that they can be applied\n",
    "to your training data and any samples of data you may have in the future. The scikit-learn\n",
    "documentation has some information on how to use various different pre-processing methods:\n",
    "\n",
    " Fit and Multiple Transform.\n",
    " Combined Fit-And-Transform."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13d1be8",
   "metadata": {},
   "source": [
    "The Fit and Multiple Transform method is the preferred approach. You call the fit()\n",
    "function to prepare the parameters of the transform once on your data. Then later you can use\n",
    "the transform() function on the same data to prepare it for modeling and again on the test or\n",
    "validation dataset or new data that you may see in the future. The Combined Fit-And-Transform\n",
    "is a convenience that you can use for one off tasks. This might be useful if you are interested\n",
    "in plotting or summarizing the transformed data. You can review the preprocess API in\n",
    "scikit-learn here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f67595",
   "metadata": {},
   "source": [
    "### Rescale Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735b209a",
   "metadata": {},
   "source": [
    "When your data is comprised of attributes with varying scales, many machine learning algorithms\n",
    "can benefit from rescaling the attributes to all have the same scale. Often this is referred to\n",
    "as normalization and attributes are often rescaled into the range between 0 and 1. This is\n",
    "useful for optimization algorithms used in the core of machine learning algorithms like gradient\n",
    "descent. It is also useful for algorithms that weight inputs like regression and neural networks\n",
    "and algorithms that use distance measures like k-Nearest Neighbors. You can rescale your data\n",
    "using scikit-learn using the MinMaxScaler class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7739ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "from numpy import set_printoptions\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867c5a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['preg','plas','pres','skin','test','mass','pedi','age','class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fd0a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = read_csv(filepath+filename, names=names)\n",
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d09b167",
   "metadata": {},
   "outputs": [],
   "source": [
    "array = dataframe.values\n",
    "array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe6c366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate array into input and output components\n",
    "\n",
    "X = array[:, 0:8]\n",
    "Y = array[:, 8]\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "rescaledX = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6581e811",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_printoptions(precision=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497104b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "rescaledX[0:5, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d21b2fb",
   "metadata": {},
   "source": [
    "### Standardize Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6edb21f5",
   "metadata": {},
   "source": [
    "Standardization is a useful technique to transform attributes with a Gaussian distribution and\n",
    "differing means and standard deviations to a standard Gaussian distribution with a mean of\n",
    "0 and a standard deviation of 1. It is most suitable for techniques that assume a Gaussian\n",
    "distribution in the input variables and work better with rescaled data, such as linear regression,\n",
    "logistic regression and linear discriminate analysis. You can standardize data using scikit-learn\n",
    "with the StandardScaler class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1bf0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from pandas import read_csv\n",
    "from numpy import set_printoptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84965090",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['preg','plas','pres','skin','test','mass','pedi','age','class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92ffef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = read_csv(filepath+filename, names=names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85662eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "array = dataframe.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a5bfcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate array into input and output components\n",
    "X = array[:, 0:8]\n",
    "Y = array[:, 8]\n",
    "scaler = StandardScaler().fit(X)\n",
    "rescaledX = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0497919d",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_printoptions(precision=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c851e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "rescaledX[0:5, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd89399d",
   "metadata": {},
   "source": [
    "The values for each attribute now have a mean value of 0 and a standard deviation of 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2f42ac",
   "metadata": {},
   "source": [
    "### Normalize Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754c3f80",
   "metadata": {},
   "source": [
    "Normalizing in scikit-learn refers to rescaling each observation (row) to have a length of 1 (called\n",
    "a unit norm or a vector with the length of 1 in linear algebra). This pre-processing method\n",
    "can be useful for sparse datasets (lots of zeros) with attributes of varying scales when using\n",
    "algorithms that weight input values such as neural networks and algorithms that use distance\n",
    "measures such as k-Nearest Neighbors. You can normalize data in Python with scikit-learn\n",
    "using the Normalizer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf4d4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "from pandas import read_csv\n",
    "from numpy import set_printoptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57bb0172",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['preg','plas','pres','skin','test','mass','pedi','age','class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230fca89",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = read_csv(filepath+filename, names=names)\n",
    "array = dataframe.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a80a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = array[:, 0:8]\n",
    "Y = array[:, 8]\n",
    "scaler = Normalizer().fit(X)\n",
    "normalizedX = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7286773",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_printoptions(precision=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7651d6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizedX[0:5, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69bb7b0",
   "metadata": {},
   "source": [
    "### Binarize Data (Make Binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30bedd5b",
   "metadata": {},
   "source": [
    "You can transform your data using a binary threshold. All values above the threshold are\n",
    "marked 1 and all equal to or below are marked as 0. This is called binarizing your data or\n",
    "thresholding your data. It can be useful when you have probabilities that you want to make\n",
    "into crisp values. It is also useful when feature engineering and you want to add new features\n",
    "that indicate something meaningful. You can create new binary attributes in Python using\n",
    "scikit-learn with the Binarizer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490d2b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Binarizer\n",
    "from pandas import read_csv\n",
    "from numpy import set_printoptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2042b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['preg','plas','pres','skin','test','mass','pedi','age','class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ed919f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = read_csv(filepath+filename, names=names)\n",
    "array = dataframe.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4013047",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = array[:, 0:8]\n",
    "Y = array[:, 8]\n",
    "binarizer = Binarizer(threshold=0).fit(X)\n",
    "binaryX = binarizer.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04047d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_printoptions(precision=3)\n",
    "binaryX[0:5, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62efd67",
   "metadata": {},
   "source": [
    "In this chapter you discovered how you can prepare your data for machine learning in Python\n",
    "using scikit-learn. You now have recipes to:\n",
    "\n",
    " Rescale data.\n",
    " Standardize data.\n",
    " Normalize data.\n",
    " Binarize data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35c0598",
   "metadata": {},
   "source": [
    "You now know how to transform your data to best expose the structure of your problem to the\n",
    "modeling algorithms. In the next lesson you will discover how to select the features of your data\n",
    "that are most relevant to making predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a1ca5b",
   "metadata": {},
   "source": [
    "## Evaluate Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50eb67c",
   "metadata": {},
   "source": [
    "### Evaluate the Performance of Machine Learning Algorithms with Resampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c1f0d2",
   "metadata": {},
   "source": [
    "You need to know how well your algorithms perform on unseen data. The best way to evaluate\n",
    "the performance of an algorithm would be to make predictions for new data to which you\n",
    "already know the answers. The second best way is to use clever techniques from statistics called\n",
    "resampling methods that allow you to make accurate estimates for how well your algorithm will\n",
    "perform on new data. In this chapter you will discover how you can estimate the accuracy of\n",
    "your machine learning algorithms using resampling methods in Python and scikit-learn on the\n",
    "Pima Indians dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524e9376",
   "metadata": {},
   "source": [
    "**Evaluate Machine Learning Algorithms**\n",
    "\n",
    "Why can’t you prepare your machine learning algorithm on your training dataset and use\n",
    "predictions from this same dataset to evaluate performance? The simple answer is overfitting.\n",
    "Imagine an algorithm that remembers every observation it is shown during training. If you\n",
    "evaluated your machine learning algorithm on the same dataset used to train the algorithm, then\n",
    "an algorithm like this would have a perfect score on the training dataset. But the predictions it\n",
    "made on new data would be terrible. We must evaluate our machine learning algorithms on\n",
    "data that is not used to train the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b1fecf",
   "metadata": {},
   "source": [
    "A model evaluation is an estimate that we can use to talk about how well we think the\n",
    "method may actually do in practice. It is not a guarantee of performance. Once we estimate the\n",
    "performance of our algorithm, we can then re-train the final algorithm on the entire training\n",
    "dataset and get it ready for operational use. Next up we are going to look at four different\n",
    "techniques that we can use to split up our training dataset and create useful estimates of\n",
    "performance for our machine learning algorithms:\n",
    "\n",
    "* Train and Test Sets.\n",
    "* k-fold Cross-Validation.\n",
    "* Leave One Out Cross-Validation.\n",
    "* Repeated Random Test-Train Splits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17cdd374",
   "metadata": {},
   "source": [
    "#### Split into Train and Test Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de043bf8",
   "metadata": {},
   "source": [
    "The simplest method that we can use to evaluate the performance of a machine learning\n",
    "algorithm is to use different training and testing datasets. We can take our original dataset and\n",
    "split it into two parts. Train the algorithm on the first part, make predictions on the second\n",
    "part and evaluate the predictions against the expected results. The size of the split can depend\n",
    "on the size and specifics of your dataset, although it is common to use 67% of the data for\n",
    "training and the remaining 33% for testing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c1c3bb",
   "metadata": {},
   "source": [
    "This algorithm evaluation technique is very fast. It is ideal for large datasets (millions of\n",
    "records) where there is strong evidence that both splits of the data are representative of the\n",
    "underlying problem. Because of the speed, it is useful to use this approach when the algorithm\n",
    "you are investigating is slow to train. A downside of this technique is that it can have a high\n",
    "variance. This means that differences in the training and test dataset can result in meaningful\n",
    "differences in the estimate of accuracy. In the example below we split the Pima Indians dataset\n",
    "into 67%/33% splits for training and test and evaluate the accuracy of a Logistic Regression\n",
    "model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc1ebc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142c876b",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['preg','plas','pres','skin','test','mass','pedi','age','class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9914ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = read_csv(filepath+filename, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:, 0:8]\n",
    "Y = array[:, 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825acf29",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.33\n",
    "seed = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5d4485",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61dd37bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(solver='liblinear')\n",
    "model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80bfe4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.score(X_test, Y_test)\n",
    "print(f'Accuracy: {result*100: 0.3f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0ce256",
   "metadata": {},
   "source": [
    "We can see that the estimated accuracy for the model was approximately 75%. Note that\n",
    "in addition to specifying the size of the split, we also specify the random seed. Because the\n",
    "split of the data is random, we want to ensure that the results are reproducible. By specifying\n",
    "the random seed we ensure that we get the same random numbers each time we run the code\n",
    "and in turn the same split of data. This is important if we want to compare this result to\n",
    "the estimated accuracy of another machine learning algorithm or the same algorithm with a\n",
    "different configuration. To ensure the comparison was apples-for-apples, we must ensure that\n",
    "they are trained and tested on exactly the same data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5694c00",
   "metadata": {},
   "source": [
    "#### K-fold Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702bb2b4",
   "metadata": {},
   "source": [
    "Cross-validation is an approach that you can use to estimate the performance of a machine\n",
    "learning algorithm with less variance than a single train-test set split. It works by splitting\n",
    "the dataset into k-parts (e.g. k = 5 or k = 10). Each split of the data is called a fold. The\n",
    "algorithm is trained on k − 1 folds with one held back and tested on the held back fold. This is\n",
    "repeated so that each fold of the dataset is given a chance to be the held back test set. After\n",
    "running cross-validation you end up with k different performance scores that you can summarize\n",
    "using a mean and a standard deviation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b02c63",
   "metadata": {},
   "source": [
    "The result is a more reliable estimate of the performance of the algorithm on new data. It is\n",
    "more accurate because the algorithm is trained and evaluated multiple times on different data.\n",
    "The choice of k must allow the size of each test partition to be large enough to be a reasonable\n",
    "sample of the problem, whilst allowing enough repetitions of the train-test evaluation of the\n",
    "algorithm to provide a fair estimate of the algorithms performance on unseen data. For modest\n",
    "sized datasets in the thousands or tens of thousands of records, k values of 3, 5 and 10 are\n",
    "common. In the example below we use 10-fold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bef394",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4627be",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['preg','plas','pres','skin','test','mass','pedi','age','class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624dd127",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = read_csv(filepath+filename, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:, 0:8]\n",
    "Y = array[:, 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd509e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=10, random_state=7, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32853280",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(solver='liblinear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8891d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = cross_val_score(model, X, Y, cv=kfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a2ff3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Accuracy: {results.mean()*100 : .3f} ({results.std()*100 : .3f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8753c0",
   "metadata": {},
   "source": [
    "You can see that we report both the mean and the standard deviation of the performance\n",
    "measure. When summarizing performance measures, it is a good practice to summarize the\n",
    "distribution of the measures, in this case assuming a Gaussian distribution of performance (a\n",
    "very reasonable assumption) and recording the mean and standard deviation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466a252d",
   "metadata": {},
   "source": [
    "#### Leave One Out Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749e5498",
   "metadata": {},
   "source": [
    "You can configure cross-validation so that the size of the fold is 1 (k is set to the number of\n",
    "observations in your dataset). This variation of cross-validation is called leave-one-out cross-\n",
    "validation. The result is a large number of performance measures that can be summarized\n",
    "in an effort to give a more reasonable estimate of the accuracy of your model on unseen data. A downside is that it can be a computationally more expensive procedure than k-fold\n",
    "cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d62e7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedcd36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['preg','plas','pres','skin','test','mass','pedi','age','class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2347158b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = read_csv(filepath+filename, names=names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff062469",
   "metadata": {},
   "outputs": [],
   "source": [
    "array = dataframe.values\n",
    "X = array[:, 0:8]\n",
    "Y = array[:, 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a4b415",
   "metadata": {},
   "outputs": [],
   "source": [
    "loov = LeaveOneOut()\n",
    "model = LogisticRegression(solver='liblinear')\n",
    "results = cross_val_score(model,X, Y, cv=loov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7d9dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Accuracy: {results.mean()*100: 0.3f}% ({results.std()*100: 0.3f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f913d7",
   "metadata": {},
   "source": [
    "You can see in the standard deviation that the score has more variance than the k-fold\n",
    "cross-validation results described above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2409c30",
   "metadata": {},
   "source": [
    "#### Repeated Random Test-Train Splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d881b920",
   "metadata": {},
   "source": [
    "Another variation on k-fold cross-validation is to create a random split of the data like the\n",
    "train/test split described above, but repeat the process of splitting and evaluation of the\n",
    "algorithm multiple times, like cross-validation. This has the speed of using a train/test split and\n",
    "the reduction in variance in the estimated performance of k-fold cross-validation. You can also\n",
    "repeat the process many more times as needed to improve the accuracy. A down side is that\n",
    "repetitions may include much of the same data in the train or the test split from run to run,\n",
    "introducing redundancy into the evaluation. The example below splits the data into a 67%/33%\n",
    "train/test split and repeats the process 10 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0c9448",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d687443d",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['preg','plas','pres','skin','test','mass','pedi','age','class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036bb00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = read_csv(filepath+filename, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:, 0:8]\n",
    "Y = array[:, 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04550a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = 10\n",
    "test_size = 0.33\n",
    "seed = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fe3f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = ShuffleSplit(n_splits=n_splits, test_size=test_size, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7c0077",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(solver='liblinear')\n",
    "results = cross_val_score(model, X, Y, cv=kfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01220296",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Accuracy: {results.mean()*100 : 0.3f}% ({results.std()*100 : 0.3f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe0dcce",
   "metadata": {},
   "source": [
    "**What Techniques to Use When**\n",
    "\n",
    "This section lists some tips to consider what resampling technique to use in different circumstances.\n",
    "\n",
    "* Generally k-fold cross-validation is the gold standard for evaluating the performance of a\n",
    "machine learning algorithm on unseen data with k set to 3, 5, or 10.\n",
    "* Using a train/test split is good for speed when using a slow algorithm and produces\n",
    "performance estimates with lower bias when using large datasets.\n",
    "* Techniques like leave-one-out cross-validation and repeated random splits can be useful\n",
    "intermediates when trying to balance variance in the estimated performance, model\n",
    "training speed and dataset size.\n",
    "\n",
    "The best advice is to experiment and find a technique for your problem that is fast and\n",
    "produces reasonable estimates of performance that you can use to make decisions. If in doubt,\n",
    "use 10-fold cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643ad229",
   "metadata": {},
   "source": [
    "### Machine Learning Algorithm Performance Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706c5d0a",
   "metadata": {},
   "source": [
    "The metrics that you choose to evaluate your machine learning algorithms are very important.\n",
    "Choice of metrics influences how the performance of machine learning algorithms is measured\n",
    "and compared. They influence how you weight the importance of different characteristics in\n",
    "the results and your ultimate choice of which algorithm to choose. In this chapter you will\n",
    "discover how to select and use different machine learning performance metrics in Python with\n",
    "scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecc9da4",
   "metadata": {},
   "source": [
    "#### Classification Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57efc0b",
   "metadata": {},
   "source": [
    "Classification problems are perhaps the most common type of machine learning problem and as\n",
    "such there is a myriad of metrics that can be used to evaluate predictions for these problems.\n",
    "In this section we will review how to use the following metrics:\n",
    "\n",
    "* Classification Accuracy.\n",
    "* Logistic Loss.\n",
    "* Area Under ROC Curve.\n",
    "* Confusion Matrix.\n",
    "* Classification Report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6278fe83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493abf8f",
   "metadata": {},
   "source": [
    "**Classification Accuracy**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c458795",
   "metadata": {},
   "source": [
    "Classification accuracy is the number of correct predictions made as a ratio of all predictions\n",
    "made. This is the most common evaluation metric for classification problems, it is also the most\n",
    "misused. It is really only suitable when there are an equal number of observations in each class\n",
    "(which is rarely the case) and that all predictions and prediction errors are equally important,\n",
    "which is often not the case. Below is an example of calculating classification accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4832e70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c279aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['preg','plas','pres','skin','test','mass','pedi','age','class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54f1b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = read_csv(filepath+filename, names=names )\n",
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8e8488",
   "metadata": {},
   "outputs": [],
   "source": [
    "array = dataframe.values\n",
    "array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fa1f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = array[:, 0:8]\n",
    "Y = array[:, 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2388ce6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=10, shuffle=True, random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b8b3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(solver='liblinear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d18981",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = cross_val_score(model, X, Y, cv=kfold, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871ca64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Accuracy is: {results.mean()*100: .3f} ({results.std(): .3f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b8da19",
   "metadata": {},
   "source": [
    "**Logistic Loss**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04f143f",
   "metadata": {},
   "source": [
    "Logistic loss (or logloss) is a performance metric for evaluating the predictions of probabilities of\n",
    "membership to a given class. The scalar probability between 0 and 1 can be seen as a measure\n",
    "of confidence for a prediction by an algorithm. Predictions that are correct or incorrect are\n",
    "rewarded or punished proportionally to the confidence of the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13c1e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6a6965",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['preg','plas','pres','skin','test','mass','pedi','age','class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ee2288",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = read_csv(filepath+filename, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:, 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c99601",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = KFold(n_splits=10, random_state=7, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406f756d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(solver='liblinear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9463d86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = cross_val_score(model, X, Y, cv=cv, scoring='neg_log_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373a0118",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Accuracy is: {results.mean()*100: .3f} ({results.std(): .3f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbce87ca",
   "metadata": {},
   "source": [
    "Smaller logloss is better with 0 representing a perfect logloss. As mentioned above, the\n",
    "measure is inverted to be ascending when using the cross val score() function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bd22c7",
   "metadata": {},
   "source": [
    "**Area Under ROC Curve**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815ebd36",
   "metadata": {},
   "source": [
    "Area under ROC Curve (or ROC AUC for short) is a performance metric for binary classification\n",
    "problems. The AUC represents a model’s ability to discriminate between positive and negative\n",
    "classes. An area of 1.0 represents a model that made all predictions perfectly. An area of 0.5\n",
    "represents a model that is as good as random. A ROC Curve is a plot of the true positive rate\n",
    "and the false positive rate for a given set of probability predictions at different thresholds used\n",
    "to map the probabilities to class labels. The area under the curve is then the approximate\n",
    "integral under the ROC Curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b298119",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold,cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05a19e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['preg','plas','pres','skin','test','mass','pedi','age','class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70bbfc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = read_csv(filepath+filename, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:, 0:8]\n",
    "Y = array[:, 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9eb63de",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = KFold(n_splits=10, random_state=7, shuffle=True)\n",
    "model = LogisticRegression(solver='liblinear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d534ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = cross_val_score(model, X, Y, cv=cv, scoring='roc_auc')\n",
    "print(f'AUC: {results.mean(): .3f} ({results.std(): 0.3f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6caa04bc",
   "metadata": {},
   "source": [
    "You can see the AUC is relatively close to 1 and greater than 0.5, suggesting some skill in\n",
    "the predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37fb523",
   "metadata": {},
   "source": [
    "**Confusion Matrix**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d9b050",
   "metadata": {},
   "source": [
    "The confusion matrix is a handy presentation of the accuracy of a model with two or more\n",
    "classes. The table presents predictions on the x-axis and true outcomes on the y-axis. The\n",
    "cells of the table are the number of predictions made by a machine learning algorithm. For\n",
    "example, a machine learning algorithm can predict 0 or 1 and each prediction may actually have\n",
    "been a 0 or 1. Predictions for 0 that were actually 0 appear in the cell for prediction = 0 and\n",
    "actual = 0, whereas predictions for 0 that were actually 1 appear in the cell for prediction = 0\n",
    "and actual = 1. And so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14db2e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9a8045",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['preg','plas','pres','skin','test','mass','pedi','age','class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f828ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = read_csv(filepath+filename, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:, 0:8]\n",
    "Y = array[:,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6f5241",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.33, random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5237b059",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(solver='liblinear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8796d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ce699e",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_Y = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41992d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = confusion_matrix(Y_test, predicted_Y)\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151624dd",
   "metadata": {},
   "source": [
    "Although the array is printed without headings, you can see that the majority of the\n",
    "predictions fall on the diagonal line of the matrix (which are correct predictions)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa69bb2",
   "metadata": {},
   "source": [
    "**Classification Report**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988c491a",
   "metadata": {},
   "source": [
    "The scikit-learn library provides a convenience report when working on classification problems to give you a quick idea of the accuracy of a model using a number of measures. The classification report() function displays the precision, recall, F1-score and support for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9719941b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a949d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['preg','plas','pres','skin','test','mass','pedi','age','class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac4c2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = read_csv(filepath+filename, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65f4779",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(solver='liblinear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6f28e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.33, random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538ad852",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7aff41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_Y = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa6605b",
   "metadata": {},
   "outputs": [],
   "source": [
    "report = classification_report(Y_test, predicted_Y)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5021f4",
   "metadata": {},
   "source": [
    "#### Regression Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624372bb",
   "metadata": {},
   "source": [
    "In this section will review 3 of the most common metrics for evaluating predictions on regression machine learning problems:\n",
    "\n",
    "* Mean Absolute Error.\n",
    "* Mean Squared Error.\n",
    "* R 2 ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b4b795",
   "metadata": {},
   "source": [
    "**Mean Absolute Error**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16431e9b",
   "metadata": {},
   "source": [
    "The Mean Absolute Error (or MAE) is the average of the absolute differences between predictions\n",
    "and actual values. It gives an idea of how wrong the predictions were. The measure gives an\n",
    "idea of the magnitude of the error, but no idea of the direction (e.g. over or under predicting)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88223df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import KFold, cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b1fa8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['CRIM','ZN','INDUS','CHAS','NOX','RM','AGE','DIS','RAD','TAX','PTRATIO',\n",
    "'B','LSTAT','MEDV']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22af05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = read_csv(filepath+housing, delim_whitespace=True, names=names)\n",
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d5a099",
   "metadata": {},
   "outputs": [],
   "source": [
    "array = dataframe.values\n",
    "X = array[:, 0:13]\n",
    "Y = array[:, 13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f6dd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = KFold(n_splits=10, random_state=7, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732b34dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833bf456",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = cross_val_score(model, X, Y, cv=cv, scoring='neg_mean_absolute_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe860970",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'MAE: {results.mean(): 0.3f} ({results.std(): .3f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652f81e1",
   "metadata": {},
   "source": [
    "#### Mean Squared Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbfe003",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827520d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['CRIM','ZN','INDUS','CHAS','NOX','RM','AGE','DIS','RAD','TAX','PTRATIO',\n",
    "'B','LSTAT','MEDV']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d234c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = read_csv(filepath+housing, delim_whitespace=True,names=names)\n",
    "array = dataframe.values\n",
    "X = array[:, 0:13]\n",
    "Y = array[:, 13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41daf88",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7a4ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = KFold(n_splits=10, random_state=7, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f8447e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = cross_val_score(model, X, Y, cv=cv, scoring='neg_mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f755549",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'MSE: {results.mean(): 0.3f} ({results.std(): 0.3f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2561f20b",
   "metadata": {},
   "source": [
    "This metric too is inverted so that the results are increasing. Remember to take the absolute\n",
    "value before taking the square root if you are interested in calculating the RMSE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70da2e83",
   "metadata": {},
   "source": [
    "#### R2 Metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfa7c1b",
   "metadata": {},
   "source": [
    "The R 2 (or R Squared) metric provides an indication of the goodness of fit of a set of predictions\n",
    "to the actual values. In statistical literature this measure is called the coefficient of determination.\n",
    "This is a value between 0 and 1 for no-fit and perfect fit respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a11f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fd47bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['CRIM','ZN','INDUS','CHAS','NOX','RM','AGE','DIS','RAD','TAX','PTRATIO',\n",
    "'B','LSTAT','MEDV']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9741f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = read_csv(filepath+housing, delim_whitespace=True, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:, 0:13]\n",
    "Y = array[:, 13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519fbbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = KFold(n_splits=10, shuffle=True, random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3709706",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753d5ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = cross_val_score(model, X, Y, cv=cv, scoring='r2') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0aafdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'R Squared: {results.mean():0.3f} ({results.std(): .3f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d947a8",
   "metadata": {},
   "source": [
    "### Spot-Check Classification Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78a5395",
   "metadata": {},
   "source": [
    "Spot-checking is a way of discovering which algorithms perform well on your machine learning problem. You cannot know which algorithms are best suited to your problem beforehand. You\n",
    "must trial a number of methods and focus attention on those that prove themselves the most promising. In this chapter you will discover six machine learning algorithms that you can use when spot-checking your classification problem in Python with scikit-learn. After completing this lesson you will know:\n",
    "1. How to spot-check machine learning algorithms on a classification problem.\n",
    "2. How to spot-check two linear classification algorithms.\n",
    "3. How to spot-check four nonlinear classification algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0ace5b",
   "metadata": {},
   "source": [
    "**Algorithm Spot-Checking**\n",
    "\n",
    "You cannot know which algorithm will work best on your dataset beforehand. You must use\n",
    "trial and error to discover a shortlist of algorithms that do well on your problem that you can then double down on and tune further. I call this process spot-checking.\n",
    "The question is not: What algorithm should I use on my dataset? Instead it is: What\n",
    "algorithms should I spot-check on my dataset? You can guess at what algorithms might do\n",
    "well on your dataset, and this can be a good starting point. I recommend trying a mixture of algorithms and see what is good at picking out the structure in your data. Below are some suggestions when spot-checking algorithms on your dataset:\n",
    "\n",
    "* Try a mixture of algorithm representations (e.g. instances and trees).\n",
    "* Try a mixture of learning algorithms (e.g. different algorithms for learning the same type of representation).\n",
    "* Try a mixture of modeling types (e.g. linear and nonlinear functions or parametric and nonparametric).\n",
    "\n",
    "Let’s get specific. In the next section, we will look at algorithms that you can use to\n",
    "spot-check on your next classification machine learning project in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88a6dc1",
   "metadata": {},
   "source": [
    "**Algorithms Overview**\n",
    "\n",
    "We are going to take a look at six classification algorithms that you can spot-check on your dataset. Starting with two linear machine learning algorithms:\n",
    "\n",
    "* Logistic Regression.\n",
    "* Linear Discriminant Analysis.\n",
    "\n",
    "Then looking at four nonlinear machine learning algorithms:\n",
    "* k-Nearest Neighbors.\n",
    "* Naive Bayes.\n",
    "* Classification and Regression Trees.\n",
    "* Support Vector Machines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d885aa",
   "metadata": {},
   "source": [
    "Each recipe is demonstrated on the Pima Indians onset of Diabetes dataset. A test harness using 10-fold cross-validation is used to demonstrate how to spot-check each machine learning algorithm and mean accuracy measures are used to indicate algorithm performance. The recipes assume that you know about each machine learning algorithm and how to use them. We will not go into the API or parameterization of each algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6f43d3",
   "metadata": {},
   "source": [
    "#### Linear Machine Learning Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f728d911",
   "metadata": {},
   "source": [
    "**Logistic Regression**\n",
    "\n",
    "Logistic regression assumes a Gaussian distribution for the numeric input variables and can model binary classification problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f853859",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9cd35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['preg','plas','pres','skin','test','mass','pedi','age','class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88ddce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = read_csv(filepath+pima, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:, 0:8]\n",
    "Y = array[:, 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad2a76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "kfodl = KFold(n_splits=10, random_state=7, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583e0fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(solver='liblinear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fbaeee",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = cross_val_score(model, X, Y, cv=kfodl)\n",
    "print(f'{results.mean(): .3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53345f6",
   "metadata": {},
   "source": [
    "**Linear Discriminant Analysis**\n",
    "\n",
    "Linear Discriminant Analysis or LDA is a statistical technique for binary and multiclass classification. It too assumes a Gaussian distribution for the numerical input variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303beda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3853ff58",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['preg','plas','pres','skin','test','mass','pedi','age','class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172f237f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = read_csv(filepath+pima, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:, 0:8]\n",
    "Y = array[:,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee05813",
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=10, random_state=7, shuffle=True)\n",
    "model = LinearDiscriminantAnalysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fda36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = cross_val_score(model , X ,Y, cv=kfold)\n",
    "print(f'{results.mean()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7f01aa",
   "metadata": {},
   "source": [
    "#### Nonlinear Machine Learning Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112f7abd",
   "metadata": {},
   "source": [
    "**k-Nearest Neighbors**\n",
    "\n",
    "The k-Nearest Neighbors algorithm (or KNN) uses a distance metric to find the k most similar instances in the training data for a new instance and takes the mean outcome of the neighbors as the prediction. You can construct a KNN model using the KNeighborsClassifier class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fd0b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43c032a",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['preg','plas','pres','skin','test','mass','pedi','age','class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf4bdc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = read_csv(filepath+pima, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8209994c",
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=10, random_state=7, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d9c239",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KNeighborsClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b9ea1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(f'{results.mean()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c33c26",
   "metadata": {},
   "source": [
    "**Naive Bayes**\n",
    "\n",
    "Naive Bayes calculates the probability of each class and the conditional probability of each class given each input value. These probabilities are estimated for new data and multiplied together,\n",
    "assuming that they are all independent (a simple or naive assumption). When working with real-valued data, a Gaussian distribution is assumed to easily estimate the probabilities for\n",
    "input variables using the Gaussian Probability Density Function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df29c7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c07556c",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['preg','plas','pres','skin','test','mass','pedi','age','class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe5c42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = read_csv(filepath+pima, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:, 0:8]\n",
    "Y = array[:,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564772d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "kfod = KFold(n_splits=10, random_state=7, shuffle=True)\n",
    "model = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0cee4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = cross_val_score(model, X, Y, cv=kfod)\n",
    "print(f'{results.mean()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393277c4",
   "metadata": {},
   "source": [
    "**Classification and Regression Trees**\n",
    "\n",
    "Classification and Regression Trees (CART or just decision trees) construct a binary tree from the training data. Split points are chosen greedily by evaluating each attribute and each value of each attribute in the training data in order to minimize a cost function (like the Gini index)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb90047",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021a5512",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['preg','plas','pres','skin','test','mass','pedi','age','class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062bfd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = read_csv(filepath+pima, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:, 0:8]\n",
    "Y = array[:, 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbf5174",
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=10, random_state=7, shuffle=True)\n",
    "model = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4881bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = cross_val_score(model, X, Y, cv=kfod)\n",
    "print(f'{results.mean(): .3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3eff5e",
   "metadata": {},
   "source": [
    "**Support Vector Machines**\n",
    "\n",
    "Support Vector Machines (or SVM) seek a line that best separates two classes. Those data instances that are closest to the line that best separates the classes are called support vectors and influence where the line is placed. SVM has been extended to support multiple classes. Of particular importance is the use of different kernel functions via the kernel parameter. A powerful Radial Basis Function is used by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ab68a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11f5d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['preg','plas','pres','skin','test','mass','pedi','age','class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ad6df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = read_csv(filepath+pima, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:, 0:8]\n",
    "Y = array[:, 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3823ce82",
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=10, random_state=7, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48cb4774",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf861ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = cross_val_score(model, X, Y, cv=kfod)\n",
    "print(f'{results.mean()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd141f13",
   "metadata": {},
   "source": [
    "### Spot-Check Regression Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db02054",
   "metadata": {},
   "source": [
    "**Algorithms Overview**\n",
    "\n",
    "In this lesson we are going to take a look at seven regression algorithms that you can spot-check on your dataset. \n",
    "\n",
    "Starting with four linear machine learning algorithms:\n",
    "* Linear Regression.\n",
    "* Ridge Regression.\n",
    "* LASSO Linear Regression.\n",
    "* Elastic Net Regression.\n",
    "\n",
    "Then looking at three nonlinear machine learning algorithms:\n",
    "* k-Nearest Neighbors.\n",
    "* Classification and Regression Trees.\n",
    "* Support Vector Machines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4573058f",
   "metadata": {},
   "source": [
    "#### Linear Machine Learning Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e019ac77",
   "metadata": {},
   "source": [
    "**Linear Regression**\n",
    "\n",
    "Linear regression assumes that the input variables have a Gaussian distribution. It is also assumed that input variables are relevant to the output variable and that they are not highly correlated with each other (a problem called collinearity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f8a02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aaf7a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['CRIM','ZN','INDUS','CHAS','NOX','RM','AGE','DIS','RAD','TAX','PTRATIO','B','LSTAT','MEDV']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4733613c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = read_csv(filepath+housing, names=names, delim_whitespace=True)\n",
    "array = dataframe.values\n",
    "X = array[:, 0:13]\n",
    "Y = array[:, 13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd9404e",
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=10, random_state=7, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d26ece9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d3dd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = cross_val_score(model, X, Y, cv=kfold, scoring='neg_mean_squared_error')\n",
    "print(f'{results.mean()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0361b45",
   "metadata": {},
   "source": [
    "**Ridge Regression**\n",
    "\n",
    "Ridge regression is an extension of linear regression where the loss function is modified to\n",
    "minimize the complexity of the model measured as the sum squared value of the coefficient\n",
    "values (also called the L2-norm)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b9c4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aef7a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['CRIM','ZN','INDUS','CHAS','NOX','RM','AGE','DIS','RAD','TAX','PTRATIO','B','LSTAT','MEDV']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd17f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = read_csv(filepath+housing, names=names, delim_whitespace=True)\n",
    "array = dataframe.values\n",
    "X = array[:, 0:13]\n",
    "Y = array[:,13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c715e4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=10, random_state=7, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9e9a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Ridge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660e670c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = cross_val_score(model, X, Y, cv=kfold, scoring='neg_mean_squared_error')\n",
    "print(f'{results.mean()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07029ac2",
   "metadata": {},
   "source": [
    "**LASSO Regression** \n",
    "\n",
    "The Least Absolute Shrinkage and Selection Operator (or LASSO for short) is a modification of linear regression, like ridge regression, where the loss function is modified to minimize the complexity of the model measured as the sum absolute value of the coefficient values (also called the L1-norm)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6d3dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.linear_model import Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d31cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['CRIM','ZN','INDUS','CHAS','NOX','RM','AGE','DIS','RAD','TAX','PTRATIO','B','LSTAT','MEDV']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e78521",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = read_csv(filepath+housing, delim_whitespace=True, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:, 0:13]\n",
    "Y = array[:, 13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751e6ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=10, random_state=7, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484f3e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Lasso()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe4431f",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = cross_val_score(model, X, Y, cv=kfold, scoring='neg_mean_squared_error')\n",
    "print(f'{results.mean()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda47ddb",
   "metadata": {},
   "source": [
    "**ElasticNet Regression**\n",
    "\n",
    "ElasticNet is a form of regularization regression that combines the properties of both Ridge Regression and LASSO regression. It seeks to minimize the complexity of the regression model (magnitude and number of regression coefficients) by penalizing the model using both the L2-norm (sum squared coefficient values) and the L1-norm (sum absolute coefficient values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028aa9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.linear_model import ElasticNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab9941f",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['CRIM','ZN','INDUS','CHAS','NOX','RM','AGE','DIS','RAD','TAX','PTRATIO','B','LSTAT','MEDV']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437bfd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = read_csv(filepath+housing, names=names, delim_whitespace=True)\n",
    "array = dataframe.values\n",
    "X = array[:, 0:13]\n",
    "Y = array[:, 13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3517c8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=10, random_state=7, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89e9661",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ElasticNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e0710d",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = cross_val_score(model, X, Y, cv=kfold, scoring='neg_mean_squared_error')\n",
    "print(f'{results.mean()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e393b040",
   "metadata": {},
   "source": [
    "#### Nonlinear Machine Learning Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b276a8cb",
   "metadata": {},
   "source": [
    "**K-Nearest Neighbors**\n",
    "\n",
    "The k-Nearest Neighbors algorithm (or KNN) locates the k most similar instances in the\n",
    "training dataset for a new data instance. From the k neighbors, the mean or median output\n",
    "variable is taken as the prediction. Of note is the distance metric used (the metric argument).\n",
    "The Minkowski distance is used by default, which is a generalization of both the Euclidean\n",
    "distance (used when all inputs have the same scale) and Manhattan distance (used when the\n",
    "scales of the input variables differ)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f434496",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.neighbors import KNeighborsRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545df08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['CRIM','ZN','INDUS','CHAS','NOX','RM','AGE','DIS','RAD','TAX','PTRATIO','B','LSTAT','MEDV']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc06c51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = read_csv(filepath+housing, names=names, delim_whitespace=True)\n",
    "array = dataframe.values\n",
    "X = array[:, 0:13]\n",
    "Y = array[: ,13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3f91a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=10, random_state=7, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1388e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KNeighborsRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8d6ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = cross_val_score(model, X, Y, cv=kfold, scoring='neg_mean_squared_error')\n",
    "print(f'{results.mean()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1225d3",
   "metadata": {},
   "source": [
    "**Classification and Regression Trees**\n",
    "\n",
    "Decision trees or the Classification and Regression Trees (CART as they are known) use the train-\n",
    "ing data to select the best points to split the data in order to minimize a cost metric. The default\n",
    "cost metric for regression decision trees is the mean squared error, specified in the criterion\n",
    "parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8c6324",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce2b63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['CRIM','ZN','INDUS','CHAS','NOX','RM','AGE','DIS','RAD','TAX','PTRATIO','B','LSTAT','MEDV']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16faebca",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = read_csv(filepath+housing, names=names, delim_whitespace=True)\n",
    "array = dataframe.values\n",
    "X = array[:, 0:13]\n",
    "Y = array[:, 13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889d619e",
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=10, shuffle=True, random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54cf87f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5b1dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = cross_val_score(model, X, Y, cv=kfold, scoring='neg_mean_squared_error')\n",
    "print(f'{results.mean()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d743d8",
   "metadata": {},
   "source": [
    "**Support Vector Machines**\n",
    "\n",
    "Support Vector Machines (SVM) were developed for binary classification. The technique has been extended for the prediction real-valued problems called Support Vector Regression (SVR). Like the classification example, SVR is built upon the LIBSVM library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4829c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.svm import SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac58545a",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['CRIM','ZN','INDUS','CHAS','NOX','RM','AGE','DIS','RAD','TAX','PTRATIO','B','LSTAT','MEDV']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30859d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = read_csv(filepath+housing, names=names, delim_whitespace=True)\n",
    "array = dataframe.values\n",
    "X = array[:,0:13]\n",
    "Y = array[:, 13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef008944",
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=10, random_state=7, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415c88bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVR(gamma='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4170f153",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = cross_val_score(model, X, Y, cv=kfold, scoring='neg_mean_squared_error')\n",
    "print(f'{results.mean()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1527346",
   "metadata": {},
   "source": [
    "### Compare Machine Learning Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575d2610",
   "metadata": {},
   "source": [
    "It is important to compare the performance of multiple different machine learning algorithms consistently. In this chapter you will discover how you can create a test harness to compare multiple different machine learning algorithms in Python with scikit-learn. You can use this test harness as a template on your own machine learning problems and add more and different algorithms to compare. After completing this lesson you will know:\n",
    "\n",
    "1. How to formulate an experiment to directly compare machine learning algorithms.\n",
    "2. A reusable template for evaluating the performance of multiple algorithms on one dataset.\n",
    "3. How to report and visualize the results when comparing algorithm performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed08968",
   "metadata": {},
   "source": [
    "**Choose The Best Machine Learning Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c85b66",
   "metadata": {},
   "source": [
    "When you work on a machine learning project, you often end up with multiple good models\n",
    "to choose from. Each model will have different performance characteristics. Using resampling\n",
    "methods like cross-validation, you can get an estimate for how accurate each model may be on\n",
    "unseen data. You need to be able to use these estimates to choose one or two best models from\n",
    "the suite of models that you have created."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63cc906",
   "metadata": {},
   "source": [
    "When you have a new dataset, it is a good idea to visualize the data using different techniques\n",
    "in order to look at the data from different perspectives. The same idea applies to model selection.\n",
    "You should use a number of different ways of looking at the estimated accuracy of your machine\n",
    "learning algorithms in order to choose the one or two algorithms to finalize. A way to do this is\n",
    "to use visualization methods to show the average accuracy, variance and other properties of the\n",
    "distribution of model accuracies. In the next section you will discover exactly how you can do\n",
    "that in Python with scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1235d5",
   "metadata": {},
   "source": [
    "**Compare Machine Learning Algorithms Consistently**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474c2644",
   "metadata": {},
   "source": [
    "The key to a fair comparison of machine learning algorithms is ensuring that each algorithm is\n",
    "evaluated in the same way on the same data. You can achieve this by forcing each algorithm to be evaluated on a consistent test harness. In the example below six different classification\n",
    "algorithms are compared on a single dataset:\n",
    "* Logistic Regression.\n",
    "* Linear Discriminant Analysis.\n",
    "* k-Nearest Neighbors.\n",
    "* Classification and Regression Trees.\n",
    "* Naive Bayes.\n",
    "* Support Vector Machines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e19aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01ef8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['preg','plas','pres','skin','test','mass','pedi','age','class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4890195f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = read_csv(filepath+pima, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:, 0:8]\n",
    "Y = array[:,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3677c440",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea992670",
   "metadata": {},
   "outputs": [],
   "source": [
    "models.append(('LR', LogisticRegression(solver='liblinear')))\n",
    "models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('CART', DecisionTreeClassifier()))\n",
    "models.append(('NB', GaussianNB()))\n",
    "models.append(('SVM', SVC()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2539d1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "names = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad67925",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, model in models:\n",
    "    kfold = KFold(n_splits=10, random_state=7, shuffle=True)\n",
    "    cv_results = cross_val_score(model, X, Y, cv=kfold, scoring='accuracy')\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    print(f'{name}: {cv_results.mean():.3f} ({cv_results.std():.3f})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b324d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = pyplot.figure()\n",
    "fig.suptitle('Algorithm Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "pyplot.boxplot(results)\n",
    "ax.set_xticklabels(names)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4e64ca",
   "metadata": {},
   "source": [
    "The example also provides a box and whisker plot showing the spread of the accuracy scores\n",
    "across each cross-validation fold for each algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111dfeb0",
   "metadata": {},
   "source": [
    "In this lesson you learned how to compare the performance of machine learning algorithms to\n",
    "each other. But what if you need to prepare your data as part of the comparison process? In\n",
    "the next lesson you will discover Pipelines in scikit-learn and how they overcome the common\n",
    "problems of data leakage when comparing machine learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2f1fc0",
   "metadata": {},
   "source": [
    "### Feature Selection For Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff11595",
   "metadata": {},
   "source": [
    "The data features that you use to train your machine learning models have a huge influence on the performance you can achieve. Irrelevant or partially relevant features can negatively impact model performance. In this chapter you will discover automatic feature selection techniques that you can use to prepare your machine learning data in Python with scikit-learn. \n",
    "\n",
    "After completing this lesson you will know how to use:\n",
    "\n",
    "1. Univariate Selection.\n",
    "2. Recursive Feature Elimination.\n",
    "3. Principle Component Analysis.\n",
    "4. Feature Importance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc33f6b",
   "metadata": {},
   "source": [
    "**Univariate Selection**\n",
    "\n",
    "Statistical tests can be used to select those features that have the strongest relationship with\n",
    "the output variable. The scikit-learn library provides the SelectKBest class 2 that can be used\n",
    "with a suite of different statistical tests to select a specific number of features. Many different\n",
    "statistical tests can be used with this selection method. For example the ANOVA F-value\n",
    "method is appropriate for numerical inputs and categorical data, as we see in the Pima dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8739c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from numpy import set_printoptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d1f353",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = load_pima_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ee6c74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.000e+00, 1.480e+02, 7.200e+01, 3.500e+01, 0.000e+00, 3.360e+01,\n",
       "        6.270e-01, 5.000e+01],\n",
       "       [1.000e+00, 8.500e+01, 6.600e+01, 2.900e+01, 0.000e+00, 2.660e+01,\n",
       "        3.510e-01, 3.100e+01],\n",
       "       [8.000e+00, 1.830e+02, 6.400e+01, 0.000e+00, 0.000e+00, 2.330e+01,\n",
       "        6.720e-01, 3.200e+01],\n",
       "       [1.000e+00, 8.900e+01, 6.600e+01, 2.300e+01, 9.400e+01, 2.810e+01,\n",
       "        1.670e-01, 2.100e+01],\n",
       "       [0.000e+00, 1.370e+02, 4.000e+01, 3.500e+01, 1.680e+02, 4.310e+01,\n",
       "        2.288e+00, 3.300e+01]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0:5, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43e5342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature extraction\n",
    "test = SelectKBest(score_func=f_classif, k=4)\n",
    "fit = test.fit(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93d8c03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 39.67 , 213.162,   3.257,   4.304,  13.281,  71.772,  23.871,\n",
       "        46.141])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# summarize scores\n",
    "set_printoptions(precision=3)\n",
    "fit.scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082089c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = fit.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3291cd22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  6. , 148. ,  33.6,  50. ],\n",
       "       [  1. ,  85. ,  26.6,  31. ],\n",
       "       [  8. , 183. ,  23.3,  32. ],\n",
       "       [  1. ,  89. ,  28.1,  21. ],\n",
       "       [  0. , 137. ,  43.1,  33. ]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[0:5, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e366d1",
   "metadata": {},
   "source": [
    "You can see the scores for each attribute and the 4 attributes chosen (those with the highest\n",
    "scores). Specifically features with indexes 0 (preq), 1 (plas), 5 (mass), and 7 (age)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76831b32",
   "metadata": {},
   "source": [
    "**Recursive Feature Elimination**\n",
    "\n",
    "The Recursive Feature Elimination (or RFE) works by recursively removing attributes and\n",
    "building a model on those attributes that remain. It uses the model accuracy to identify which attributes (and combination of attributes) contribute the most to predicting the target attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eef4a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af16f7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = load_pima_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3387b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(solver='liblinear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f9df9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfe = RFE(model, n_features_to_select=3)\n",
    "fit = rfe.fit(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac243b92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Features: 3\n"
     ]
    }
   ],
   "source": [
    "print(f'Num Features: {fit.n_features_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d85431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Features: [ True False False False False  True  True False]\n"
     ]
    }
   ],
   "source": [
    "print(f'Selected Features: {fit.support_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7672286e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Ranking: [1 2 3 5 6 1 1 4]\n"
     ]
    }
   ],
   "source": [
    "print(f'Feature Ranking: {fit.ranking_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26bf9d9d",
   "metadata": {},
   "source": [
    "**Principal Component Analysis**\n",
    "\n",
    "Principal Component Analysis (or PCA) uses linear algebra to transform the dataset into a\n",
    "compressed form. Generally this is called a data reduction technique. A property of PCA is that\n",
    "you can choose the number of dimensions or principal components in the transformed result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb2b52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e3dd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = load_pima_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0a453c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee9035e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit = pca.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a8f1c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained Variance: [0.889 0.062 0.026]\n"
     ]
    }
   ],
   "source": [
    "print(f'Explained Variance: {fit.explained_variance_ratio_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a75d48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2.022e-03  9.781e-02  1.609e-02  6.076e-02  9.931e-01  1.401e-02\n",
      "   5.372e-04 -3.565e-03]\n",
      " [-2.265e-02 -9.722e-01 -1.419e-01  5.786e-02  9.463e-02 -4.697e-02\n",
      "  -8.168e-04 -1.402e-01]\n",
      " [-2.246e-02  1.434e-01 -9.225e-01 -3.070e-01  2.098e-02 -1.324e-01\n",
      "  -6.400e-04 -1.255e-01]]\n"
     ]
    }
   ],
   "source": [
    "print(fit.components_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda7143f",
   "metadata": {},
   "source": [
    "**Feature Importance**\n",
    "\n",
    "Bagged decision trees like Random Forest and Extra Trees can be used to estimate the importance\n",
    "of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795fb10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa6113f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X,Y = load_pima_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5db3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ExtraTreesClassifier(n_estimators=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0a6996",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>ExtraTreesClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">ExtraTreesClassifier</label><div class=\"sk-toggleable__content\"><pre>ExtraTreesClassifier()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "ExtraTreesClassifier()"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2de7593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.112 0.237 0.098 0.081 0.075 0.139 0.12  0.139]\n"
     ]
    }
   ],
   "source": [
    "print(model.feature_importances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c81e8a",
   "metadata": {},
   "source": [
    "### Automate Machine Learning Workflows with Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a6fb7d",
   "metadata": {},
   "source": [
    "There are standard workflows in a machine learning project that can be automated. In Python\n",
    "scikit-learn, Pipelines help to clearly define and automate these workflows. In this chapter you\n",
    "will discover Pipelines in scikit-learn and how you can automate common machine learningworkflows. After completing this lesson you will know:\n",
    "1. How to use pipelines to minimize data leakage.\n",
    "2. How to construct a data preparation and modeling pipeline.\n",
    "3. How to construct a feature extraction and modeling pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1500d9",
   "metadata": {},
   "source": [
    "**Automating Machine Learning Workflows**\n",
    "\n",
    "There are standard workflows in applied machine learning. Standard because they overcome\n",
    "common problems like data leakage in your test harness. Python scikit-learn provides a Pipeline\n",
    "utility to help automate machine learning workflows. Pipelines work by allowing for a linear\n",
    "sequence of data transforms to be chained together culminating in a modeling process that can\n",
    "be evaluated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc06d6b",
   "metadata": {},
   "source": [
    "The goal is to ensure that all of the steps in the pipeline are constrained to the data available\n",
    "for the evaluation, such as the training dataset or each fold of the cross-validation procedure.\n",
    "You can learn more about Pipelines in scikit-learn by reading the Pipeline section 1 of the user\n",
    "guide. You can also review the API documentation for the Pipeline and FeatureUnion classes\n",
    "and the pipeline module."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3741e01",
   "metadata": {},
   "source": [
    "#### Data Preparation and Modeling Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f680bb6",
   "metadata": {},
   "source": [
    "An easy trap to fall into in applied machine learning is leaking data from your training dataset\n",
    "to your test dataset. To avoid this trap you need a robust test harness with strong separation of training and testing. This includes data preparation. Data preparation is one easy way to leak\n",
    "knowledge of the whole training dataset to the algorithm. For example, preparing your data\n",
    "using normalization or standardization on the entire training dataset before learning would not\n",
    "be a valid test because the training dataset would have been influenced by the scale of the data\n",
    "in the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2988f29b",
   "metadata": {},
   "source": [
    "Pipelines help you prevent data leakage in your test harness by ensuring that data preparation\n",
    "like standardization is constrained to each fold of your cross-validation procedure. The example\n",
    "below demonstrates this important data preparation and model evaluation workflow on the\n",
    "Pima Indians onset of diabetes dataset. The pipeline is defined with two steps:\n",
    "1. Standardize the data.\n",
    "2. Learn a Linear Discriminant Analysis model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b7a98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline that standardizes the data then creates a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b327a2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c2f8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = read_csv(filepath+pima, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:, 0:8]\n",
    "Y = array[:, 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ba0bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=10, random_state=7, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2c9fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pipeline\n",
    "pipline_items = []\n",
    "pipline_items.append(('standarize', StandardScaler()))\n",
    "pipline_items.append(('lda', LinearDiscriminantAnalysis()))\n",
    "pipeline = Pipeline(pipline_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd38398",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = cross_val_score(pipeline, X, Y, cv=kfold)\n",
    "print(f'{results.mean()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70209d7",
   "metadata": {},
   "source": [
    "Notice how we create a Python list of steps that are provided to the Pipeline for processing\n",
    "the data. Also notice how the Pipeline itself is treated like an estimator and is evaluated in its\n",
    "entirety by the k-fold cross-validation procedure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0b3c2c",
   "metadata": {},
   "source": [
    "#### Feature Extraction and Modeling Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0196ce87",
   "metadata": {},
   "source": [
    "Feature extraction is another procedure that is susceptible to data leakage. Like data preparation,\n",
    "feature extraction procedures must be restricted to the data in your training dataset. The\n",
    "pipeline provides a handy tool called the FeatureUnion which allows the results of multiple\n",
    "feature selection and extraction procedures to be combined into a larger dataset on which a\n",
    "model can be trained. Importantly, all the feature extraction and the feature union occurs\n",
    "within each fold of the cross-validation procedure. The example below demonstrates the pipeline\n",
    "defined with four steps:\n",
    "\n",
    "1. Feature Extraction with Principal Component Analysis (3 features).\n",
    "2. Feature Extraction with Statistical Selection (6 features).\n",
    "3. Feature Union.\n",
    "4. Learn a Logistic Regression Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd649525",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abd3cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "datafarem = read_csv(filepath+pima, names=pima_names)\n",
    "array = datafarem.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:, 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4838c8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create feature union\n",
    "features = []\n",
    "features.append(('pca', PCA(n_components=3)))\n",
    "features.append(('select_best', SelectKBest(k=6)))\n",
    "feature_union = FeatureUnion(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1463d1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pipeline\n",
    "pipeline_items = []\n",
    "pipeline_items.append(('feature_union', feature_union)) \n",
    "pipeline_items.append(('logistic', LogisticRegression(solver='liblinear')))\n",
    "pipeline = Pipeline(pipeline_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcbdcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate pipeline\n",
    "kfold = KFold(n_splits=10, random_state=seed, shuffle=True)\n",
    "results = cross_val_score(pipeline, X, Y, cv=kfold)\n",
    "print(f'{results.mean()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7165c3a",
   "metadata": {},
   "source": [
    "Notice how the FeatureUnion is its own Pipeline that in turn is a single step in the final\n",
    "Pipeline used to feed Logistic Regression. This might get you thinking about how you can start\n",
    "embedding pipelines within pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295215c8",
   "metadata": {},
   "source": [
    "In this chapter you discovered the difficulties of data leakage in applied machine learning. You\n",
    "discovered the Pipeline utilities in Python scikit-learn and how they can be used to automate\n",
    "standard applied machine learning workflows. You learned how to use Pipelines in two important\n",
    "use cases:\n",
    "\n",
    "* Data preparation and modeling constrained to each fold of the cross-validation procedure.\n",
    "* Feature extraction and feature union constrained to each fold of the cross-validation\n",
    "procedure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0765b94",
   "metadata": {},
   "source": [
    "## Improve Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f1198f",
   "metadata": {},
   "source": [
    "### Improve Performance with Ensembles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041146d0",
   "metadata": {},
   "source": [
    "Ensembles can give you a boost in accuracy on your dataset. In this chapter you will discover\n",
    "how you can create some of the most powerful types of ensembles in Python using scikit-learn.\n",
    "This lesson will step you through Boosting, Bagging and Majority Voting and show you how you\n",
    "can continue to ratchet up the accuracy of the models on your own datasets. After completing\n",
    "this lesson you will know:\n",
    "\n",
    "1. How to use bagging ensemble methods such as bagged decision trees, random forest and\n",
    "extra trees.\n",
    "2. How to use boosting ensemble methods such as AdaBoost and stochastic gradient boosting.\n",
    "3. How to use voting ensemble methods to combine the predictions from multiple algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0084ef",
   "metadata": {},
   "source": [
    "The three most popular methods for combining the predictions from different models are:\n",
    "\n",
    "* Bagging. Building multiple models (typically of the same type) from different subsamples\n",
    "of the training dataset.\n",
    "* Boosting. Building multiple models (typically of the same type) each of which learns to\n",
    "fix the prediction errors of a prior model in the sequence of models.\n",
    "* Voting. Building multiple models (typically of differing types) and simple statistics (like\n",
    "calculating the mean) are used to combine predictions.\n",
    "This assumes you are generally familiar with machine learning algorithms and ensemble\n",
    "methods and will not go into the details of how the algorithms work or their parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e5f600",
   "metadata": {},
   "source": [
    "#### Bagging Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5101b56",
   "metadata": {},
   "source": [
    "Bootstrap Aggregation (or Bagging) involves taking multiple samples from your training dataset\n",
    "(with replacement) and training a model for each sample. The final output prediction is averaged\n",
    "across the predictions of all of the sub-models. The three bagging models covered in this section\n",
    "are as follows:\n",
    "* Bagged Decision Trees.\n",
    "* Random Forest.\n",
    "* Extra Trees."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026a6ecb",
   "metadata": {},
   "source": [
    "**Bagged Decision Trees**\n",
    "\n",
    "Bagging performs best with algorithms that have high variance. A popular example are\n",
    "decision trees, often constructed without pruning. In the example below is an example\n",
    "of using the BaggingClassifier with the Classification and Regression Trees algorithm\n",
    "(DecisionTreeClassifier)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af27211a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded577f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = read_csv(filepath+pima, names=pima_names)\n",
    "array = datafarem.values\n",
    "X = array[:, 0:8]\n",
    "Y = array[:, 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e146f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=10, random_state=seed, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcc8217",
   "metadata": {},
   "outputs": [],
   "source": [
    "cart = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85cb25ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BaggingClassifier(base_estimator=cart, n_estimators=100, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfcd6c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(f'{results.mean()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae0c2b9",
   "metadata": {},
   "source": [
    "**Random Forest**\n",
    "\n",
    "Random Forests is an extension of bagged decision trees. Samples of the training dataset are\n",
    "taken with replacement, but the trees are constructed in a way that reduces the correlation\n",
    "between individual classifiers. Specifically, rather than greedily choosing the best split point in\n",
    "the construction of each tree, only a random subset of features are considered for each split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9adb6a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8feea5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = load_pima_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197fefff",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_estimators=100, max_features=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344f4163",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = cross_val_score(model, X, Y, \n",
    "                          cv=KFold(n_splits=10, random_state=7, shuffle=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7fdce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7577922077922078\n"
     ]
    }
   ],
   "source": [
    "print(f'{results.mean()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44de40cf",
   "metadata": {},
   "source": [
    "**Extra Trees**\n",
    "\n",
    "Extra Trees are another modification of bagging where random trees are constructed from\n",
    "samples of the training dataset. You can construct an Extra Trees model for classification using\n",
    "the ExtraTreesClassifier class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b099e2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22c50a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = load_pima_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ddb54e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ExtraTreesClassifier(n_estimators=100, max_features=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677705c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = cross_val_score(model, X, Y, \n",
    "                         cv= KFold(n_splits=10, random_state=7, shuffle=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac841d9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7656527682843473\n"
     ]
    }
   ],
   "source": [
    "print(f'{results.mean()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5530b9ec",
   "metadata": {},
   "source": [
    "#### Boosting Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ced7a8",
   "metadata": {},
   "source": [
    "Boosting ensemble algorithms create a sequence of models that attempt to correct the mistakes\n",
    "of the models before them in the sequence. Once created, the models make predictions which\n",
    "may be weighted by their demonstrated accuracy and the results are combined to create a final\n",
    "output prediction. The two most common boosting ensemble machine learning algorithms are:\n",
    "* AdaBoost.\n",
    "* Stochastic Gradient Boosting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2228cb36",
   "metadata": {},
   "source": [
    "**AdaBoost**\n",
    "\n",
    "AdaBoost was perhaps the first successful boosting ensemble algorithm. It generally works\n",
    "by weighting instances in the dataset by how easy or difficult they are to classify, allowing\n",
    "the algorithm to pay less or more attention to them in the construction of subsequent models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09700206",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ded631",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = load_pima_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c773913",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AdaBoostClassifier(n_estimators=30, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6932ff16",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = cross_val_score(model, X, Y, \n",
    "                         cv=KFold(n_splits=10, random_state=seed, shuffle=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1258f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7552802460697198\n"
     ]
    }
   ],
   "source": [
    "print(f'{results.mean()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803b6804",
   "metadata": {},
   "source": [
    "**Stochastic Gradient Boosting**\n",
    "\n",
    "Stochastic Gradient Boosting (also called Gradient Boosting Machines) are one of the most\n",
    "sophisticated ensemble techniques. It is also a technique that is proving to be perhaps one of\n",
    "the best techniques available for improving performance via ensembles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed872707",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a90f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = load_pima_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d46900",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GradientBoostingClassifier(n_estimators=100, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bc4cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = cross_val_score(model, X, Y, \n",
    "                         cv=KFold(n_splits=10, random_state=seed, shuffle=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5fa455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7604921394395079\n"
     ]
    }
   ],
   "source": [
    "print(f'{results.mean()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a50bc0b",
   "metadata": {},
   "source": [
    "#### Voting Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b453a844",
   "metadata": {},
   "source": [
    "Voting is one of the simplest ways of combining the predictions from multiple machine learning\n",
    "algorithms. It works by first creating two or more standalone models from your training dataset.\n",
    "A Voting Classifier can then be used to wrap your models and average the predictions of the\n",
    "sub-models when asked to make predictions for new data. The predictions of the sub-models can\n",
    "be weighted, but specifying the weights for classifiers manually or even heuristically is difficult.\n",
    "More advanced methods can learn how to best weight the predictions from sub-models, but this\n",
    "is called stacking (stacked aggregation) and is currently not provided in scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a2e60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fe91c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = load_pima_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2744d5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "models.append(('logistic', LogisticRegression(solver='liblinear')))\n",
    "models.append(('cart', DecisionTreeClassifier()))\n",
    "models.append(('svm', SVC(gamma='auto')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bafe431",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble = VotingClassifier(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55ca2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = cross_val_score(ensemble, X, Y, \n",
    "                         cv=KFold(n_splits=10, random_state=seed, shuffle=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f432e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7461893369788107\n"
     ]
    }
   ],
   "source": [
    "print(f'{results.mean()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29029a17",
   "metadata": {},
   "source": [
    "### Improve Performance with Algorithm Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba31593",
   "metadata": {},
   "source": [
    "achine learning models are parameterized so that their behavior can be tuned for a given\n",
    "problem. Models can have many parameters and finding the best combination of parameters can\n",
    "be treated as a search problem. In this chapter you will discover how to tune the parameters of\n",
    "machine learning algorithms in Python using the scikit-learn. After completing this lesson you\n",
    "will know:\n",
    "1. The importance of algorithm parameter tuning to improve algorithm performance.\n",
    "2. How to use a grid search algorithm tuning strategy.\n",
    "3. How to use a random search algorithm tuning strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d433835",
   "metadata": {},
   "source": [
    "**Machine Learning Algorithm Parameters**\n",
    "\n",
    "Algorithm tuning is a final step in the process of applied machine learning before finalizing your\n",
    "model. It is sometimes called hyperparameter optimization where the algorithm parameters\n",
    "are referred to as hyperparameters, whereas the coefficients found by the machine learning\n",
    "algorithm itself are referred to as parameters. Optimization suggests the search-nature of the\n",
    "problem. Phrased as a search problem, you can use different search strategies to find a good and\n",
    "robust parameter or set of parameters for an algorithm on a given problem. Python scikit-learn\n",
    "provides two simple methods for algorithm parameter tuning:\n",
    "\n",
    "* Grid Search Parameter Tuning.\n",
    "* Random Search Parameter Tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917a5b46",
   "metadata": {},
   "source": [
    "#### Grid Search Parameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4ef2bf",
   "metadata": {},
   "source": [
    "Grid search is an approach to parameter tuning that will methodically build and evaluate a\n",
    "model for each combination of algorithm parameters specified in a grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5dfb60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81652c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = load_pima_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab61834",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = numpy.array([1, 0.1, 0.01, 0.001, 0.0001, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65addf16",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = dict(alpha=alphas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f73e4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RidgeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197e91f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28dc5d75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=3, estimator=RidgeClassifier(),\n",
       "             param_grid={&#x27;alpha&#x27;: array([1.e+00, 1.e-01, 1.e-02, 1.e-03, 1.e-04, 0.e+00])})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=3, estimator=RidgeClassifier(),\n",
       "             param_grid={&#x27;alpha&#x27;: array([1.e+00, 1.e-01, 1.e-02, 1.e-03, 1.e-04, 0.e+00])})</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: RidgeClassifier</label><div class=\"sk-toggleable__content\"><pre>RidgeClassifier()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RidgeClassifier</label><div class=\"sk-toggleable__content\"><pre>RidgeClassifier()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=3, estimator=RidgeClassifier(),\n",
       "             param_grid={'alpha': array([1.e+00, 1.e-01, 1.e-02, 1.e-03, 1.e-04, 0.e+00])})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1125b5a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7708333333333334\n"
     ]
    }
   ],
   "source": [
    "print(grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214f039f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(grid.best_estimator_.alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d80369",
   "metadata": {},
   "source": [
    "#### Random Search Parameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf6b315",
   "metadata": {},
   "source": [
    "Random search is an approach to parameter tuning that will sample algorithm parameters from\n",
    "a random distribution (i.e. uniform) for a fixed number of iterations. A model is constructed\n",
    "and evaluated for each combination of parameters chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d308ede0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import uniform\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4243dd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = load_pima_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619b939b",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'alpha': uniform()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701bd04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RidgeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99386806",
   "metadata": {},
   "outputs": [],
   "source": [
    "rsearch = RandomizedSearchCV(estimator=model, \n",
    "                            param_distributions=param_grid,\n",
    "                           n_iter=100, cv=3, random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa10e08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomizedSearchCV(cv=3, estimator=RidgeClassifier(), n_iter=100,\n",
       "                   param_distributions={&#x27;alpha&#x27;: &lt;scipy.stats._distn_infrastructure.rv_continuous_frozen object at 0x7ff9ffd58100&gt;},\n",
       "                   random_state=7)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomizedSearchCV</label><div class=\"sk-toggleable__content\"><pre>RandomizedSearchCV(cv=3, estimator=RidgeClassifier(), n_iter=100,\n",
       "                   param_distributions={&#x27;alpha&#x27;: &lt;scipy.stats._distn_infrastructure.rv_continuous_frozen object at 0x7ff9ffd58100&gt;},\n",
       "                   random_state=7)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: RidgeClassifier</label><div class=\"sk-toggleable__content\"><pre>RidgeClassifier()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RidgeClassifier</label><div class=\"sk-toggleable__content\"><pre>RidgeClassifier()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomizedSearchCV(cv=3, estimator=RidgeClassifier(), n_iter=100,\n",
       "                   param_distributions={'alpha': <scipy.stats._distn_infrastructure.rv_continuous_frozen object>},\n",
       "                   random_state=7)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rsearch.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e39db2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7708333333333334\n"
     ]
    }
   ],
   "source": [
    "print(rsearch.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a7e697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07630828937395717\n"
     ]
    }
   ],
   "source": [
    "print(rsearch.best_estimator_.alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4209a7",
   "metadata": {},
   "source": [
    "Algorithm parameter tuning is an important step for improving algorithm performance right\n",
    "before presenting results or preparing a system for production. In this chapter you discovered\n",
    "algorithm parameter tuning and two methods that you can use right now in Python and\n",
    "scikit-learn to improve your algorithm results:\n",
    "\n",
    "* Grid Search Parameter Tuning\n",
    "* Random Search Parameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99e5551",
   "metadata": {},
   "source": [
    "## Present Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eebacd7",
   "metadata": {},
   "source": [
    "Finding an accurate machine learning model is not the end of the project. In this chapter you\n",
    "will discover how to save and load your machine learning model in Python using scikit-learn.\n",
    "This allows you to save your model to file and load it later in order to make predictions. After\n",
    "completing this lesson you will know:\n",
    "\n",
    "1. The importance of serializing models for reuse.\n",
    "2. How to use pickle to serialize and deserialize machine learning models.\n",
    "3. How to use Joblib to serialize and deserialize machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f401c461",
   "metadata": {},
   "source": [
    "### Finalize Your Model with pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d8e079",
   "metadata": {},
   "source": [
    "Pickle is the standard way of serializing objects in Python. You can use the pickle 1 operation\n",
    "to serialize your machine learning algorithms and save the serialized format to a file. Later you\n",
    "can load this file to deserialize your model and use it to make new predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c370656b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from pickle import dump, load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd97d83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = load_pima_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5d7476",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size=0.33, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09789d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(solver=&#x27;liblinear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" checked><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(solver=&#x27;liblinear&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(solver='liblinear')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LogisticRegression(solver='liblinear')\n",
    "model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735734b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(model, open('models/model1.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9a85fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = load(open('models/model1.sav', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee7de13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7559055118110236\n"
     ]
    }
   ],
   "source": [
    "result = loaded_model.score(X_test, Y_test)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4baa66",
   "metadata": {},
   "source": [
    "### Finalize Your Model with Joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7886d57",
   "metadata": {},
   "source": [
    "The Joblib 2 library is part of the SciPy ecosystem and provides utilities for pipelining Python\n",
    "jobs. It provides utilities for saving and loading Python objects that make use of NumPy data\n",
    "structures, efficiently 3 . This can be useful for some machine learning algorithms that require a\n",
    "lot of parameters or store the entire dataset (e.g. k-Nearest Neighbors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f333e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from joblib import dump, load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c628b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = load_pima_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f7fe58",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.33, random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd972a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(solver='liblinear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff33ca7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(solver=&#x27;liblinear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" checked><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(solver=&#x27;liblinear&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(solver='liblinear')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51dcecd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/model2.sav']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dump(model, 'models/model2.sav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc11a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = load('models/model2.sav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2debeec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7559055118110236\n"
     ]
    }
   ],
   "source": [
    "result = loaded_model.score(X_test, Y_test)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6aceb1",
   "metadata": {},
   "source": [
    "**Tips for Finalizing Your Model**\n",
    "\n",
    "This section lists some important considerations when finalizing your machine learning models.\n",
    "* Python Version. Take note of the Python version. You almost certainly require the\n",
    "same major (and maybe minor) version of Python used to serialize the model when you\n",
    "later load it and deserialize it.\n",
    "* Library Versions. The version of all major libraries used in your machine learning\n",
    "project almost certainly need to be the same when deserializing a saved model. This is\n",
    "not limited to the version of NumPy and the version of scikit-learn.\n",
    "* Manual Serialization. You might like to manually output the parameters of your\n",
    "learned model so that you can use them directly in scikit-learn or another platform in\n",
    "the future. Often the techniques used internally by machine learning algorithms to make\n",
    "predictions are a lot simpler than those used to learn the parameters and can be easy to\n",
    "implement in custom code that you have control over.\n",
    "\n",
    "Take note of the version so that you can re-create the environment if for some reason you\n",
    "cannot reload your model on another machine or another platform at a later time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78b5fda",
   "metadata": {},
   "source": [
    "**Summary**\n",
    "\n",
    "In this chapter you discovered how to persist your machine learning algorithms in Python with\n",
    "scikit-learn. You learned two techniques that you can use:\n",
    "* The pickle API for serializing standard Python objects.\n",
    "* The Joblib API for efficiently serializing Python objects with NumPy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5dd7dfc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
